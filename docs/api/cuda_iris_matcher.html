<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 16.0.0"/>
    <title>cuda_iris_matcher API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;z-index:999;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;opacity:0;}nav.pdoc{--pad:clamp(0.5rem, 2vw, 1.75rem);--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent; z-index:1}nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc > div > ul{margin-left:calc(0px - var(--pad));}nav.pdoc li a{padding:.2rem 0 .2rem calc(var(--pad) + var(--indent));}nav.pdoc > div > ul > li > a{padding-left:var(--pad);}nav.pdoc li{transition:all 100ms;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{cursor:pointer;display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .alert{padding:1rem 1rem 1rem calc(1.5rem + 24px);border:1px solid transparent;border-radius:.25rem;background-repeat:no-repeat;background-position:.75rem center;margin-bottom:1rem;}.pdoc .alert > em{display:none;}.pdoc .alert > *:last-child{margin-bottom:0;}.pdoc .alert.note{color:#084298;background-color:#cfe2ff;border-color:#b6d4fe;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23084298%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8%2016A8%208%200%201%200%208%200a8%208%200%200%200%200%2016zm.93-9.412-1%204.705c-.07.34.029.533.304.533.194%200%20.487-.07.686-.246l-.088.416c-.287.346-.92.598-1.465.598-.703%200-1.002-.422-.808-1.319l.738-3.468c.064-.293.006-.399-.287-.47l-.451-.081.082-.381%202.29-.287zM8%205.5a1%201%200%201%201%200-2%201%201%200%200%201%200%202z%22/%3E%3C/svg%3E");}.pdoc .alert.tip{color:#0a3622;background-color:#d1e7dd;border-color:#a3cfbb;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%230a3622%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%206a6%206%200%201%201%2010.174%204.31c-.203.196-.359.4-.453.619l-.762%201.769A.5.5%200%200%201%2010.5%2013a.5.5%200%200%201%200%201%20.5.5%200%200%201%200%201l-.224.447a1%201%200%200%201-.894.553H6.618a1%201%200%200%201-.894-.553L5.5%2015a.5.5%200%200%201%200-1%20.5.5%200%200%201%200-1%20.5.5%200%200%201-.46-.302l-.761-1.77a2%202%200%200%200-.453-.618A5.98%205.98%200%200%201%202%206m6-5a5%205%200%200%200-3.479%208.592c.263.254.514.564.676.941L5.83%2012h4.342l.632-1.467c.162-.377.413-.687.676-.941A5%205%200%200%200%208%201%22/%3E%3C/svg%3E");}.pdoc .alert.important{color:#055160;background-color:#cff4fc;border-color:#9eeaf9;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23055160%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M2%200a2%202%200%200%200-2%202v12a2%202%200%200%200%202%202h12a2%202%200%200%200%202-2V2a2%202%200%200%200-2-2zm6%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.warning{color:#664d03;background-color:#fff3cd;border-color:#ffecb5;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23664d03%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M8.982%201.566a1.13%201.13%200%200%200-1.96%200L.165%2013.233c-.457.778.091%201.767.98%201.767h13.713c.889%200%201.438-.99.98-1.767L8.982%201.566zM8%205c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%205.995A.905.905%200%200%201%208%205zm.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2z%22/%3E%3C/svg%3E");}.pdoc .alert.caution{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M11.46.146A.5.5%200%200%200%2011.107%200H4.893a.5.5%200%200%200-.353.146L.146%204.54A.5.5%200%200%200%200%204.893v6.214a.5.5%200%200%200%20.146.353l4.394%204.394a.5.5%200%200%200%20.353.146h6.214a.5.5%200%200%200%20.353-.146l4.394-4.394a.5.5%200%200%200%20.146-.353V4.893a.5.5%200%200%200-.146-.353zM8%204c.535%200%20.954.462.9.995l-.35%203.507a.552.552%200%200%201-1.1%200L7.1%204.995A.905.905%200%200%201%208%204m.002%206a1%201%200%201%201%200%202%201%201%200%200%201%200-2%22/%3E%3C/svg%3E");}.pdoc .alert.danger{color:#842029;background-color:#f8d7da;border-color:#f5c2c7;background-image:url("data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20width%3D%2224%22%20height%3D%2224%22%20fill%3D%22%23842029%22%20viewBox%3D%220%200%2016%2016%22%3E%3Cpath%20d%3D%22M5.52.359A.5.5%200%200%201%206%200h4a.5.5%200%200%201%20.474.658L8.694%206H12.5a.5.5%200%200%201%20.395.807l-7%209a.5.5%200%200%201-.873-.454L6.823%209.5H3.5a.5.5%200%200%201-.48-.641l2.5-8.5z%22/%3E%3C/svg%3E");}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc > section:not(.module-info) h1{font-size:1.5rem;font-weight:500;}.pdoc > section:not(.module-info) h2{font-size:1.4rem;font-weight:500;}.pdoc > section:not(.module-info) h3{font-size:1.3rem;font-weight:500;}.pdoc > section:not(.module-info) h4{font-size:1.2rem;}.pdoc > section:not(.module-info) h5{font-size:1.1rem;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--accent);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc > section:not(.module-info){margin-bottom:1.5rem;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.view-source-toggle-state,.view-source-toggle-state ~ .pdoc-code{display:none;}.view-source-toggle-state:checked ~ .pdoc-code{display:block;}.view-source-button{display:inline-block;float:right;font-size:.75rem;line-height:1.5rem;color:var(--muted);padding:0 .4rem 0 1.3rem;cursor:pointer;text-indent:-2px;}.view-source-button > span{visibility:hidden;}.module-info .view-source-button{float:none;display:flex;justify-content:flex-end;margin:-1.2rem .4rem -.2rem 0;}.view-source-button::before{position:absolute;content:"View Source";display:list-item;list-style-type:disclosure-closed;}.view-source-toggle-state:checked ~ .attr .view-source-button::before,.view-source-toggle-state:checked ~ .view-source-button::before{list-style-type:disclosure-open;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc section:not(.module-info) .docstring{margin-left:clamp(0rem, 5vw - 2rem, 1rem);}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target,.pdoc .pdoc-code > pre > span:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc .pdoc-code > pre > span:target{display:block;}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc *{scroll-margin:2rem;}.pdoc .pdoc-code .linenos{user-select:none;}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc section, .pdoc .classattr{position:relative;}.pdoc .headerlink{--width:clamp(1rem, 3vw, 2rem);position:absolute;top:0;left:calc(0rem - var(--width));transition:all 100ms ease-in-out;opacity:0;}.pdoc .headerlink::before{content:"#";display:block;text-align:center;width:var(--width);height:2.3rem;line-height:2.3rem;font-size:1.5rem;}.pdoc .attr:hover ~ .headerlink,.pdoc *:target > .headerlink,.pdoc .headerlink:hover{opacity:1;}.pdoc .attr{display:block;margin:.5rem 0 .5rem;padding:.4rem .4rem .4rem 1rem;background-color:var(--accent);overflow-x:auto;}.pdoc .classattr{margin-left:2rem;}.pdoc .decorator-deprecated{color:#842029;}.pdoc .decorator-deprecated ~ span{filter:grayscale(1) opacity(0.8);}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{background-color:transparent;}.pdoc .param, .pdoc .return-annotation{white-space:pre;}.pdoc .signature.multiline .param{display:block;}.pdoc .signature.condensed .param{display:inline-block;}.pdoc .annotation{color:var(--annotation);}.pdoc .view-value-toggle-state,.pdoc .view-value-toggle-state ~ .default_value{display:none;}.pdoc .view-value-toggle-state:checked ~ .default_value{display:inherit;}.pdoc .view-value-button{font-size:.5rem;vertical-align:middle;border-style:dashed;margin-top:-0.1rem;}.pdoc .view-value-button:hover{background:white;}.pdoc .view-value-button::before{content:"show";text-align:center;width:2.2em;display:inline-block;}.pdoc .view-value-toggle-state:checked ~ .view-value-button::before{content:"hide";}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:27px;vertical-align:bottom;width:50px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>




            <h2>API Documentation</h2>
                <ul class="memberlist">
            <li>
                    <a class="function" href="#masked_hamming_cuda">masked_hamming_cuda</a>
            </li>
            <li>
                    <a class="function" href="#masked_hamming_ab_cuda">masked_hamming_ab_cuda</a>
            </li>
            <li>
                    <a class="function" href="#pack_theta_major">pack_theta_major</a>
            </li>
            <li>
                    <a class="function" href="#repack_to_theta_major">repack_to_theta_major</a>
            </li>
            <li>
                    <a class="function" href="#masked_hamming_sharded">masked_hamming_sharded</a>
            </li>
            <li>
                    <a class="function" href="#masked_hamming_ab_sharded">masked_hamming_ab_sharded</a>
            </li>
            <li>
                    <a class="function" href="#pack_theta_major_batched">pack_theta_major_batched</a>
            </li>
            <li>
                    <a class="function" href="#get_device_count">get_device_count</a>
            </li>
            <li>
                    <a class="function" href="#get_shard_info">get_shard_info</a>
            </li>
            <li>
                    <a class="function" href="#get_self_shard_info">get_self_shard_info</a>
            </li>
            <li>
                    <a class="function" href="#get_total_shards">get_total_shards</a>
            </li>
            <li>
                    <a class="class" href="#ShardConfig">ShardConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ShardConfig.__init__">ShardConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#ShardConfig.device_id">device_id</a>
                        </li>
                        <li>
                                <a class="variable" href="#ShardConfig.a_start">a_start</a>
                        </li>
                        <li>
                                <a class="variable" href="#ShardConfig.a_end">a_end</a>
                        </li>
                        <li>
                                <a class="variable" href="#ShardConfig.b_start">b_start</a>
                        </li>
                        <li>
                                <a class="variable" href="#ShardConfig.b_end">b_end</a>
                        </li>
                        <li>
                                <a class="variable" href="#ShardConfig.global_shard_id">global_shard_id</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="variable" href="#CATEGORY_TRUE_MATCH">CATEGORY_TRUE_MATCH</a>
            </li>
            <li>
                    <a class="variable" href="#CATEGORY_FALSE_MATCH">CATEGORY_FALSE_MATCH</a>
            </li>
            <li>
                    <a class="variable" href="#CATEGORY_FALSE_NON_MATCH">CATEGORY_FALSE_NON_MATCH</a>
            </li>
            <li>
                    <a class="variable" href="#CATEGORY_TRUE_NON_MATCH">CATEGORY_TRUE_NON_MATCH</a>
            </li>
            <li>
                    <a class="variable" href="#INCLUDE_TM">INCLUDE_TM</a>
            </li>
            <li>
                    <a class="variable" href="#INCLUDE_FM">INCLUDE_FM</a>
            </li>
            <li>
                    <a class="variable" href="#INCLUDE_FNM">INCLUDE_FNM</a>
            </li>
            <li>
                    <a class="variable" href="#INCLUDE_TNM">INCLUDE_TNM</a>
            </li>
            <li>
                    <a class="variable" href="#INCLUDE_ALL">INCLUDE_ALL</a>
            </li>
            <li>
                    <a class="variable" href="#DEFAULT_R_DIM">DEFAULT_R_DIM</a>
            </li>
            <li>
                    <a class="variable" href="#DEFAULT_THETA_DIM">DEFAULT_THETA_DIM</a>
            </li>
            <li>
                    <a class="variable" href="#DEFAULT_D0_DIM">DEFAULT_D0_DIM</a>
            </li>
            <li>
                    <a class="variable" href="#DEFAULT_D1_DIM">DEFAULT_D1_DIM</a>
            </li>
            <li>
                    <a class="variable" href="#DEFAULT_DIMS">DEFAULT_DIMS</a>
            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev" target="_blank">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22160%22%20viewBox%3D%220%200%20150%2080%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M132.316%2048.886c.276-4.679%202.342-6.698%204.409-7.982s4.27-1.165%206.751-1.055c1.586.07%203.044.156%204.222-.482%201.142-.619%202.026-1.932%202.162-3.739.268-3.576-1.929-5.368-5.006-5.551s-7.599.524-10.517%201.606c-4.455%201.652-8.588%206.606-9.552%208.992s-2.342%206.193-1.745%2010.873%202.664%209.221%205.878%2011.79%205.878%203.808%2010.103%204.312%203.444.229%206.062.229%205.006-2.202%204.914-4.909-2.296-5.001-4.501-4.863-3.077.505-5.281.229-7.715-2.064-7.899-9.451z%22%20fill%3D%22%23198754%22/%3E%3Ccircle%20cx%3D%22101.504%22%20cy%3D%2248.943%22%20r%3D%2214.208%22%20fill%3D%22none%22%20stroke%3D%22%23198754%22%20stroke-width%3D%229.354%22/%3E%3Cpath%20d%3D%22M87.81.002c-3.637.065-5.001.454-7.014%201.232s-3.443%201.363-6.3%204.282c-1.723%201.76-3.148%205.019-3.776%207.329-.413%201.521-.316%202.63-.316%202.63l-.195%2034.612c.065%205.774-6.755%208.305-9.612%208.37s-9.678-1.038-9.743-9.408%207.128-9.521%208.362-9.521c1.413-.13%202.526-.021%203.718-.016%202.071.009%204.157-.778%204.092-4.671s-4.157-4.736-4.157-4.736c-6.3-.843-11.43%202.206-11.43%202.206S40.917%2038.15%2041.372%2049.634%2051.568%2068.19%2061.311%2068.125s18.316-7.007%2018.445-17.193l.13-22.772c.046-2.291%202.683-3.644%204.476-4.203.745-.232%201.694-.274%201.694-.274l10.457-.13s4.871-.324%207.729-3.114%204.352-6.294%204.352-6.294.974-3.049.13-4.606-.195-1.233-2.792-3.309-8.573-4.477-8.573-4.477S91.447-.063%2087.81.002zM0%2047.169l.065%2028.417S0%2080.127%204.481%2079.997s5.072-3.866%205.049-4.152l-.113-28.482s1.624-7.656%209.937-7.721%2010.002%206.942%2010.002%208.499-.909%2010.51-9.093%2010.51c-.948%200-2.99-.567-4.145-.272-3.919%201-3.194%204.554-3.194%204.554s.065%205.061%207.404%204.996%2018.575-6.034%2018.575-19.074S26.953%2030.04%2019.549%2029.91%201.234%2035.296%200%2047.169z%22%20fill%3D%22%23198754%22/%3E%3Cg%20transform%3D%22matrix%28.325601%200%200%20.325256%20-10.32669%20-45.802786%29%22%3E%3Ccircle%20cx%3D%22297.554%22%20cy%3D%22172.286%22%20r%3D%2216.5%22%20fill%3D%22%23fff%22/%3E%3Cellipse%20cx%3D%22297.709%22%20cy%3D%22172.642%22%20rx%3D%2211.071%22%20ry%3D%2210.871%22%20fill%3D%22%23105a48%22/%3E%3Ccircle%20cx%3D%22304.104%22%20cy%3D%22167.667%22%20r%3D%224.5%22%20fill%3D%22%23fff%22/%3E%3C/g%3E%3Cpath%20d%3D%22M94.661%2017.032l.893-1.476s.99.714%201.916.925%201.575.114%202.955.114l14.565-.162c1.283-.032%203.085-.762%203.02-3.293s-.373-3.503-.373-3.503l1.283-.487s.52.503.877%201.573.309%201.995.292%202.66-.227%201.541-.227%201.541%201.564-.308%202.359-1.038.823-.779%201.489-1.508.812-.86.812-.86.552-.13.877.26.341.957.065%201.46-1.672%202.206-3.247%203.066-2.76%201.427-3.929%201.768-3.848.73-7.063.714l-10.944-.114s-2.143-.081-3.02-.373-2.241-.973-2.598-1.265z%22%20fill%3D%22%23d36d49%22/%3E%3Cg%20fill%3D%22%23105a48%22%3E%3Cellipse%20cx%3D%2293.052%22%20cy%3D%2243.567%22%20rx%3D%22.869%22%20ry%3D%221.014%22%20transform%3D%22rotate%28341.022%29%22/%3E%3Cellipse%20cx%3D%22104.3%22%20cy%3D%22-16.184%22%20rx%3D%22.865%22%20ry%3D%221.009%22%20transform%3D%22rotate%2814.786%29%22/%3E%3C/g%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section class="module-info">
                    <h1 class="modulename">
cuda_iris_matcher    </h1>

                
                        <input id="mod-cuda_iris_matcher-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">

                        <label class="view-source-button" for="mod-cuda_iris_matcher-view-source"><span>View Source</span></label>

                        <div class="pdoc-code codehilite"><pre><span></span><span id="L-1"><a href="#L-1"><span class="linenos"> 1</span></a><span class="kn">from</span> <span class="nn">.ops</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-2"><a href="#L-2"><span class="linenos"> 2</span></a>    <span class="n">masked_hamming_cuda</span><span class="p">,</span>
</span><span id="L-3"><a href="#L-3"><span class="linenos"> 3</span></a>    <span class="n">masked_hamming_ab_cuda</span><span class="p">,</span>
</span><span id="L-4"><a href="#L-4"><span class="linenos"> 4</span></a>    <span class="n">pack_theta_major_cuda</span> <span class="k">as</span> <span class="n">pack_theta_major</span><span class="p">,</span>
</span><span id="L-5"><a href="#L-5"><span class="linenos"> 5</span></a>    <span class="n">repack_to_theta_major_cuda</span> <span class="k">as</span> <span class="n">repack_to_theta_major</span><span class="p">,</span>
</span><span id="L-6"><a href="#L-6"><span class="linenos"> 6</span></a>    <span class="c1"># Classification constants</span>
</span><span id="L-7"><a href="#L-7"><span class="linenos"> 7</span></a>    <span class="n">CATEGORY_TRUE_MATCH</span><span class="p">,</span>
</span><span id="L-8"><a href="#L-8"><span class="linenos"> 8</span></a>    <span class="n">CATEGORY_FALSE_MATCH</span><span class="p">,</span>
</span><span id="L-9"><a href="#L-9"><span class="linenos"> 9</span></a>    <span class="n">CATEGORY_FALSE_NON_MATCH</span><span class="p">,</span>
</span><span id="L-10"><a href="#L-10"><span class="linenos">10</span></a>    <span class="n">CATEGORY_TRUE_NON_MATCH</span><span class="p">,</span>
</span><span id="L-11"><a href="#L-11"><span class="linenos">11</span></a>    <span class="n">INCLUDE_TM</span><span class="p">,</span>
</span><span id="L-12"><a href="#L-12"><span class="linenos">12</span></a>    <span class="n">INCLUDE_FM</span><span class="p">,</span>
</span><span id="L-13"><a href="#L-13"><span class="linenos">13</span></a>    <span class="n">INCLUDE_FNM</span><span class="p">,</span>
</span><span id="L-14"><a href="#L-14"><span class="linenos">14</span></a>    <span class="n">INCLUDE_TNM</span><span class="p">,</span>
</span><span id="L-15"><a href="#L-15"><span class="linenos">15</span></a>    <span class="n">INCLUDE_ALL</span><span class="p">,</span>
</span><span id="L-16"><a href="#L-16"><span class="linenos">16</span></a>    <span class="c1"># Dimension constants</span>
</span><span id="L-17"><a href="#L-17"><span class="linenos">17</span></a>    <span class="n">DEFAULT_R_DIM</span><span class="p">,</span>
</span><span id="L-18"><a href="#L-18"><span class="linenos">18</span></a>    <span class="n">DEFAULT_THETA_DIM</span><span class="p">,</span>
</span><span id="L-19"><a href="#L-19"><span class="linenos">19</span></a>    <span class="n">DEFAULT_D0_DIM</span><span class="p">,</span>
</span><span id="L-20"><a href="#L-20"><span class="linenos">20</span></a>    <span class="n">DEFAULT_D1_DIM</span><span class="p">,</span>
</span><span id="L-21"><a href="#L-21"><span class="linenos">21</span></a>    <span class="n">DEFAULT_DIMS</span><span class="p">,</span>
</span><span id="L-22"><a href="#L-22"><span class="linenos">22</span></a><span class="p">)</span>
</span><span id="L-23"><a href="#L-23"><span class="linenos">23</span></a>
</span><span id="L-24"><a href="#L-24"><span class="linenos">24</span></a><span class="kn">from</span> <span class="nn">.sharding</span> <span class="kn">import</span> <span class="p">(</span>
</span><span id="L-25"><a href="#L-25"><span class="linenos">25</span></a>    <span class="n">masked_hamming_sharded</span><span class="p">,</span>
</span><span id="L-26"><a href="#L-26"><span class="linenos">26</span></a>    <span class="n">masked_hamming_ab_sharded</span><span class="p">,</span>
</span><span id="L-27"><a href="#L-27"><span class="linenos">27</span></a>    <span class="n">pack_theta_major_batched</span><span class="p">,</span>
</span><span id="L-28"><a href="#L-28"><span class="linenos">28</span></a>    <span class="n">get_device_count</span><span class="p">,</span>
</span><span id="L-29"><a href="#L-29"><span class="linenos">29</span></a>    <span class="n">get_shard_info</span><span class="p">,</span>
</span><span id="L-30"><a href="#L-30"><span class="linenos">30</span></a>    <span class="n">get_self_shard_info</span><span class="p">,</span>
</span><span id="L-31"><a href="#L-31"><span class="linenos">31</span></a>    <span class="n">get_total_shards</span><span class="p">,</span>
</span><span id="L-32"><a href="#L-32"><span class="linenos">32</span></a>    <span class="n">ShardConfig</span><span class="p">,</span>
</span><span id="L-33"><a href="#L-33"><span class="linenos">33</span></a><span class="p">)</span>
</span><span id="L-34"><a href="#L-34"><span class="linenos">34</span></a>
</span><span id="L-35"><a href="#L-35"><span class="linenos">35</span></a><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
</span><span id="L-36"><a href="#L-36"><span class="linenos">36</span></a>    <span class="s2">&quot;masked_hamming_cuda&quot;</span><span class="p">,</span>
</span><span id="L-37"><a href="#L-37"><span class="linenos">37</span></a>    <span class="s2">&quot;masked_hamming_ab_cuda&quot;</span><span class="p">,</span>
</span><span id="L-38"><a href="#L-38"><span class="linenos">38</span></a>    <span class="s2">&quot;pack_theta_major&quot;</span><span class="p">,</span>
</span><span id="L-39"><a href="#L-39"><span class="linenos">39</span></a>    <span class="s2">&quot;repack_to_theta_major&quot;</span><span class="p">,</span>
</span><span id="L-40"><a href="#L-40"><span class="linenos">40</span></a>    <span class="c1"># Sharded versions (multi-GPU / multi-host / large dataset support)</span>
</span><span id="L-41"><a href="#L-41"><span class="linenos">41</span></a>    <span class="s2">&quot;masked_hamming_sharded&quot;</span><span class="p">,</span>
</span><span id="L-42"><a href="#L-42"><span class="linenos">42</span></a>    <span class="s2">&quot;masked_hamming_ab_sharded&quot;</span><span class="p">,</span>
</span><span id="L-43"><a href="#L-43"><span class="linenos">43</span></a>    <span class="s2">&quot;pack_theta_major_batched&quot;</span><span class="p">,</span>
</span><span id="L-44"><a href="#L-44"><span class="linenos">44</span></a>    <span class="s2">&quot;get_device_count&quot;</span><span class="p">,</span>
</span><span id="L-45"><a href="#L-45"><span class="linenos">45</span></a>    <span class="s2">&quot;get_shard_info&quot;</span><span class="p">,</span>
</span><span id="L-46"><a href="#L-46"><span class="linenos">46</span></a>    <span class="s2">&quot;get_self_shard_info&quot;</span><span class="p">,</span>
</span><span id="L-47"><a href="#L-47"><span class="linenos">47</span></a>    <span class="s2">&quot;get_total_shards&quot;</span><span class="p">,</span>
</span><span id="L-48"><a href="#L-48"><span class="linenos">48</span></a>    <span class="s2">&quot;ShardConfig&quot;</span><span class="p">,</span>
</span><span id="L-49"><a href="#L-49"><span class="linenos">49</span></a>    <span class="c1"># Classification constants</span>
</span><span id="L-50"><a href="#L-50"><span class="linenos">50</span></a>    <span class="s2">&quot;CATEGORY_TRUE_MATCH&quot;</span><span class="p">,</span>
</span><span id="L-51"><a href="#L-51"><span class="linenos">51</span></a>    <span class="s2">&quot;CATEGORY_FALSE_MATCH&quot;</span><span class="p">,</span>
</span><span id="L-52"><a href="#L-52"><span class="linenos">52</span></a>    <span class="s2">&quot;CATEGORY_FALSE_NON_MATCH&quot;</span><span class="p">,</span>
</span><span id="L-53"><a href="#L-53"><span class="linenos">53</span></a>    <span class="s2">&quot;CATEGORY_TRUE_NON_MATCH&quot;</span><span class="p">,</span>
</span><span id="L-54"><a href="#L-54"><span class="linenos">54</span></a>    <span class="s2">&quot;INCLUDE_TM&quot;</span><span class="p">,</span>
</span><span id="L-55"><a href="#L-55"><span class="linenos">55</span></a>    <span class="s2">&quot;INCLUDE_FM&quot;</span><span class="p">,</span>
</span><span id="L-56"><a href="#L-56"><span class="linenos">56</span></a>    <span class="s2">&quot;INCLUDE_FNM&quot;</span><span class="p">,</span>
</span><span id="L-57"><a href="#L-57"><span class="linenos">57</span></a>    <span class="s2">&quot;INCLUDE_TNM&quot;</span><span class="p">,</span>
</span><span id="L-58"><a href="#L-58"><span class="linenos">58</span></a>    <span class="s2">&quot;INCLUDE_ALL&quot;</span><span class="p">,</span>
</span><span id="L-59"><a href="#L-59"><span class="linenos">59</span></a>    <span class="c1"># Dimension constants</span>
</span><span id="L-60"><a href="#L-60"><span class="linenos">60</span></a>    <span class="s2">&quot;DEFAULT_R_DIM&quot;</span><span class="p">,</span>
</span><span id="L-61"><a href="#L-61"><span class="linenos">61</span></a>    <span class="s2">&quot;DEFAULT_THETA_DIM&quot;</span><span class="p">,</span>
</span><span id="L-62"><a href="#L-62"><span class="linenos">62</span></a>    <span class="s2">&quot;DEFAULT_D0_DIM&quot;</span><span class="p">,</span>
</span><span id="L-63"><a href="#L-63"><span class="linenos">63</span></a>    <span class="s2">&quot;DEFAULT_D1_DIM&quot;</span><span class="p">,</span>
</span><span id="L-64"><a href="#L-64"><span class="linenos">64</span></a>    <span class="s2">&quot;DEFAULT_DIMS&quot;</span><span class="p">,</span>
</span><span id="L-65"><a href="#L-65"><span class="linenos">65</span></a><span class="p">]</span>
</span></pre></div>


            </section>
                <section id="masked_hamming_cuda">
                            <input id="masked_hamming_cuda-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">masked_hamming_cuda</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span>,</span><span class="param">	<span class="n">non_match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span>,</span><span class="param">	<span class="n">is_similarity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">include_flags</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span>,</span><span class="param">	<span class="n">max_pairs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000</span>,</span><span class="param">	<span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span>,</span><span class="param">	<span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="masked_hamming_cuda-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#masked_hamming_cuda"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="masked_hamming_cuda-103"><a href="#masked_hamming_cuda-103"><span class="linenos">103</span></a><span class="k">def</span> <span class="nf">masked_hamming_cuda</span><span class="p">(</span>
</span><span id="masked_hamming_cuda-104"><a href="#masked_hamming_cuda-104"><span class="linenos">104</span></a>    <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-105"><a href="#masked_hamming_cuda-105"><span class="linenos">105</span></a>    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-106"><a href="#masked_hamming_cuda-106"><span class="linenos">106</span></a>    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-107"><a href="#masked_hamming_cuda-107"><span class="linenos">107</span></a>    <span class="n">match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-108"><a href="#masked_hamming_cuda-108"><span class="linenos">108</span></a>    <span class="n">non_match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-109"><a href="#masked_hamming_cuda-109"><span class="linenos">109</span></a>    <span class="n">is_similarity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-110"><a href="#masked_hamming_cuda-110"><span class="linenos">110</span></a>    <span class="n">include_flags</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">INCLUDE_ALL</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-111"><a href="#masked_hamming_cuda-111"><span class="linenos">111</span></a>    <span class="n">max_pairs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000_000</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-112"><a href="#masked_hamming_cuda-112"><span class="linenos">112</span></a>    <span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-113"><a href="#masked_hamming_cuda-113"><span class="linenos">113</span></a>    <span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_R_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-114"><a href="#masked_hamming_cuda-114"><span class="linenos">114</span></a>    <span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_THETA_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-115"><a href="#masked_hamming_cuda-115"><span class="linenos">115</span></a>    <span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D0_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-116"><a href="#masked_hamming_cuda-116"><span class="linenos">116</span></a>    <span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D1_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-117"><a href="#masked_hamming_cuda-117"><span class="linenos">117</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="masked_hamming_cuda-118"><a href="#masked_hamming_cuda-118"><span class="linenos">118</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="masked_hamming_cuda-119"><a href="#masked_hamming_cuda-119"><span class="linenos">119</span></a><span class="sd">    Compute minimum fractional hamming distance for all pairs within a single set.</span>
</span><span id="masked_hamming_cuda-120"><a href="#masked_hamming_cuda-120"><span class="linenos">120</span></a><span class="sd">    Only the lower triangle (i &gt; j) is computed.</span>
</span><span id="masked_hamming_cuda-121"><a href="#masked_hamming_cuda-121"><span class="linenos">121</span></a>
</span><span id="masked_hamming_cuda-122"><a href="#masked_hamming_cuda-122"><span class="linenos">122</span></a><span class="sd">    Accepts either packed or unpacked data - packing is done on GPU automatically.</span>
</span><span id="masked_hamming_cuda-123"><a href="#masked_hamming_cuda-123"><span class="linenos">123</span></a>
</span><span id="masked_hamming_cuda-124"><a href="#masked_hamming_cuda-124"><span class="linenos">124</span></a><span class="sd">    Args:</span>
</span><span id="masked_hamming_cuda-125"><a href="#masked_hamming_cuda-125"><span class="linenos">125</span></a><span class="sd">        data: Tensor of shape [M, k_words] int32 (packed) OR</span>
</span><span id="masked_hamming_cuda-126"><a href="#masked_hamming_cuda-126"><span class="linenos">126</span></a><span class="sd">              [M, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked).</span>
</span><span id="masked_hamming_cuda-127"><a href="#masked_hamming_cuda-127"><span class="linenos">127</span></a><span class="sd">              If unpacked, will be packed on GPU automatically.</span>
</span><span id="masked_hamming_cuda-128"><a href="#masked_hamming_cuda-128"><span class="linenos">128</span></a><span class="sd">        mask: Same shape/dtype as data</span>
</span><span id="masked_hamming_cuda-129"><a href="#masked_hamming_cuda-129"><span class="linenos">129</span></a><span class="sd">        labels: Optional CUDA int32 tensor of shape [M] with identity labels.</span>
</span><span id="masked_hamming_cuda-130"><a href="#masked_hamming_cuda-130"><span class="linenos">130</span></a><span class="sd">                If None, no classification is performed and all pairs are returned.</span>
</span><span id="masked_hamming_cuda-131"><a href="#masked_hamming_cuda-131"><span class="linenos">131</span></a><span class="sd">        match_threshold: Threshold for match classification (default: 0.35)</span>
</span><span id="masked_hamming_cuda-132"><a href="#masked_hamming_cuda-132"><span class="linenos">132</span></a><span class="sd">        non_match_threshold: Threshold for non-match classification (default: 0.35)</span>
</span><span id="masked_hamming_cuda-133"><a href="#masked_hamming_cuda-133"><span class="linenos">133</span></a><span class="sd">        is_similarity: If True, higher values = more similar (use &gt;= for match).</span>
</span><span id="masked_hamming_cuda-134"><a href="#masked_hamming_cuda-134"><span class="linenos">134</span></a><span class="sd">                       If False (default), lower values = more similar (use &lt;= for match).</span>
</span><span id="masked_hamming_cuda-135"><a href="#masked_hamming_cuda-135"><span class="linenos">135</span></a><span class="sd">        include_flags: Bitmask of categories to include (default: INCLUDE_ALL)</span>
</span><span id="masked_hamming_cuda-136"><a href="#masked_hamming_cuda-136"><span class="linenos">136</span></a><span class="sd">                       Use INCLUDE_TM | INCLUDE_FM | ... to combine flags.</span>
</span><span id="masked_hamming_cuda-137"><a href="#masked_hamming_cuda-137"><span class="linenos">137</span></a><span class="sd">        max_pairs: Maximum number of pairs to return (default: 1,000,000)</span>
</span><span id="masked_hamming_cuda-138"><a href="#masked_hamming_cuda-138"><span class="linenos">138</span></a><span class="sd">        dims: Optional tuple (r_dim, theta_dim, d0_dim, d1_dim). If provided, overrides</span>
</span><span id="masked_hamming_cuda-139"><a href="#masked_hamming_cuda-139"><span class="linenos">139</span></a><span class="sd">              individual dimension parameters.</span>
</span><span id="masked_hamming_cuda-140"><a href="#masked_hamming_cuda-140"><span class="linenos">140</span></a><span class="sd">        r_dim: Radial dimension of iris code (default: 16)</span>
</span><span id="masked_hamming_cuda-141"><a href="#masked_hamming_cuda-141"><span class="linenos">141</span></a><span class="sd">        theta_dim: Angular dimension of iris code (default: 200)</span>
</span><span id="masked_hamming_cuda-142"><a href="#masked_hamming_cuda-142"><span class="linenos">142</span></a><span class="sd">        d0_dim: First inner dimension (default: 2)</span>
</span><span id="masked_hamming_cuda-143"><a href="#masked_hamming_cuda-143"><span class="linenos">143</span></a><span class="sd">        d1_dim: Second inner dimension (default: 2)</span>
</span><span id="masked_hamming_cuda-144"><a href="#masked_hamming_cuda-144"><span class="linenos">144</span></a>
</span><span id="masked_hamming_cuda-145"><a href="#masked_hamming_cuda-145"><span class="linenos">145</span></a><span class="sd">    Returns:</span>
</span><span id="masked_hamming_cuda-146"><a href="#masked_hamming_cuda-146"><span class="linenos">146</span></a><span class="sd">        Tuple of (pair_indices, categories, distances, count):</span>
</span><span id="masked_hamming_cuda-147"><a href="#masked_hamming_cuda-147"><span class="linenos">147</span></a><span class="sd">        - pair_indices: [N, 2] int32 - (row, col) indices of pairs</span>
</span><span id="masked_hamming_cuda-148"><a href="#masked_hamming_cuda-148"><span class="linenos">148</span></a><span class="sd">        - categories: [N] uint8 - category codes (0=TM, 1=FM, 2=FNM, 3=TNM, 255=unclassified)</span>
</span><span id="masked_hamming_cuda-149"><a href="#masked_hamming_cuda-149"><span class="linenos">149</span></a><span class="sd">        - distances: [N] float32 - distance values</span>
</span><span id="masked_hamming_cuda-150"><a href="#masked_hamming_cuda-150"><span class="linenos">150</span></a><span class="sd">        - count: [1] int32 - actual number of pairs (N == len(pair_indices))</span>
</span><span id="masked_hamming_cuda-151"><a href="#masked_hamming_cuda-151"><span class="linenos">151</span></a>
</span><span id="masked_hamming_cuda-152"><a href="#masked_hamming_cuda-152"><span class="linenos">152</span></a><span class="sd">    Note:</span>
</span><span id="masked_hamming_cuda-153"><a href="#masked_hamming_cuda-153"><span class="linenos">153</span></a><span class="sd">        The returned tensors are pre-sliced to contain only the valid entries.</span>
</span><span id="masked_hamming_cuda-154"><a href="#masked_hamming_cuda-154"><span class="linenos">154</span></a><span class="sd">        Synchronization is handled internally.</span>
</span><span id="masked_hamming_cuda-155"><a href="#masked_hamming_cuda-155"><span class="linenos">155</span></a>
</span><span id="masked_hamming_cuda-156"><a href="#masked_hamming_cuda-156"><span class="linenos">156</span></a><span class="sd">    Example:</span>
</span><span id="masked_hamming_cuda-157"><a href="#masked_hamming_cuda-157"><span class="linenos">157</span></a><span class="sd">        Basic usage with packed data:</span>
</span><span id="masked_hamming_cuda-158"><a href="#masked_hamming_cuda-158"><span class="linenos">158</span></a>
</span><span id="masked_hamming_cuda-159"><a href="#masked_hamming_cuda-159"><span class="linenos">159</span></a><span class="sd">        &gt;&gt;&gt; import torch</span>
</span><span id="masked_hamming_cuda-160"><a href="#masked_hamming_cuda-160"><span class="linenos">160</span></a><span class="sd">        &gt;&gt;&gt; import cuda_iris_matcher as ih</span>
</span><span id="masked_hamming_cuda-161"><a href="#masked_hamming_cuda-161"><span class="linenos">161</span></a><span class="sd">        &gt;&gt;&gt; # Create packed iris codes [M, 400] and masks</span>
</span><span id="masked_hamming_cuda-162"><a href="#masked_hamming_cuda-162"><span class="linenos">162</span></a><span class="sd">        &gt;&gt;&gt; data = torch.randint(0, 2**31, (100, 400), dtype=torch.int32, device=&quot;cuda&quot;)</span>
</span><span id="masked_hamming_cuda-163"><a href="#masked_hamming_cuda-163"><span class="linenos">163</span></a><span class="sd">        &gt;&gt;&gt; mask = torch.full((100, 400), 0x7FFFFFFF, dtype=torch.int32, device=&quot;cuda&quot;)</span>
</span><span id="masked_hamming_cuda-164"><a href="#masked_hamming_cuda-164"><span class="linenos">164</span></a><span class="sd">        &gt;&gt;&gt; # Compute all pairwise distances</span>
</span><span id="masked_hamming_cuda-165"><a href="#masked_hamming_cuda-165"><span class="linenos">165</span></a><span class="sd">        &gt;&gt;&gt; pairs, cats, dists, count = ih.masked_hamming_cuda(data, mask)</span>
</span><span id="masked_hamming_cuda-166"><a href="#masked_hamming_cuda-166"><span class="linenos">166</span></a><span class="sd">        &gt;&gt;&gt; print(f&quot;Found {count.item()} pairs&quot;)</span>
</span><span id="masked_hamming_cuda-167"><a href="#masked_hamming_cuda-167"><span class="linenos">167</span></a>
</span><span id="masked_hamming_cuda-168"><a href="#masked_hamming_cuda-168"><span class="linenos">168</span></a><span class="sd">        With identity labels for classification:</span>
</span><span id="masked_hamming_cuda-169"><a href="#masked_hamming_cuda-169"><span class="linenos">169</span></a>
</span><span id="masked_hamming_cuda-170"><a href="#masked_hamming_cuda-170"><span class="linenos">170</span></a><span class="sd">        &gt;&gt;&gt; labels = torch.arange(100, dtype=torch.int32, device=&quot;cuda&quot;)</span>
</span><span id="masked_hamming_cuda-171"><a href="#masked_hamming_cuda-171"><span class="linenos">171</span></a><span class="sd">        &gt;&gt;&gt; pairs, cats, dists, count = ih.masked_hamming_cuda(</span>
</span><span id="masked_hamming_cuda-172"><a href="#masked_hamming_cuda-172"><span class="linenos">172</span></a><span class="sd">        ...     data, mask, labels=labels,</span>
</span><span id="masked_hamming_cuda-173"><a href="#masked_hamming_cuda-173"><span class="linenos">173</span></a><span class="sd">        ...     match_threshold=0.35,</span>
</span><span id="masked_hamming_cuda-174"><a href="#masked_hamming_cuda-174"><span class="linenos">174</span></a><span class="sd">        ...     include_flags=ih.INCLUDE_TM | ih.INCLUDE_FM  # Only matches</span>
</span><span id="masked_hamming_cuda-175"><a href="#masked_hamming_cuda-175"><span class="linenos">175</span></a><span class="sd">        ... )</span>
</span><span id="masked_hamming_cuda-176"><a href="#masked_hamming_cuda-176"><span class="linenos">176</span></a><span class="sd">        &gt;&gt;&gt; true_matches = pairs[cats == ih.CATEGORY_TRUE_MATCH]</span>
</span><span id="masked_hamming_cuda-177"><a href="#masked_hamming_cuda-177"><span class="linenos">177</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="masked_hamming_cuda-178"><a href="#masked_hamming_cuda-178"><span class="linenos">178</span></a>    <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span> <span class="o">=</span> <span class="n">_resolve_dims</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_cuda-179"><a href="#masked_hamming_cuda-179"><span class="linenos">179</span></a>    
</span><span id="masked_hamming_cuda-180"><a href="#masked_hamming_cuda-180"><span class="linenos">180</span></a>    <span class="c1"># Auto-pack if unpacked data is provided</span>
</span><span id="masked_hamming_cuda-181"><a href="#masked_hamming_cuda-181"><span class="linenos">181</span></a>    <span class="n">data</span> <span class="o">=</span> <span class="n">_ensure_packed</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_cuda-182"><a href="#masked_hamming_cuda-182"><span class="linenos">182</span></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">_ensure_packed</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_cuda-183"><a href="#masked_hamming_cuda-183"><span class="linenos">183</span></a>    
</span><span id="masked_hamming_cuda-184"><a href="#masked_hamming_cuda-184"><span class="linenos">184</span></a>    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">masked_hamming_cuda</span><span class="p">(</span>
</span><span id="masked_hamming_cuda-185"><a href="#masked_hamming_cuda-185"><span class="linenos">185</span></a>        <span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-186"><a href="#masked_hamming_cuda-186"><span class="linenos">186</span></a>        <span class="n">match_threshold</span><span class="p">,</span> <span class="n">non_match_threshold</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-187"><a href="#masked_hamming_cuda-187"><span class="linenos">187</span></a>        <span class="n">is_similarity</span><span class="p">,</span> <span class="n">include_flags</span><span class="p">,</span> <span class="n">max_pairs</span><span class="p">,</span>
</span><span id="masked_hamming_cuda-188"><a href="#masked_hamming_cuda-188"><span class="linenos">188</span></a>        <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span>
</span><span id="masked_hamming_cuda-189"><a href="#masked_hamming_cuda-189"><span class="linenos">189</span></a>    <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Compute minimum fractional hamming distance for all pairs within a single set.
Only the lower triangle (i &gt; j) is computed.</p>

<p>Accepts either packed or unpacked data - packing is done on GPU automatically.</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>data:</strong>  Tensor of shape [M, k_words] int32 (packed) OR
[M, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked).
If unpacked, will be packed on GPU automatically.</li>
<li><strong>mask:</strong>  Same shape/dtype as data</li>
<li><strong>labels:</strong>  Optional CUDA int32 tensor of shape [M] with identity labels.
If None, no classification is performed and all pairs are returned.</li>
<li><strong>match_threshold:</strong>  Threshold for match classification (default: 0.35)</li>
<li><strong>non_match_threshold:</strong>  Threshold for non-match classification (default: 0.35)</li>
<li><strong>is_similarity:</strong>  If True, higher values = more similar (use &gt;= for match).
If False (default), lower values = more similar (use &lt;= for match).</li>
<li><strong>include_flags:</strong>  Bitmask of categories to include (default: INCLUDE_ALL)
Use INCLUDE_TM | INCLUDE_FM | ... to combine flags.</li>
<li><strong>max_pairs:</strong>  Maximum number of pairs to return (default: 1,000,000)</li>
<li><strong>dims:</strong>  Optional tuple (r_dim, theta_dim, d0_dim, d1_dim). If provided, overrides
individual dimension parameters.</li>
<li><strong>r_dim:</strong>  Radial dimension of iris code (default: 16)</li>
<li><strong>theta_dim:</strong>  Angular dimension of iris code (default: 200)</li>
<li><strong>d0_dim:</strong>  First inner dimension (default: 2)</li>
<li><strong>d1_dim:</strong>  Second inner dimension (default: 2)</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>Tuple of (pair_indices, categories, distances, count):</p>
  
  <ul>
  <li>pair_indices: [N, 2] int32 - (row, col) indices of pairs</li>
  <li>categories: [N] uint8 - category codes (0=TM, 1=FM, 2=FNM, 3=TNM, 255=unclassified)</li>
  <li>distances: [N] float32 - distance values</li>
  <li>count: [1] int32 - actual number of pairs (N == len(pair_indices))</li>
  </ul>
</blockquote>

<h6 id="note">Note:</h6>

<blockquote>
  <p>The returned tensors are pre-sliced to contain only the valid entries.
  Synchronization is handled internally.</p>
</blockquote>

<h6 id="example">Example:</h6>

<blockquote>
  <p>Basic usage with packed data:</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cuda_iris_matcher</span> <span class="k">as</span> <span class="nn">ih</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create packed iris codes [M, 400] and masks</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="mh">0x7FFFFFFF</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Compute all pairwise distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairs</span><span class="p">,</span> <span class="n">cats</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_cuda</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="n">count</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2"> pairs&quot;</span><span class="p">)</span>
</code></pre>
  </div>
  
  <p>With identity labels for classification:</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairs</span><span class="p">,</span> <span class="n">cats</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_cuda</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">match_threshold</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">include_flags</span><span class="o">=</span><span class="n">ih</span><span class="o">.</span><span class="n">INCLUDE_TM</span> <span class="o">|</span> <span class="n">ih</span><span class="o">.</span><span class="n">INCLUDE_FM</span>  <span class="c1"># Only matches</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">true_matches</span> <span class="o">=</span> <span class="n">pairs</span><span class="p">[</span><span class="n">cats</span> <span class="o">==</span> <span class="n">ih</span><span class="o">.</span><span class="n">CATEGORY_TRUE_MATCH</span><span class="p">]</span>
</code></pre>
  </div>
</blockquote>
</div>


                </section>
                <section id="masked_hamming_ab_cuda">
                            <input id="masked_hamming_ab_cuda-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">masked_hamming_ab_cuda</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">data_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">data_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">labels_a</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">labels_b</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span>,</span><span class="param">	<span class="n">non_match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span>,</span><span class="param">	<span class="n">is_similarity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">include_flags</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span>,</span><span class="param">	<span class="n">max_pairs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000</span>,</span><span class="param">	<span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span>,</span><span class="param">	<span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="masked_hamming_ab_cuda-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#masked_hamming_ab_cuda"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="masked_hamming_ab_cuda-192"><a href="#masked_hamming_ab_cuda-192"><span class="linenos">192</span></a><span class="k">def</span> <span class="nf">masked_hamming_ab_cuda</span><span class="p">(</span>
</span><span id="masked_hamming_ab_cuda-193"><a href="#masked_hamming_ab_cuda-193"><span class="linenos">193</span></a>    <span class="n">data_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-194"><a href="#masked_hamming_ab_cuda-194"><span class="linenos">194</span></a>    <span class="n">mask_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-195"><a href="#masked_hamming_ab_cuda-195"><span class="linenos">195</span></a>    <span class="n">data_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-196"><a href="#masked_hamming_ab_cuda-196"><span class="linenos">196</span></a>    <span class="n">mask_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-197"><a href="#masked_hamming_ab_cuda-197"><span class="linenos">197</span></a>    <span class="n">labels_a</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-198"><a href="#masked_hamming_ab_cuda-198"><span class="linenos">198</span></a>    <span class="n">labels_b</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-199"><a href="#masked_hamming_ab_cuda-199"><span class="linenos">199</span></a>    <span class="n">match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-200"><a href="#masked_hamming_ab_cuda-200"><span class="linenos">200</span></a>    <span class="n">non_match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-201"><a href="#masked_hamming_ab_cuda-201"><span class="linenos">201</span></a>    <span class="n">is_similarity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-202"><a href="#masked_hamming_ab_cuda-202"><span class="linenos">202</span></a>    <span class="n">include_flags</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">INCLUDE_ALL</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-203"><a href="#masked_hamming_ab_cuda-203"><span class="linenos">203</span></a>    <span class="n">max_pairs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000_000</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-204"><a href="#masked_hamming_ab_cuda-204"><span class="linenos">204</span></a>    <span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-205"><a href="#masked_hamming_ab_cuda-205"><span class="linenos">205</span></a>    <span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_R_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-206"><a href="#masked_hamming_ab_cuda-206"><span class="linenos">206</span></a>    <span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_THETA_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-207"><a href="#masked_hamming_ab_cuda-207"><span class="linenos">207</span></a>    <span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D0_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-208"><a href="#masked_hamming_ab_cuda-208"><span class="linenos">208</span></a>    <span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D1_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-209"><a href="#masked_hamming_ab_cuda-209"><span class="linenos">209</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="masked_hamming_ab_cuda-210"><a href="#masked_hamming_ab_cuda-210"><span class="linenos">210</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="masked_hamming_ab_cuda-211"><a href="#masked_hamming_ab_cuda-211"><span class="linenos">211</span></a><span class="sd">    Compute minimum fractional hamming distance between two different sets A and B.</span>
</span><span id="masked_hamming_ab_cuda-212"><a href="#masked_hamming_ab_cuda-212"><span class="linenos">212</span></a><span class="sd">    Computes the full M_A x M_B matrix (not just lower triangle).</span>
</span><span id="masked_hamming_ab_cuda-213"><a href="#masked_hamming_ab_cuda-213"><span class="linenos">213</span></a>
</span><span id="masked_hamming_ab_cuda-214"><a href="#masked_hamming_ab_cuda-214"><span class="linenos">214</span></a><span class="sd">    Accepts either packed or unpacked data - packing is done on GPU automatically.</span>
</span><span id="masked_hamming_ab_cuda-215"><a href="#masked_hamming_ab_cuda-215"><span class="linenos">215</span></a>
</span><span id="masked_hamming_ab_cuda-216"><a href="#masked_hamming_ab_cuda-216"><span class="linenos">216</span></a><span class="sd">    Args:</span>
</span><span id="masked_hamming_ab_cuda-217"><a href="#masked_hamming_ab_cuda-217"><span class="linenos">217</span></a><span class="sd">        data_a: Tensor of shape [M_A, k_words] int32 (packed) OR</span>
</span><span id="masked_hamming_ab_cuda-218"><a href="#masked_hamming_ab_cuda-218"><span class="linenos">218</span></a><span class="sd">                [M_A, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</span>
</span><span id="masked_hamming_ab_cuda-219"><a href="#masked_hamming_ab_cuda-219"><span class="linenos">219</span></a><span class="sd">        mask_a: Same shape/dtype as data_a</span>
</span><span id="masked_hamming_ab_cuda-220"><a href="#masked_hamming_ab_cuda-220"><span class="linenos">220</span></a><span class="sd">        data_b: Tensor of shape [M_B, k_words] int32 (packed) OR</span>
</span><span id="masked_hamming_ab_cuda-221"><a href="#masked_hamming_ab_cuda-221"><span class="linenos">221</span></a><span class="sd">                [M_B, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</span>
</span><span id="masked_hamming_ab_cuda-222"><a href="#masked_hamming_ab_cuda-222"><span class="linenos">222</span></a><span class="sd">        mask_b: Same shape/dtype as data_b</span>
</span><span id="masked_hamming_ab_cuda-223"><a href="#masked_hamming_ab_cuda-223"><span class="linenos">223</span></a><span class="sd">        labels_a: Optional CUDA int32 tensor of shape [M_A] with identity labels.</span>
</span><span id="masked_hamming_ab_cuda-224"><a href="#masked_hamming_ab_cuda-224"><span class="linenos">224</span></a><span class="sd">        labels_b: Optional CUDA int32 tensor of shape [M_B] with identity labels.</span>
</span><span id="masked_hamming_ab_cuda-225"><a href="#masked_hamming_ab_cuda-225"><span class="linenos">225</span></a><span class="sd">                  Both labels_a and labels_b must be provided, or neither.</span>
</span><span id="masked_hamming_ab_cuda-226"><a href="#masked_hamming_ab_cuda-226"><span class="linenos">226</span></a><span class="sd">                  If None, no classification is performed and all pairs are returned.</span>
</span><span id="masked_hamming_ab_cuda-227"><a href="#masked_hamming_ab_cuda-227"><span class="linenos">227</span></a><span class="sd">        match_threshold: Threshold for match classification (default: 0.35)</span>
</span><span id="masked_hamming_ab_cuda-228"><a href="#masked_hamming_ab_cuda-228"><span class="linenos">228</span></a><span class="sd">        non_match_threshold: Threshold for non-match classification (default: 0.35)</span>
</span><span id="masked_hamming_ab_cuda-229"><a href="#masked_hamming_ab_cuda-229"><span class="linenos">229</span></a><span class="sd">        is_similarity: If True, higher values = more similar (use &gt;= for match).</span>
</span><span id="masked_hamming_ab_cuda-230"><a href="#masked_hamming_ab_cuda-230"><span class="linenos">230</span></a><span class="sd">                       If False (default), lower values = more similar (use &lt;= for match).</span>
</span><span id="masked_hamming_ab_cuda-231"><a href="#masked_hamming_ab_cuda-231"><span class="linenos">231</span></a><span class="sd">        include_flags: Bitmask of categories to include (default: INCLUDE_ALL)</span>
</span><span id="masked_hamming_ab_cuda-232"><a href="#masked_hamming_ab_cuda-232"><span class="linenos">232</span></a><span class="sd">                       Use INCLUDE_TM | INCLUDE_FM | ... to combine flags.</span>
</span><span id="masked_hamming_ab_cuda-233"><a href="#masked_hamming_ab_cuda-233"><span class="linenos">233</span></a><span class="sd">        max_pairs: Maximum number of pairs to return (default: 1,000,000)</span>
</span><span id="masked_hamming_ab_cuda-234"><a href="#masked_hamming_ab_cuda-234"><span class="linenos">234</span></a><span class="sd">        dims: Optional tuple (r_dim, theta_dim, d0_dim, d1_dim). If provided, overrides</span>
</span><span id="masked_hamming_ab_cuda-235"><a href="#masked_hamming_ab_cuda-235"><span class="linenos">235</span></a><span class="sd">              individual dimension parameters.</span>
</span><span id="masked_hamming_ab_cuda-236"><a href="#masked_hamming_ab_cuda-236"><span class="linenos">236</span></a><span class="sd">        r_dim: Radial dimension of iris code (default: 16)</span>
</span><span id="masked_hamming_ab_cuda-237"><a href="#masked_hamming_ab_cuda-237"><span class="linenos">237</span></a><span class="sd">        theta_dim: Angular dimension of iris code (default: 200)</span>
</span><span id="masked_hamming_ab_cuda-238"><a href="#masked_hamming_ab_cuda-238"><span class="linenos">238</span></a><span class="sd">        d0_dim: First inner dimension (default: 2)</span>
</span><span id="masked_hamming_ab_cuda-239"><a href="#masked_hamming_ab_cuda-239"><span class="linenos">239</span></a><span class="sd">        d1_dim: Second inner dimension (default: 2)</span>
</span><span id="masked_hamming_ab_cuda-240"><a href="#masked_hamming_ab_cuda-240"><span class="linenos">240</span></a>
</span><span id="masked_hamming_ab_cuda-241"><a href="#masked_hamming_ab_cuda-241"><span class="linenos">241</span></a><span class="sd">    Returns:</span>
</span><span id="masked_hamming_ab_cuda-242"><a href="#masked_hamming_ab_cuda-242"><span class="linenos">242</span></a><span class="sd">        Tuple of (pair_indices, categories, distances, count):</span>
</span><span id="masked_hamming_ab_cuda-243"><a href="#masked_hamming_ab_cuda-243"><span class="linenos">243</span></a><span class="sd">        - pair_indices: [N, 2] int32 - (row, col) indices of pairs</span>
</span><span id="masked_hamming_ab_cuda-244"><a href="#masked_hamming_ab_cuda-244"><span class="linenos">244</span></a><span class="sd">        - categories: [N] uint8 - category codes (0=TM, 1=FM, 2=FNM, 3=TNM, 255=unclassified)</span>
</span><span id="masked_hamming_ab_cuda-245"><a href="#masked_hamming_ab_cuda-245"><span class="linenos">245</span></a><span class="sd">        - distances: [N] float32 - distance values</span>
</span><span id="masked_hamming_ab_cuda-246"><a href="#masked_hamming_ab_cuda-246"><span class="linenos">246</span></a><span class="sd">        - count: [1] int32 - actual number of pairs (N == len(pair_indices))</span>
</span><span id="masked_hamming_ab_cuda-247"><a href="#masked_hamming_ab_cuda-247"><span class="linenos">247</span></a>
</span><span id="masked_hamming_ab_cuda-248"><a href="#masked_hamming_ab_cuda-248"><span class="linenos">248</span></a><span class="sd">    Note:</span>
</span><span id="masked_hamming_ab_cuda-249"><a href="#masked_hamming_ab_cuda-249"><span class="linenos">249</span></a><span class="sd">        The returned tensors are pre-sliced to contain only the valid entries.</span>
</span><span id="masked_hamming_ab_cuda-250"><a href="#masked_hamming_ab_cuda-250"><span class="linenos">250</span></a><span class="sd">        Synchronization is handled internally.</span>
</span><span id="masked_hamming_ab_cuda-251"><a href="#masked_hamming_ab_cuda-251"><span class="linenos">251</span></a>
</span><span id="masked_hamming_ab_cuda-252"><a href="#masked_hamming_ab_cuda-252"><span class="linenos">252</span></a><span class="sd">    Example:</span>
</span><span id="masked_hamming_ab_cuda-253"><a href="#masked_hamming_ab_cuda-253"><span class="linenos">253</span></a><span class="sd">        Compare a gallery set against probe samples:</span>
</span><span id="masked_hamming_ab_cuda-254"><a href="#masked_hamming_ab_cuda-254"><span class="linenos">254</span></a>
</span><span id="masked_hamming_ab_cuda-255"><a href="#masked_hamming_ab_cuda-255"><span class="linenos">255</span></a><span class="sd">        &gt;&gt;&gt; import torch</span>
</span><span id="masked_hamming_ab_cuda-256"><a href="#masked_hamming_ab_cuda-256"><span class="linenos">256</span></a><span class="sd">        &gt;&gt;&gt; import cuda_iris_matcher as ih</span>
</span><span id="masked_hamming_ab_cuda-257"><a href="#masked_hamming_ab_cuda-257"><span class="linenos">257</span></a><span class="sd">        &gt;&gt;&gt; # Gallery: 10000 enrolled iris codes</span>
</span><span id="masked_hamming_ab_cuda-258"><a href="#masked_hamming_ab_cuda-258"><span class="linenos">258</span></a><span class="sd">        &gt;&gt;&gt; gallery = torch.randint(0, 2**31, (10000, 400), dtype=torch.int32, device=&quot;cuda&quot;)</span>
</span><span id="masked_hamming_ab_cuda-259"><a href="#masked_hamming_ab_cuda-259"><span class="linenos">259</span></a><span class="sd">        &gt;&gt;&gt; gallery_mask = torch.full_like(gallery, 0x7FFFFFFF)</span>
</span><span id="masked_hamming_ab_cuda-260"><a href="#masked_hamming_ab_cuda-260"><span class="linenos">260</span></a><span class="sd">        &gt;&gt;&gt; # Probe: 50 query iris codes</span>
</span><span id="masked_hamming_ab_cuda-261"><a href="#masked_hamming_ab_cuda-261"><span class="linenos">261</span></a><span class="sd">        &gt;&gt;&gt; probe = torch.randint(0, 2**31, (50, 400), dtype=torch.int32, device=&quot;cuda&quot;)</span>
</span><span id="masked_hamming_ab_cuda-262"><a href="#masked_hamming_ab_cuda-262"><span class="linenos">262</span></a><span class="sd">        &gt;&gt;&gt; probe_mask = torch.full_like(probe, 0x7FFFFFFF)</span>
</span><span id="masked_hamming_ab_cuda-263"><a href="#masked_hamming_ab_cuda-263"><span class="linenos">263</span></a><span class="sd">        &gt;&gt;&gt; # Find all matches</span>
</span><span id="masked_hamming_ab_cuda-264"><a href="#masked_hamming_ab_cuda-264"><span class="linenos">264</span></a><span class="sd">        &gt;&gt;&gt; pairs, _, dists, count = ih.masked_hamming_ab_cuda(</span>
</span><span id="masked_hamming_ab_cuda-265"><a href="#masked_hamming_ab_cuda-265"><span class="linenos">265</span></a><span class="sd">        ...     gallery, gallery_mask, probe, probe_mask,</span>
</span><span id="masked_hamming_ab_cuda-266"><a href="#masked_hamming_ab_cuda-266"><span class="linenos">266</span></a><span class="sd">        ...     match_threshold=0.35</span>
</span><span id="masked_hamming_ab_cuda-267"><a href="#masked_hamming_ab_cuda-267"><span class="linenos">267</span></a><span class="sd">        ... )</span>
</span><span id="masked_hamming_ab_cuda-268"><a href="#masked_hamming_ab_cuda-268"><span class="linenos">268</span></a><span class="sd">        &gt;&gt;&gt; # pairs[:, 0] = gallery index, pairs[:, 1] = probe index</span>
</span><span id="masked_hamming_ab_cuda-269"><a href="#masked_hamming_ab_cuda-269"><span class="linenos">269</span></a><span class="sd">        &gt;&gt;&gt; print(f&quot;Found {count.item()} comparisons&quot;)</span>
</span><span id="masked_hamming_ab_cuda-270"><a href="#masked_hamming_ab_cuda-270"><span class="linenos">270</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="masked_hamming_ab_cuda-271"><a href="#masked_hamming_ab_cuda-271"><span class="linenos">271</span></a>    <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span> <span class="o">=</span> <span class="n">_resolve_dims</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_cuda-272"><a href="#masked_hamming_ab_cuda-272"><span class="linenos">272</span></a>    
</span><span id="masked_hamming_ab_cuda-273"><a href="#masked_hamming_ab_cuda-273"><span class="linenos">273</span></a>    <span class="c1"># Auto-pack if unpacked data is provided</span>
</span><span id="masked_hamming_ab_cuda-274"><a href="#masked_hamming_ab_cuda-274"><span class="linenos">274</span></a>    <span class="n">data_a</span> <span class="o">=</span> <span class="n">_ensure_packed</span><span class="p">(</span><span class="n">data_a</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_cuda-275"><a href="#masked_hamming_ab_cuda-275"><span class="linenos">275</span></a>    <span class="n">mask_a</span> <span class="o">=</span> <span class="n">_ensure_packed</span><span class="p">(</span><span class="n">mask_a</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_cuda-276"><a href="#masked_hamming_ab_cuda-276"><span class="linenos">276</span></a>    <span class="n">data_b</span> <span class="o">=</span> <span class="n">_ensure_packed</span><span class="p">(</span><span class="n">data_b</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_cuda-277"><a href="#masked_hamming_ab_cuda-277"><span class="linenos">277</span></a>    <span class="n">mask_b</span> <span class="o">=</span> <span class="n">_ensure_packed</span><span class="p">(</span><span class="n">mask_b</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_cuda-278"><a href="#masked_hamming_ab_cuda-278"><span class="linenos">278</span></a>    
</span><span id="masked_hamming_ab_cuda-279"><a href="#masked_hamming_ab_cuda-279"><span class="linenos">279</span></a>    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">masked_hamming_ab_cuda</span><span class="p">(</span>
</span><span id="masked_hamming_ab_cuda-280"><a href="#masked_hamming_ab_cuda-280"><span class="linenos">280</span></a>        <span class="n">data_a</span><span class="p">,</span> <span class="n">mask_a</span><span class="p">,</span> <span class="n">data_b</span><span class="p">,</span> <span class="n">mask_b</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-281"><a href="#masked_hamming_ab_cuda-281"><span class="linenos">281</span></a>        <span class="n">labels_a</span><span class="p">,</span> <span class="n">labels_b</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-282"><a href="#masked_hamming_ab_cuda-282"><span class="linenos">282</span></a>        <span class="n">match_threshold</span><span class="p">,</span> <span class="n">non_match_threshold</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-283"><a href="#masked_hamming_ab_cuda-283"><span class="linenos">283</span></a>        <span class="n">is_similarity</span><span class="p">,</span> <span class="n">include_flags</span><span class="p">,</span> <span class="n">max_pairs</span><span class="p">,</span>
</span><span id="masked_hamming_ab_cuda-284"><a href="#masked_hamming_ab_cuda-284"><span class="linenos">284</span></a>        <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span>
</span><span id="masked_hamming_ab_cuda-285"><a href="#masked_hamming_ab_cuda-285"><span class="linenos">285</span></a>    <span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Compute minimum fractional hamming distance between two different sets A and B.
Computes the full M_A x M_B matrix (not just lower triangle).</p>

<p>Accepts either packed or unpacked data - packing is done on GPU automatically.</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>data_a:</strong>  Tensor of shape [M_A, k_words] int32 (packed) OR
[M_A, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</li>
<li><strong>mask_a:</strong>  Same shape/dtype as data_a</li>
<li><strong>data_b:</strong>  Tensor of shape [M_B, k_words] int32 (packed) OR
[M_B, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</li>
<li><strong>mask_b:</strong>  Same shape/dtype as data_b</li>
<li><strong>labels_a:</strong>  Optional CUDA int32 tensor of shape [M_A] with identity labels.</li>
<li><strong>labels_b:</strong>  Optional CUDA int32 tensor of shape [M_B] with identity labels.
Both labels_a and labels_b must be provided, or neither.
If None, no classification is performed and all pairs are returned.</li>
<li><strong>match_threshold:</strong>  Threshold for match classification (default: 0.35)</li>
<li><strong>non_match_threshold:</strong>  Threshold for non-match classification (default: 0.35)</li>
<li><strong>is_similarity:</strong>  If True, higher values = more similar (use &gt;= for match).
If False (default), lower values = more similar (use &lt;= for match).</li>
<li><strong>include_flags:</strong>  Bitmask of categories to include (default: INCLUDE_ALL)
Use INCLUDE_TM | INCLUDE_FM | ... to combine flags.</li>
<li><strong>max_pairs:</strong>  Maximum number of pairs to return (default: 1,000,000)</li>
<li><strong>dims:</strong>  Optional tuple (r_dim, theta_dim, d0_dim, d1_dim). If provided, overrides
individual dimension parameters.</li>
<li><strong>r_dim:</strong>  Radial dimension of iris code (default: 16)</li>
<li><strong>theta_dim:</strong>  Angular dimension of iris code (default: 200)</li>
<li><strong>d0_dim:</strong>  First inner dimension (default: 2)</li>
<li><strong>d1_dim:</strong>  Second inner dimension (default: 2)</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>Tuple of (pair_indices, categories, distances, count):</p>
  
  <ul>
  <li>pair_indices: [N, 2] int32 - (row, col) indices of pairs</li>
  <li>categories: [N] uint8 - category codes (0=TM, 1=FM, 2=FNM, 3=TNM, 255=unclassified)</li>
  <li>distances: [N] float32 - distance values</li>
  <li>count: [1] int32 - actual number of pairs (N == len(pair_indices))</li>
  </ul>
</blockquote>

<h6 id="note">Note:</h6>

<blockquote>
  <p>The returned tensors are pre-sliced to contain only the valid entries.
  Synchronization is handled internally.</p>
</blockquote>

<h6 id="example">Example:</h6>

<blockquote>
  <p>Compare a gallery set against probe samples:</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cuda_iris_matcher</span> <span class="k">as</span> <span class="nn">ih</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Gallery: 10000 enrolled iris codes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gallery</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gallery_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">gallery</span><span class="p">,</span> <span class="mh">0x7FFFFFFF</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Probe: 50 query iris codes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probe_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">probe</span><span class="p">,</span> <span class="mh">0x7FFFFFFF</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Find all matches</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_ab_cuda</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gallery</span><span class="p">,</span> <span class="n">gallery_mask</span><span class="p">,</span> <span class="n">probe</span><span class="p">,</span> <span class="n">probe_mask</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">match_threshold</span><span class="o">=</span><span class="mf">0.35</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># pairs[:, 0] = gallery index, pairs[:, 1] = probe index</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="n">count</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2"> comparisons&quot;</span><span class="p">)</span>
</code></pre>
  </div>
</blockquote>
</div>


                </section>
                <section id="pack_theta_major">
                            <input id="pack_theta_major-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">pack_theta_major</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">bits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span>,</span><span class="param">	<span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="pack_theta_major-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#pack_theta_major"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="pack_theta_major-288"><a href="#pack_theta_major-288"><span class="linenos">288</span></a><span class="k">def</span> <span class="nf">pack_theta_major_cuda</span><span class="p">(</span>
</span><span id="pack_theta_major-289"><a href="#pack_theta_major-289"><span class="linenos">289</span></a>    <span class="n">bits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="pack_theta_major-290"><a href="#pack_theta_major-290"><span class="linenos">290</span></a>    <span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="pack_theta_major-291"><a href="#pack_theta_major-291"><span class="linenos">291</span></a>    <span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_R_DIM</span><span class="p">,</span>
</span><span id="pack_theta_major-292"><a href="#pack_theta_major-292"><span class="linenos">292</span></a>    <span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_THETA_DIM</span><span class="p">,</span>
</span><span id="pack_theta_major-293"><a href="#pack_theta_major-293"><span class="linenos">293</span></a>    <span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D0_DIM</span><span class="p">,</span>
</span><span id="pack_theta_major-294"><a href="#pack_theta_major-294"><span class="linenos">294</span></a>    <span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D1_DIM</span><span class="p">,</span>
</span><span id="pack_theta_major-295"><a href="#pack_theta_major-295"><span class="linenos">295</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="pack_theta_major-296"><a href="#pack_theta_major-296"><span class="linenos">296</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="pack_theta_major-297"><a href="#pack_theta_major-297"><span class="linenos">297</span></a><span class="sd">    Pack iris code bits into theta-major int32 words using CUDA.</span>
</span><span id="pack_theta_major-298"><a href="#pack_theta_major-298"><span class="linenos">298</span></a>
</span><span id="pack_theta_major-299"><a href="#pack_theta_major-299"><span class="linenos">299</span></a><span class="sd">    Args:</span>
</span><span id="pack_theta_major-300"><a href="#pack_theta_major-300"><span class="linenos">300</span></a><span class="sd">        bits: CUDA uint8 tensor of shape (M, r_dim, theta_dim, d0_dim, d1_dim) with values in {0, 1}.</span>
</span><span id="pack_theta_major-301"><a href="#pack_theta_major-301"><span class="linenos">301</span></a><span class="sd">              Modified in-place; do not use after this call.</span>
</span><span id="pack_theta_major-302"><a href="#pack_theta_major-302"><span class="linenos">302</span></a><span class="sd">        dims: Optional tuple (r_dim, theta_dim, d0_dim, d1_dim). If provided, overrides</span>
</span><span id="pack_theta_major-303"><a href="#pack_theta_major-303"><span class="linenos">303</span></a><span class="sd">              individual dimension parameters.</span>
</span><span id="pack_theta_major-304"><a href="#pack_theta_major-304"><span class="linenos">304</span></a><span class="sd">        r_dim: Radial dimension of iris code (default: 16)</span>
</span><span id="pack_theta_major-305"><a href="#pack_theta_major-305"><span class="linenos">305</span></a><span class="sd">        theta_dim: Angular dimension of iris code (default: 200)</span>
</span><span id="pack_theta_major-306"><a href="#pack_theta_major-306"><span class="linenos">306</span></a><span class="sd">        d0_dim: First inner dimension (default: 2)</span>
</span><span id="pack_theta_major-307"><a href="#pack_theta_major-307"><span class="linenos">307</span></a><span class="sd">        d1_dim: Second inner dimension (default: 2)</span>
</span><span id="pack_theta_major-308"><a href="#pack_theta_major-308"><span class="linenos">308</span></a>
</span><span id="pack_theta_major-309"><a href="#pack_theta_major-309"><span class="linenos">309</span></a><span class="sd">    Returns:</span>
</span><span id="pack_theta_major-310"><a href="#pack_theta_major-310"><span class="linenos">310</span></a><span class="sd">        Packed int32 tensor of shape (M, k_words) where k_words = r_dim * theta_dim * d0_dim * d1_dim / 32.</span>
</span><span id="pack_theta_major-311"><a href="#pack_theta_major-311"><span class="linenos">311</span></a><span class="sd">        Shares storage with input (no additional memory allocation).</span>
</span><span id="pack_theta_major-312"><a href="#pack_theta_major-312"><span class="linenos">312</span></a>
</span><span id="pack_theta_major-313"><a href="#pack_theta_major-313"><span class="linenos">313</span></a><span class="sd">    Constraints:</span>
</span><span id="pack_theta_major-314"><a href="#pack_theta_major-314"><span class="linenos">314</span></a><span class="sd">        - r_dim * d0_dim * d1_dim must be divisible by 32 (for whole-word theta shifts)</span>
</span><span id="pack_theta_major-315"><a href="#pack_theta_major-315"><span class="linenos">315</span></a><span class="sd">        - r_dim * theta_dim * d0_dim * d1_dim must be divisible by 256 (TensorCore alignment)</span>
</span><span id="pack_theta_major-316"><a href="#pack_theta_major-316"><span class="linenos">316</span></a>
</span><span id="pack_theta_major-317"><a href="#pack_theta_major-317"><span class="linenos">317</span></a><span class="sd">    Example:</span>
</span><span id="pack_theta_major-318"><a href="#pack_theta_major-318"><span class="linenos">318</span></a><span class="sd">        Pack raw iris codes for efficient matching:</span>
</span><span id="pack_theta_major-319"><a href="#pack_theta_major-319"><span class="linenos">319</span></a>
</span><span id="pack_theta_major-320"><a href="#pack_theta_major-320"><span class="linenos">320</span></a><span class="sd">        &gt;&gt;&gt; import torch</span>
</span><span id="pack_theta_major-321"><a href="#pack_theta_major-321"><span class="linenos">321</span></a><span class="sd">        &gt;&gt;&gt; import cuda_iris_matcher as ih</span>
</span><span id="pack_theta_major-322"><a href="#pack_theta_major-322"><span class="linenos">322</span></a><span class="sd">        &gt;&gt;&gt; # Raw binary iris codes from feature extractor</span>
</span><span id="pack_theta_major-323"><a href="#pack_theta_major-323"><span class="linenos">323</span></a><span class="sd">        &gt;&gt;&gt; raw_codes = torch.randint(0, 2, (1000, 16, 200, 2, 2), dtype=torch.uint8, device=&quot;cuda&quot;)</span>
</span><span id="pack_theta_major-324"><a href="#pack_theta_major-324"><span class="linenos">324</span></a><span class="sd">        &gt;&gt;&gt; raw_masks = torch.ones_like(raw_codes)</span>
</span><span id="pack_theta_major-325"><a href="#pack_theta_major-325"><span class="linenos">325</span></a><span class="sd">        &gt;&gt;&gt; # Pack for efficient GPU matching (clone since it&#39;s in-place)</span>
</span><span id="pack_theta_major-326"><a href="#pack_theta_major-326"><span class="linenos">326</span></a><span class="sd">        &gt;&gt;&gt; packed_codes = ih.pack_theta_major(raw_codes.clone())</span>
</span><span id="pack_theta_major-327"><a href="#pack_theta_major-327"><span class="linenos">327</span></a><span class="sd">        &gt;&gt;&gt; packed_masks = ih.pack_theta_major(raw_masks.clone())</span>
</span><span id="pack_theta_major-328"><a href="#pack_theta_major-328"><span class="linenos">328</span></a><span class="sd">        &gt;&gt;&gt; print(packed_codes.shape)  # torch.Size([1000, 400])</span>
</span><span id="pack_theta_major-329"><a href="#pack_theta_major-329"><span class="linenos">329</span></a><span class="sd">        &gt;&gt;&gt; print(packed_codes.dtype)  # torch.int32</span>
</span><span id="pack_theta_major-330"><a href="#pack_theta_major-330"><span class="linenos">330</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="pack_theta_major-331"><a href="#pack_theta_major-331"><span class="linenos">331</span></a>    <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span> <span class="o">=</span> <span class="n">_resolve_dims</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="pack_theta_major-332"><a href="#pack_theta_major-332"><span class="linenos">332</span></a>    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">pack_theta_major_cuda</span><span class="p">(</span><span class="n">bits</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Pack iris code bits into theta-major int32 words using CUDA.</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>bits:</strong>  CUDA uint8 tensor of shape (M, r_dim, theta_dim, d0_dim, d1_dim) with values in {0, 1}.
Modified in-place; do not use after this call.</li>
<li><strong>dims:</strong>  Optional tuple (r_dim, theta_dim, d0_dim, d1_dim). If provided, overrides
individual dimension parameters.</li>
<li><strong>r_dim:</strong>  Radial dimension of iris code (default: 16)</li>
<li><strong>theta_dim:</strong>  Angular dimension of iris code (default: 200)</li>
<li><strong>d0_dim:</strong>  First inner dimension (default: 2)</li>
<li><strong>d1_dim:</strong>  Second inner dimension (default: 2)</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>Packed int32 tensor of shape (M, k_words) where k_words = r_dim * theta_dim * d0_dim * d1_dim / 32.
  Shares storage with input (no additional memory allocation).</p>
</blockquote>

<h6 id="constraints">Constraints:</h6>

<blockquote>
  <ul>
  <li>r_dim * d0_dim * d1_dim must be divisible by 32 (for whole-word theta shifts)</li>
  <li>r_dim * theta_dim * d0_dim * d1_dim must be divisible by 256 (TensorCore alignment)</li>
  </ul>
</blockquote>

<h6 id="example">Example:</h6>

<blockquote>
  <p>Pack raw iris codes for efficient matching:</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cuda_iris_matcher</span> <span class="k">as</span> <span class="nn">ih</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Raw binary iris codes from feature extractor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">raw_codes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">raw_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">raw_codes</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Pack for efficient GPU matching (clone since it&#39;s in-place)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">packed_codes</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">pack_theta_major</span><span class="p">(</span><span class="n">raw_codes</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">packed_masks</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">pack_theta_major</span><span class="p">(</span><span class="n">raw_masks</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">packed_codes</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1000, 400])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">packed_codes</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>  <span class="c1"># torch.int32</span>
</code></pre>
  </div>
</blockquote>
</div>


                </section>
                <section id="repack_to_theta_major">
                            <input id="repack_to_theta_major-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">repack_to_theta_major</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span>,</span><span class="param">	<span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="repack_to_theta_major-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#repack_to_theta_major"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="repack_to_theta_major-335"><a href="#repack_to_theta_major-335"><span class="linenos">335</span></a><span class="k">def</span> <span class="nf">repack_to_theta_major_cuda</span><span class="p">(</span>
</span><span id="repack_to_theta_major-336"><a href="#repack_to_theta_major-336"><span class="linenos">336</span></a>    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="repack_to_theta_major-337"><a href="#repack_to_theta_major-337"><span class="linenos">337</span></a>    <span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="repack_to_theta_major-338"><a href="#repack_to_theta_major-338"><span class="linenos">338</span></a>    <span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_R_DIM</span><span class="p">,</span>
</span><span id="repack_to_theta_major-339"><a href="#repack_to_theta_major-339"><span class="linenos">339</span></a>    <span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_THETA_DIM</span><span class="p">,</span>
</span><span id="repack_to_theta_major-340"><a href="#repack_to_theta_major-340"><span class="linenos">340</span></a>    <span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D0_DIM</span><span class="p">,</span>
</span><span id="repack_to_theta_major-341"><a href="#repack_to_theta_major-341"><span class="linenos">341</span></a>    <span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D1_DIM</span><span class="p">,</span>
</span><span id="repack_to_theta_major-342"><a href="#repack_to_theta_major-342"><span class="linenos">342</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="repack_to_theta_major-343"><a href="#repack_to_theta_major-343"><span class="linenos">343</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
</span><span id="repack_to_theta_major-344"><a href="#repack_to_theta_major-344"><span class="linenos">344</span></a><span class="sd">    Repack int32 words from r-major to theta-major order using CUDA.</span>
</span><span id="repack_to_theta_major-345"><a href="#repack_to_theta_major-345"><span class="linenos">345</span></a>
</span><span id="repack_to_theta_major-346"><a href="#repack_to_theta_major-346"><span class="linenos">346</span></a><span class="sd">    Args:</span>
</span><span id="repack_to_theta_major-347"><a href="#repack_to_theta_major-347"><span class="linenos">347</span></a><span class="sd">        input: CUDA int32 tensor of shape (M, k_words) packed in r-major order.</span>
</span><span id="repack_to_theta_major-348"><a href="#repack_to_theta_major-348"><span class="linenos">348</span></a><span class="sd">        dims: Optional tuple (r_dim, theta_dim, d0_dim, d1_dim). If provided, overrides</span>
</span><span id="repack_to_theta_major-349"><a href="#repack_to_theta_major-349"><span class="linenos">349</span></a><span class="sd">              individual dimension parameters.</span>
</span><span id="repack_to_theta_major-350"><a href="#repack_to_theta_major-350"><span class="linenos">350</span></a><span class="sd">        r_dim: Radial dimension of iris code (default: 16)</span>
</span><span id="repack_to_theta_major-351"><a href="#repack_to_theta_major-351"><span class="linenos">351</span></a><span class="sd">        theta_dim: Angular dimension of iris code (default: 200)</span>
</span><span id="repack_to_theta_major-352"><a href="#repack_to_theta_major-352"><span class="linenos">352</span></a><span class="sd">        d0_dim: First inner dimension (default: 2)</span>
</span><span id="repack_to_theta_major-353"><a href="#repack_to_theta_major-353"><span class="linenos">353</span></a><span class="sd">        d1_dim: Second inner dimension (default: 2)</span>
</span><span id="repack_to_theta_major-354"><a href="#repack_to_theta_major-354"><span class="linenos">354</span></a>
</span><span id="repack_to_theta_major-355"><a href="#repack_to_theta_major-355"><span class="linenos">355</span></a><span class="sd">    Returns:</span>
</span><span id="repack_to_theta_major-356"><a href="#repack_to_theta_major-356"><span class="linenos">356</span></a><span class="sd">        New CUDA int32 tensor of shape (M, k_words) packed in theta-major order.</span>
</span><span id="repack_to_theta_major-357"><a href="#repack_to_theta_major-357"><span class="linenos">357</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="repack_to_theta_major-358"><a href="#repack_to_theta_major-358"><span class="linenos">358</span></a>    <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span> <span class="o">=</span> <span class="n">_resolve_dims</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="repack_to_theta_major-359"><a href="#repack_to_theta_major-359"><span class="linenos">359</span></a>    <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">repack_to_theta_major_cuda</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Repack int32 words from r-major to theta-major order using CUDA.</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>input:</strong>  CUDA int32 tensor of shape (M, k_words) packed in r-major order.</li>
<li><strong>dims:</strong>  Optional tuple (r_dim, theta_dim, d0_dim, d1_dim). If provided, overrides
individual dimension parameters.</li>
<li><strong>r_dim:</strong>  Radial dimension of iris code (default: 16)</li>
<li><strong>theta_dim:</strong>  Angular dimension of iris code (default: 200)</li>
<li><strong>d0_dim:</strong>  First inner dimension (default: 2)</li>
<li><strong>d1_dim:</strong>  Second inner dimension (default: 2)</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>New CUDA int32 tensor of shape (M, k_words) packed in theta-major order.</p>
</blockquote>
</div>


                </section>
                <section id="masked_hamming_sharded">
                            <input id="masked_hamming_sharded-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">masked_hamming_sharded</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span>,</span><span class="param">	<span class="n">non_match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span>,</span><span class="param">	<span class="n">is_similarity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">include_flags</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span>,</span><span class="param">	<span class="n">max_pairs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000</span>,</span><span class="param">	<span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span>,</span><span class="param">	<span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">host_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>,</span><span class="param">	<span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="masked_hamming_sharded-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#masked_hamming_sharded"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="masked_hamming_sharded-825"><a href="#masked_hamming_sharded-825"><span class="linenos"> 825</span></a><span class="k">def</span> <span class="nf">masked_hamming_sharded</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-826"><a href="#masked_hamming_sharded-826"><span class="linenos"> 826</span></a>    <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-827"><a href="#masked_hamming_sharded-827"><span class="linenos"> 827</span></a>    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-828"><a href="#masked_hamming_sharded-828"><span class="linenos"> 828</span></a>    <span class="n">labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-829"><a href="#masked_hamming_sharded-829"><span class="linenos"> 829</span></a>    <span class="n">match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-830"><a href="#masked_hamming_sharded-830"><span class="linenos"> 830</span></a>    <span class="n">non_match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-831"><a href="#masked_hamming_sharded-831"><span class="linenos"> 831</span></a>    <span class="n">is_similarity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-832"><a href="#masked_hamming_sharded-832"><span class="linenos"> 832</span></a>    <span class="n">include_flags</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">INCLUDE_ALL</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-833"><a href="#masked_hamming_sharded-833"><span class="linenos"> 833</span></a>    <span class="n">max_pairs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000_000</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-834"><a href="#masked_hamming_sharded-834"><span class="linenos"> 834</span></a>    <span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-835"><a href="#masked_hamming_sharded-835"><span class="linenos"> 835</span></a>    <span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_R_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-836"><a href="#masked_hamming_sharded-836"><span class="linenos"> 836</span></a>    <span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_THETA_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-837"><a href="#masked_hamming_sharded-837"><span class="linenos"> 837</span></a>    <span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D0_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-838"><a href="#masked_hamming_sharded-838"><span class="linenos"> 838</span></a>    <span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D1_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-839"><a href="#masked_hamming_sharded-839"><span class="linenos"> 839</span></a>    <span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-840"><a href="#masked_hamming_sharded-840"><span class="linenos"> 840</span></a>    <span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-841"><a href="#masked_hamming_sharded-841"><span class="linenos"> 841</span></a>    <span class="n">host_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-842"><a href="#masked_hamming_sharded-842"><span class="linenos"> 842</span></a>    <span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-843"><a href="#masked_hamming_sharded-843"><span class="linenos"> 843</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="masked_hamming_sharded-844"><a href="#masked_hamming_sharded-844"><span class="linenos"> 844</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Sharded version of masked_hamming_cuda for multi-GPU and multi-host datasets.</span>
</span><span id="masked_hamming_sharded-845"><a href="#masked_hamming_sharded-845"><span class="linenos"> 845</span></a>
</span><span id="masked_hamming_sharded-846"><a href="#masked_hamming_sharded-846"><span class="linenos"> 846</span></a><span class="sd">    Computes the lower triangle (i &gt; j) by tiling and distributing across devices.</span>
</span><span id="masked_hamming_sharded-847"><a href="#masked_hamming_sharded-847"><span class="linenos"> 847</span></a><span class="sd">    Accepts either packed (int32) or unpacked (uint8) data - packing is done on GPU.</span>
</span><span id="masked_hamming_sharded-848"><a href="#masked_hamming_sharded-848"><span class="linenos"> 848</span></a>
</span><span id="masked_hamming_sharded-849"><a href="#masked_hamming_sharded-849"><span class="linenos"> 849</span></a><span class="sd">    For multi-host operation:</span>
</span><span id="masked_hamming_sharded-850"><a href="#masked_hamming_sharded-850"><span class="linenos"> 850</span></a><span class="sd">    - Each host should have the FULL data tensor (or be able to access all rows)</span>
</span><span id="masked_hamming_sharded-851"><a href="#masked_hamming_sharded-851"><span class="linenos"> 851</span></a><span class="sd">    - Set host_index to this host&#39;s index (0 to num_hosts-1)</span>
</span><span id="masked_hamming_sharded-852"><a href="#masked_hamming_sharded-852"><span class="linenos"> 852</span></a><span class="sd">    - Set num_hosts to total number of hosts</span>
</span><span id="masked_hamming_sharded-853"><a href="#masked_hamming_sharded-853"><span class="linenos"> 853</span></a><span class="sd">    - Each host will process only its assigned tiles</span>
</span><span id="masked_hamming_sharded-854"><a href="#masked_hamming_sharded-854"><span class="linenos"> 854</span></a><span class="sd">    - Results from all hosts should be aggregated by the caller</span>
</span><span id="masked_hamming_sharded-855"><a href="#masked_hamming_sharded-855"><span class="linenos"> 855</span></a>
</span><span id="masked_hamming_sharded-856"><a href="#masked_hamming_sharded-856"><span class="linenos"> 856</span></a><span class="sd">    Args:</span>
</span><span id="masked_hamming_sharded-857"><a href="#masked_hamming_sharded-857"><span class="linenos"> 857</span></a><span class="sd">        data: Tensor of shape [M, k_words] int32 (packed) OR</span>
</span><span id="masked_hamming_sharded-858"><a href="#masked_hamming_sharded-858"><span class="linenos"> 858</span></a><span class="sd">              [M, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</span>
</span><span id="masked_hamming_sharded-859"><a href="#masked_hamming_sharded-859"><span class="linenos"> 859</span></a><span class="sd">        mask: Same shape/dtype as data</span>
</span><span id="masked_hamming_sharded-860"><a href="#masked_hamming_sharded-860"><span class="linenos"> 860</span></a><span class="sd">        labels: Optional int32 tensor of shape [M] with identity labels.</span>
</span><span id="masked_hamming_sharded-861"><a href="#masked_hamming_sharded-861"><span class="linenos"> 861</span></a><span class="sd">        match_threshold: Threshold for match classification (default: 0.35)</span>
</span><span id="masked_hamming_sharded-862"><a href="#masked_hamming_sharded-862"><span class="linenos"> 862</span></a><span class="sd">        non_match_threshold: Threshold for non-match classification (default: 0.35)</span>
</span><span id="masked_hamming_sharded-863"><a href="#masked_hamming_sharded-863"><span class="linenos"> 863</span></a><span class="sd">        is_similarity: If True, higher values = more similar</span>
</span><span id="masked_hamming_sharded-864"><a href="#masked_hamming_sharded-864"><span class="linenos"> 864</span></a><span class="sd">        include_flags: Bitmask of categories to include (default: INCLUDE_ALL)</span>
</span><span id="masked_hamming_sharded-865"><a href="#masked_hamming_sharded-865"><span class="linenos"> 865</span></a><span class="sd">        max_pairs: Maximum total pairs to return (default: 1,000,000)</span>
</span><span id="masked_hamming_sharded-866"><a href="#masked_hamming_sharded-866"><span class="linenos"> 866</span></a><span class="sd">        dims: Optional tuple (r_dim, theta_dim, d0_dim, d1_dim)</span>
</span><span id="masked_hamming_sharded-867"><a href="#masked_hamming_sharded-867"><span class="linenos"> 867</span></a><span class="sd">        r_dim, theta_dim, d0_dim, d1_dim: Iris code dimensions</span>
</span><span id="masked_hamming_sharded-868"><a href="#masked_hamming_sharded-868"><span class="linenos"> 868</span></a><span class="sd">        min_shards: Minimum number of shards (useful for testing on single GPU)</span>
</span><span id="masked_hamming_sharded-869"><a href="#masked_hamming_sharded-869"><span class="linenos"> 869</span></a><span class="sd">        max_tile_size: Maximum rows per tile (None = auto based on memory)</span>
</span><span id="masked_hamming_sharded-870"><a href="#masked_hamming_sharded-870"><span class="linenos"> 870</span></a><span class="sd">        host_index: Index of this host for multi-host operation (default: 0)</span>
</span><span id="masked_hamming_sharded-871"><a href="#masked_hamming_sharded-871"><span class="linenos"> 871</span></a><span class="sd">        num_hosts: Total number of hosts for multi-host operation (default: 1)</span>
</span><span id="masked_hamming_sharded-872"><a href="#masked_hamming_sharded-872"><span class="linenos"> 872</span></a>
</span><span id="masked_hamming_sharded-873"><a href="#masked_hamming_sharded-873"><span class="linenos"> 873</span></a><span class="sd">    Returns:</span>
</span><span id="masked_hamming_sharded-874"><a href="#masked_hamming_sharded-874"><span class="linenos"> 874</span></a><span class="sd">        Tuple of (pair_indices, categories, distances, count):</span>
</span><span id="masked_hamming_sharded-875"><a href="#masked_hamming_sharded-875"><span class="linenos"> 875</span></a><span class="sd">        - pair_indices: [N, 2] int32 - (row, col) indices of pairs (row &gt; col)</span>
</span><span id="masked_hamming_sharded-876"><a href="#masked_hamming_sharded-876"><span class="linenos"> 876</span></a><span class="sd">        - categories: [N] uint8 - category codes</span>
</span><span id="masked_hamming_sharded-877"><a href="#masked_hamming_sharded-877"><span class="linenos"> 877</span></a><span class="sd">        - distances: [N] float32 - distance values</span>
</span><span id="masked_hamming_sharded-878"><a href="#masked_hamming_sharded-878"><span class="linenos"> 878</span></a><span class="sd">        - count: [1] int32 - actual number of pairs</span>
</span><span id="masked_hamming_sharded-879"><a href="#masked_hamming_sharded-879"><span class="linenos"> 879</span></a>
</span><span id="masked_hamming_sharded-880"><a href="#masked_hamming_sharded-880"><span class="linenos"> 880</span></a><span class="sd">    Example:</span>
</span><span id="masked_hamming_sharded-881"><a href="#masked_hamming_sharded-881"><span class="linenos"> 881</span></a><span class="sd">        Multi-GPU matching (automatically uses all available GPUs):</span>
</span><span id="masked_hamming_sharded-882"><a href="#masked_hamming_sharded-882"><span class="linenos"> 882</span></a>
</span><span id="masked_hamming_sharded-883"><a href="#masked_hamming_sharded-883"><span class="linenos"> 883</span></a><span class="sd">        &gt;&gt;&gt; import torch</span>
</span><span id="masked_hamming_sharded-884"><a href="#masked_hamming_sharded-884"><span class="linenos"> 884</span></a><span class="sd">        &gt;&gt;&gt; import cuda_iris_matcher as ih</span>
</span><span id="masked_hamming_sharded-885"><a href="#masked_hamming_sharded-885"><span class="linenos"> 885</span></a><span class="sd">        &gt;&gt;&gt; # Large dataset on CPU</span>
</span><span id="masked_hamming_sharded-886"><a href="#masked_hamming_sharded-886"><span class="linenos"> 886</span></a><span class="sd">        &gt;&gt;&gt; data = torch.randint(0, 2**31, (50000, 400), dtype=torch.int32)</span>
</span><span id="masked_hamming_sharded-887"><a href="#masked_hamming_sharded-887"><span class="linenos"> 887</span></a><span class="sd">        &gt;&gt;&gt; mask = torch.full_like(data, 0x7FFFFFFF)</span>
</span><span id="masked_hamming_sharded-888"><a href="#masked_hamming_sharded-888"><span class="linenos"> 888</span></a><span class="sd">        &gt;&gt;&gt; # Automatically distributes across GPUs</span>
</span><span id="masked_hamming_sharded-889"><a href="#masked_hamming_sharded-889"><span class="linenos"> 889</span></a><span class="sd">        &gt;&gt;&gt; pairs, cats, dists, count = ih.masked_hamming_sharded(data, mask)</span>
</span><span id="masked_hamming_sharded-890"><a href="#masked_hamming_sharded-890"><span class="linenos"> 890</span></a><span class="sd">        &gt;&gt;&gt; print(f&quot;Using {ih.get_device_count()} GPUs, found {count.item()} pairs&quot;)</span>
</span><span id="masked_hamming_sharded-891"><a href="#masked_hamming_sharded-891"><span class="linenos"> 891</span></a>
</span><span id="masked_hamming_sharded-892"><a href="#masked_hamming_sharded-892"><span class="linenos"> 892</span></a><span class="sd">        Force tiling on single GPU (useful for memory-limited scenarios):</span>
</span><span id="masked_hamming_sharded-893"><a href="#masked_hamming_sharded-893"><span class="linenos"> 893</span></a>
</span><span id="masked_hamming_sharded-894"><a href="#masked_hamming_sharded-894"><span class="linenos"> 894</span></a><span class="sd">        &gt;&gt;&gt; pairs, cats, dists, count = ih.masked_hamming_sharded(</span>
</span><span id="masked_hamming_sharded-895"><a href="#masked_hamming_sharded-895"><span class="linenos"> 895</span></a><span class="sd">        ...     data, mask, min_shards=4  # Split into at least 4 tiles</span>
</span><span id="masked_hamming_sharded-896"><a href="#masked_hamming_sharded-896"><span class="linenos"> 896</span></a><span class="sd">        ... )</span>
</span><span id="masked_hamming_sharded-897"><a href="#masked_hamming_sharded-897"><span class="linenos"> 897</span></a>
</span><span id="masked_hamming_sharded-898"><a href="#masked_hamming_sharded-898"><span class="linenos"> 898</span></a><span class="sd">        Multi-host distributed computation:</span>
</span><span id="masked_hamming_sharded-899"><a href="#masked_hamming_sharded-899"><span class="linenos"> 899</span></a>
</span><span id="masked_hamming_sharded-900"><a href="#masked_hamming_sharded-900"><span class="linenos"> 900</span></a><span class="sd">        &gt;&gt;&gt; # On host 0 of 4:</span>
</span><span id="masked_hamming_sharded-901"><a href="#masked_hamming_sharded-901"><span class="linenos"> 901</span></a><span class="sd">        &gt;&gt;&gt; result_0 = ih.masked_hamming_sharded(data, mask, host_index=0, num_hosts=4)</span>
</span><span id="masked_hamming_sharded-902"><a href="#masked_hamming_sharded-902"><span class="linenos"> 902</span></a><span class="sd">        &gt;&gt;&gt; # On host 1 of 4:</span>
</span><span id="masked_hamming_sharded-903"><a href="#masked_hamming_sharded-903"><span class="linenos"> 903</span></a><span class="sd">        &gt;&gt;&gt; result_1 = ih.masked_hamming_sharded(data, mask, host_index=1, num_hosts=4)</span>
</span><span id="masked_hamming_sharded-904"><a href="#masked_hamming_sharded-904"><span class="linenos"> 904</span></a><span class="sd">        &gt;&gt;&gt; # ... aggregate results from all hosts</span>
</span><span id="masked_hamming_sharded-905"><a href="#masked_hamming_sharded-905"><span class="linenos"> 905</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="masked_hamming_sharded-906"><a href="#masked_hamming_sharded-906"><span class="linenos"> 906</span></a>    <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span> <span class="o">=</span> <span class="n">_resolve_dims</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-907"><a href="#masked_hamming_sharded-907"><span class="linenos"> 907</span></a>        <span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span>
</span><span id="masked_hamming_sharded-908"><a href="#masked_hamming_sharded-908"><span class="linenos"> 908</span></a>    <span class="p">)</span>
</span><span id="masked_hamming_sharded-909"><a href="#masked_hamming_sharded-909"><span class="linenos"> 909</span></a>    <span class="n">k_words</span> <span class="o">=</span> <span class="n">r_dim</span> <span class="o">*</span> <span class="n">theta_dim</span> <span class="o">*</span> <span class="n">d0_dim</span> <span class="o">*</span> <span class="n">d1_dim</span> <span class="o">//</span> <span class="mi">32</span>
</span><span id="masked_hamming_sharded-910"><a href="#masked_hamming_sharded-910"><span class="linenos"> 910</span></a>
</span><span id="masked_hamming_sharded-911"><a href="#masked_hamming_sharded-911"><span class="linenos"> 911</span></a>    <span class="n">m</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-912"><a href="#masked_hamming_sharded-912"><span class="linenos"> 912</span></a>
</span><span id="masked_hamming_sharded-913"><a href="#masked_hamming_sharded-913"><span class="linenos"> 913</span></a>    <span class="c1"># Determine number of devices</span>
</span><span id="masked_hamming_sharded-914"><a href="#masked_hamming_sharded-914"><span class="linenos"> 914</span></a>    <span class="n">num_devices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
</span><span id="masked_hamming_sharded-915"><a href="#masked_hamming_sharded-915"><span class="linenos"> 915</span></a>    <span class="k">if</span> <span class="n">num_devices</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-916"><a href="#masked_hamming_sharded-916"><span class="linenos"> 916</span></a>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No CUDA devices available&quot;</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-917"><a href="#masked_hamming_sharded-917"><span class="linenos"> 917</span></a>
</span><span id="masked_hamming_sharded-918"><a href="#masked_hamming_sharded-918"><span class="linenos"> 918</span></a>    <span class="c1"># Check if data is packed or unpacked</span>
</span><span id="masked_hamming_sharded-919"><a href="#masked_hamming_sharded-919"><span class="linenos"> 919</span></a>    <span class="n">data_is_packed</span> <span class="o">=</span> <span class="n">_is_packed</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k_words</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-920"><a href="#masked_hamming_sharded-920"><span class="linenos"> 920</span></a>    <span class="n">mask_is_packed</span> <span class="o">=</span> <span class="n">_is_packed</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">k_words</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-921"><a href="#masked_hamming_sharded-921"><span class="linenos"> 921</span></a>
</span><span id="masked_hamming_sharded-922"><a href="#masked_hamming_sharded-922"><span class="linenos"> 922</span></a>    <span class="c1"># For single GPU without forced sharding, use non-sharded kernel directly</span>
</span><span id="masked_hamming_sharded-923"><a href="#masked_hamming_sharded-923"><span class="linenos"> 923</span></a>    <span class="k">if</span> <span class="n">num_devices</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">min_shards</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-924"><a href="#masked_hamming_sharded-924"><span class="linenos"> 924</span></a>        <span class="n">primary_device</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="masked_hamming_sharded-925"><a href="#masked_hamming_sharded-925"><span class="linenos"> 925</span></a>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">primary_device</span><span class="p">):</span>
</span><span id="masked_hamming_sharded-926"><a href="#masked_hamming_sharded-926"><span class="linenos"> 926</span></a>            <span class="k">if</span> <span class="n">data_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-927"><a href="#masked_hamming_sharded-927"><span class="linenos"> 927</span></a>                <span class="n">data_gpu</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">data</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">data</span>
</span><span id="masked_hamming_sharded-928"><a href="#masked_hamming_sharded-928"><span class="linenos"> 928</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-929"><a href="#masked_hamming_sharded-929"><span class="linenos"> 929</span></a>                <span class="n">data_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">primary_device</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-930"><a href="#masked_hamming_sharded-930"><span class="linenos"> 930</span></a>            <span class="k">if</span> <span class="n">mask_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-931"><a href="#masked_hamming_sharded-931"><span class="linenos"> 931</span></a>                <span class="n">mask_gpu</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">mask</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">mask</span>
</span><span id="masked_hamming_sharded-932"><a href="#masked_hamming_sharded-932"><span class="linenos"> 932</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-933"><a href="#masked_hamming_sharded-933"><span class="linenos"> 933</span></a>                <span class="n">mask_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">primary_device</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-934"><a href="#masked_hamming_sharded-934"><span class="linenos"> 934</span></a>        <span class="n">labels_gpu</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="masked_hamming_sharded-935"><a href="#masked_hamming_sharded-935"><span class="linenos"> 935</span></a>        <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-936"><a href="#masked_hamming_sharded-936"><span class="linenos"> 936</span></a>            <span class="n">labels_gpu</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">labels</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">labels</span>
</span><span id="masked_hamming_sharded-937"><a href="#masked_hamming_sharded-937"><span class="linenos"> 937</span></a>        <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">masked_hamming_cuda</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-938"><a href="#masked_hamming_sharded-938"><span class="linenos"> 938</span></a>            <span class="n">data_gpu</span><span class="p">,</span> <span class="n">mask_gpu</span><span class="p">,</span> <span class="n">labels_gpu</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-939"><a href="#masked_hamming_sharded-939"><span class="linenos"> 939</span></a>            <span class="n">match_threshold</span><span class="p">,</span> <span class="n">non_match_threshold</span><span class="p">,</span> <span class="n">is_similarity</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-940"><a href="#masked_hamming_sharded-940"><span class="linenos"> 940</span></a>            <span class="n">include_flags</span><span class="p">,</span> <span class="n">max_pairs</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span>
</span><span id="masked_hamming_sharded-941"><a href="#masked_hamming_sharded-941"><span class="linenos"> 941</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_sharded-942"><a href="#masked_hamming_sharded-942"><span class="linenos"> 942</span></a>
</span><span id="masked_hamming_sharded-943"><a href="#masked_hamming_sharded-943"><span class="linenos"> 943</span></a>    <span class="c1"># Compute shard configurations for lower triangle</span>
</span><span id="masked_hamming_sharded-944"><a href="#masked_hamming_sharded-944"><span class="linenos"> 944</span></a>    <span class="c1"># For multi-host, compute shards as if all hosts&#39; GPUs were available</span>
</span><span id="masked_hamming_sharded-945"><a href="#masked_hamming_sharded-945"><span class="linenos"> 945</span></a>    <span class="n">total_devices</span> <span class="o">=</span> <span class="n">num_devices</span> <span class="o">*</span> <span class="n">num_hosts</span>
</span><span id="masked_hamming_sharded-946"><a href="#masked_hamming_sharded-946"><span class="linenos"> 946</span></a>    <span class="n">all_shards</span> <span class="o">=</span> <span class="n">_compute_self_shard_configs</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-947"><a href="#masked_hamming_sharded-947"><span class="linenos"> 947</span></a>        <span class="n">m</span><span class="p">,</span> <span class="n">k_words</span><span class="p">,</span> <span class="n">max_pairs</span><span class="p">,</span> <span class="n">total_devices</span><span class="p">,</span> <span class="n">min_shards</span> <span class="o">*</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">max_tile_size</span>
</span><span id="masked_hamming_sharded-948"><a href="#masked_hamming_sharded-948"><span class="linenos"> 948</span></a>    <span class="p">)</span>
</span><span id="masked_hamming_sharded-949"><a href="#masked_hamming_sharded-949"><span class="linenos"> 949</span></a>
</span><span id="masked_hamming_sharded-950"><a href="#masked_hamming_sharded-950"><span class="linenos"> 950</span></a>    <span class="c1"># Filter to only shards assigned to this host</span>
</span><span id="masked_hamming_sharded-951"><a href="#masked_hamming_sharded-951"><span class="linenos"> 951</span></a>    <span class="n">shards</span> <span class="o">=</span> <span class="n">_filter_shards_for_host</span><span class="p">(</span><span class="n">all_shards</span><span class="p">,</span> <span class="n">host_index</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-952"><a href="#masked_hamming_sharded-952"><span class="linenos"> 952</span></a>
</span><span id="masked_hamming_sharded-953"><a href="#masked_hamming_sharded-953"><span class="linenos"> 953</span></a>    <span class="c1"># If no shards assigned to this host, return empty results</span>
</span><span id="masked_hamming_sharded-954"><a href="#masked_hamming_sharded-954"><span class="linenos"> 954</span></a>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-955"><a href="#masked_hamming_sharded-955"><span class="linenos"> 955</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="masked_hamming_sharded-956"><a href="#masked_hamming_sharded-956"><span class="linenos"> 956</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
</span><span id="masked_hamming_sharded-957"><a href="#masked_hamming_sharded-957"><span class="linenos"> 957</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span>
</span><span id="masked_hamming_sharded-958"><a href="#masked_hamming_sharded-958"><span class="linenos"> 958</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span><span id="masked_hamming_sharded-959"><a href="#masked_hamming_sharded-959"><span class="linenos"> 959</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
</span><span id="masked_hamming_sharded-960"><a href="#masked_hamming_sharded-960"><span class="linenos"> 960</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_sharded-961"><a href="#masked_hamming_sharded-961"><span class="linenos"> 961</span></a>
</span><span id="masked_hamming_sharded-962"><a href="#masked_hamming_sharded-962"><span class="linenos"> 962</span></a>    <span class="n">max_pairs_per_shard</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_pairs</span> <span class="o">//</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_shards</span><span class="p">)),</span> <span class="mi">1000</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-963"><a href="#masked_hamming_sharded-963"><span class="linenos"> 963</span></a>
</span><span id="masked_hamming_sharded-964"><a href="#masked_hamming_sharded-964"><span class="linenos"> 964</span></a>    <span class="c1"># Labels stay on CPU (small)</span>
</span><span id="masked_hamming_sharded-965"><a href="#masked_hamming_sharded-965"><span class="linenos"> 965</span></a>    <span class="n">labels_cpu</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">labels</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">labels</span>
</span><span id="masked_hamming_sharded-966"><a href="#masked_hamming_sharded-966"><span class="linenos"> 966</span></a>
</span><span id="masked_hamming_sharded-967"><a href="#masked_hamming_sharded-967"><span class="linenos"> 967</span></a>    <span class="c1"># Ensure data is on CPU for direct transfers to each GPU</span>
</span><span id="masked_hamming_sharded-968"><a href="#masked_hamming_sharded-968"><span class="linenos"> 968</span></a>    <span class="n">data_cpu</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">data</span>
</span><span id="masked_hamming_sharded-969"><a href="#masked_hamming_sharded-969"><span class="linenos"> 969</span></a>    <span class="n">mask_cpu</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">mask</span>
</span><span id="masked_hamming_sharded-970"><a href="#masked_hamming_sharded-970"><span class="linenos"> 970</span></a>
</span><span id="masked_hamming_sharded-971"><a href="#masked_hamming_sharded-971"><span class="linenos"> 971</span></a>    <span class="c1"># Determine which row ranges each GPU needs</span>
</span><span id="masked_hamming_sharded-972"><a href="#masked_hamming_sharded-972"><span class="linenos"> 972</span></a>    <span class="n">device_ids</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">device_id</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shards</span><span class="p">))</span>
</span><span id="masked_hamming_sharded-973"><a href="#masked_hamming_sharded-973"><span class="linenos"> 973</span></a>    <span class="n">device_ranges</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">set</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{</span><span class="n">d</span><span class="p">:</span> <span class="nb">set</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">device_ids</span><span class="p">}</span>
</span><span id="masked_hamming_sharded-974"><a href="#masked_hamming_sharded-974"><span class="linenos"> 974</span></a>    <span class="k">for</span> <span class="n">shard</span> <span class="ow">in</span> <span class="n">shards</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-975"><a href="#masked_hamming_sharded-975"><span class="linenos"> 975</span></a>        <span class="n">device_ranges</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">shard</span><span class="o">.</span><span class="n">a_start</span><span class="p">,</span> <span class="n">shard</span><span class="o">.</span><span class="n">a_end</span><span class="p">))</span>
</span><span id="masked_hamming_sharded-976"><a href="#masked_hamming_sharded-976"><span class="linenos"> 976</span></a>        <span class="n">device_ranges</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">shard</span><span class="o">.</span><span class="n">b_start</span><span class="p">,</span> <span class="n">shard</span><span class="o">.</span><span class="n">b_end</span><span class="p">))</span>
</span><span id="masked_hamming_sharded-977"><a href="#masked_hamming_sharded-977"><span class="linenos"> 977</span></a>
</span><span id="masked_hamming_sharded-978"><a href="#masked_hamming_sharded-978"><span class="linenos"> 978</span></a>    <span class="c1"># Merge overlapping ranges and compute unique rows needed per device</span>
</span><span id="masked_hamming_sharded-979"><a href="#masked_hamming_sharded-979"><span class="linenos"> 979</span></a>    <span class="k">def</span> <span class="nf">merge_ranges</span><span class="p">(</span><span class="n">ranges</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
</span><span id="masked_hamming_sharded-980"><a href="#masked_hamming_sharded-980"><span class="linenos"> 980</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Merge ranges into a single contiguous range (min_start, max_end).&quot;&quot;&quot;</span>
</span><span id="masked_hamming_sharded-981"><a href="#masked_hamming_sharded-981"><span class="linenos"> 981</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">ranges</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-982"><a href="#masked_hamming_sharded-982"><span class="linenos"> 982</span></a>            <span class="k">return</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-983"><a href="#masked_hamming_sharded-983"><span class="linenos"> 983</span></a>        <span class="n">min_start</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ranges</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-984"><a href="#masked_hamming_sharded-984"><span class="linenos"> 984</span></a>        <span class="n">max_end</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ranges</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-985"><a href="#masked_hamming_sharded-985"><span class="linenos"> 985</span></a>        <span class="k">return</span> <span class="p">(</span><span class="n">min_start</span><span class="p">,</span> <span class="n">max_end</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-986"><a href="#masked_hamming_sharded-986"><span class="linenos"> 986</span></a>
</span><span id="masked_hamming_sharded-987"><a href="#masked_hamming_sharded-987"><span class="linenos"> 987</span></a>    <span class="n">device_row_ranges</span> <span class="o">=</span> <span class="p">{</span><span class="n">d</span><span class="p">:</span> <span class="n">merge_ranges</span><span class="p">(</span><span class="n">ranges</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">ranges</span> <span class="ow">in</span> <span class="n">device_ranges</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span><span id="masked_hamming_sharded-988"><a href="#masked_hamming_sharded-988"><span class="linenos"> 988</span></a>
</span><span id="masked_hamming_sharded-989"><a href="#masked_hamming_sharded-989"><span class="linenos"> 989</span></a>    <span class="c1"># Transfer data directly from CPU to each GPU (only needed rows) in parallel</span>
</span><span id="masked_hamming_sharded-990"><a href="#masked_hamming_sharded-990"><span class="linenos"> 990</span></a>    <span class="k">def</span> <span class="nf">transfer_to_device</span><span class="p">(</span><span class="n">device_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
</span><span id="masked_hamming_sharded-991"><a href="#masked_hamming_sharded-991"><span class="linenos"> 991</span></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Transfer needed rows to a GPU, pack if needed, return (device_id, data, mask, offset).&quot;&quot;&quot;</span>
</span><span id="masked_hamming_sharded-992"><a href="#masked_hamming_sharded-992"><span class="linenos"> 992</span></a>        <span class="n">row_start</span><span class="p">,</span> <span class="n">row_end</span> <span class="o">=</span> <span class="n">device_row_ranges</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-993"><a href="#masked_hamming_sharded-993"><span class="linenos"> 993</span></a>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_id</span><span class="p">):</span>
</span><span id="masked_hamming_sharded-994"><a href="#masked_hamming_sharded-994"><span class="linenos"> 994</span></a>            <span class="n">data_slice</span> <span class="o">=</span> <span class="n">data_cpu</span><span class="p">[</span><span class="n">row_start</span><span class="p">:</span><span class="n">row_end</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-995"><a href="#masked_hamming_sharded-995"><span class="linenos"> 995</span></a>            <span class="n">mask_slice</span> <span class="o">=</span> <span class="n">mask_cpu</span><span class="p">[</span><span class="n">row_start</span><span class="p">:</span><span class="n">row_end</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-996"><a href="#masked_hamming_sharded-996"><span class="linenos"> 996</span></a>
</span><span id="masked_hamming_sharded-997"><a href="#masked_hamming_sharded-997"><span class="linenos"> 997</span></a>            <span class="k">if</span> <span class="n">data_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-998"><a href="#masked_hamming_sharded-998"><span class="linenos"> 998</span></a>                <span class="n">data_gpu</span> <span class="o">=</span> <span class="n">data_slice</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-999"><a href="#masked_hamming_sharded-999"><span class="linenos"> 999</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1000"><a href="#masked_hamming_sharded-1000"><span class="linenos">1000</span></a>                <span class="n">data_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">data_slice</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1001"><a href="#masked_hamming_sharded-1001"><span class="linenos">1001</span></a>
</span><span id="masked_hamming_sharded-1002"><a href="#masked_hamming_sharded-1002"><span class="linenos">1002</span></a>            <span class="k">if</span> <span class="n">mask_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1003"><a href="#masked_hamming_sharded-1003"><span class="linenos">1003</span></a>                <span class="n">mask_gpu</span> <span class="o">=</span> <span class="n">mask_slice</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1004"><a href="#masked_hamming_sharded-1004"><span class="linenos">1004</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1005"><a href="#masked_hamming_sharded-1005"><span class="linenos">1005</span></a>                <span class="n">mask_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">mask_slice</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1006"><a href="#masked_hamming_sharded-1006"><span class="linenos">1006</span></a>
</span><span id="masked_hamming_sharded-1007"><a href="#masked_hamming_sharded-1007"><span class="linenos">1007</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1008"><a href="#masked_hamming_sharded-1008"><span class="linenos">1008</span></a>        <span class="k">return</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">data_gpu</span><span class="p">,</span> <span class="n">mask_gpu</span><span class="p">,</span> <span class="n">row_start</span>
</span><span id="masked_hamming_sharded-1009"><a href="#masked_hamming_sharded-1009"><span class="linenos">1009</span></a>
</span><span id="masked_hamming_sharded-1010"><a href="#masked_hamming_sharded-1010"><span class="linenos">1010</span></a>    <span class="c1"># Transfer to all GPUs in parallel</span>
</span><span id="masked_hamming_sharded-1011"><a href="#masked_hamming_sharded-1011"><span class="linenos">1011</span></a>    <span class="n">device_data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="masked_hamming_sharded-1012"><a href="#masked_hamming_sharded-1012"><span class="linenos">1012</span></a>    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">device_ids</span><span class="p">))</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1013"><a href="#masked_hamming_sharded-1013"><span class="linenos">1013</span></a>        <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">transfer_to_device</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">device_ids</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-1014"><a href="#masked_hamming_sharded-1014"><span class="linenos">1014</span></a>        <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="n">futures</span><span class="p">):</span>
</span><span id="masked_hamming_sharded-1015"><a href="#masked_hamming_sharded-1015"><span class="linenos">1015</span></a>            <span class="n">device_id</span><span class="p">,</span> <span class="n">data_gpu</span><span class="p">,</span> <span class="n">mask_gpu</span><span class="p">,</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
</span><span id="masked_hamming_sharded-1016"><a href="#masked_hamming_sharded-1016"><span class="linenos">1016</span></a>            <span class="n">device_data</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_gpu</span><span class="p">,</span> <span class="n">mask_gpu</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1017"><a href="#masked_hamming_sharded-1017"><span class="linenos">1017</span></a>
</span><span id="masked_hamming_sharded-1018"><a href="#masked_hamming_sharded-1018"><span class="linenos">1018</span></a>    <span class="c1"># Define kernel launch function (returns GPU tensors without sync)</span>
</span><span id="masked_hamming_sharded-1019"><a href="#masked_hamming_sharded-1019"><span class="linenos">1019</span></a>    <span class="k">def</span> <span class="nf">launch_kernel</span><span class="p">(</span><span class="n">shard</span><span class="p">:</span> <span class="n">ShardConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ShardKernelResult</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1020"><a href="#masked_hamming_sharded-1020"><span class="linenos">1020</span></a>        <span class="n">data_gpu</span><span class="p">,</span> <span class="n">mask_gpu</span><span class="p">,</span> <span class="n">row_offset</span> <span class="o">=</span> <span class="n">device_data</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-1021"><a href="#masked_hamming_sharded-1021"><span class="linenos">1021</span></a>
</span><span id="masked_hamming_sharded-1022"><a href="#masked_hamming_sharded-1022"><span class="linenos">1022</span></a>        <span class="c1"># Adjust indices relative to the data slice on this device</span>
</span><span id="masked_hamming_sharded-1023"><a href="#masked_hamming_sharded-1023"><span class="linenos">1023</span></a>        <span class="n">local_a_start</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">a_start</span> <span class="o">-</span> <span class="n">row_offset</span>
</span><span id="masked_hamming_sharded-1024"><a href="#masked_hamming_sharded-1024"><span class="linenos">1024</span></a>        <span class="n">local_a_end</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">a_end</span> <span class="o">-</span> <span class="n">row_offset</span>
</span><span id="masked_hamming_sharded-1025"><a href="#masked_hamming_sharded-1025"><span class="linenos">1025</span></a>        <span class="n">local_b_start</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">b_start</span> <span class="o">-</span> <span class="n">row_offset</span>
</span><span id="masked_hamming_sharded-1026"><a href="#masked_hamming_sharded-1026"><span class="linenos">1026</span></a>        <span class="n">local_b_end</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">b_end</span> <span class="o">-</span> <span class="n">row_offset</span>
</span><span id="masked_hamming_sharded-1027"><a href="#masked_hamming_sharded-1027"><span class="linenos">1027</span></a>
</span><span id="masked_hamming_sharded-1028"><a href="#masked_hamming_sharded-1028"><span class="linenos">1028</span></a>        <span class="n">data_a_tile</span> <span class="o">=</span> <span class="n">data_gpu</span><span class="p">[</span><span class="n">local_a_start</span><span class="p">:</span><span class="n">local_a_end</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-1029"><a href="#masked_hamming_sharded-1029"><span class="linenos">1029</span></a>        <span class="n">mask_a_tile</span> <span class="o">=</span> <span class="n">mask_gpu</span><span class="p">[</span><span class="n">local_a_start</span><span class="p">:</span><span class="n">local_a_end</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-1030"><a href="#masked_hamming_sharded-1030"><span class="linenos">1030</span></a>        <span class="n">data_b_tile</span> <span class="o">=</span> <span class="n">data_gpu</span><span class="p">[</span><span class="n">local_b_start</span><span class="p">:</span><span class="n">local_b_end</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-1031"><a href="#masked_hamming_sharded-1031"><span class="linenos">1031</span></a>        <span class="n">mask_b_tile</span> <span class="o">=</span> <span class="n">mask_gpu</span><span class="p">[</span><span class="n">local_b_start</span><span class="p">:</span><span class="n">local_b_end</span><span class="p">]</span>
</span><span id="masked_hamming_sharded-1032"><a href="#masked_hamming_sharded-1032"><span class="linenos">1032</span></a>
</span><span id="masked_hamming_sharded-1033"><a href="#masked_hamming_sharded-1033"><span class="linenos">1033</span></a>        <span class="n">labels_a_tile</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="masked_hamming_sharded-1034"><a href="#masked_hamming_sharded-1034"><span class="linenos">1034</span></a>        <span class="n">labels_b_tile</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="masked_hamming_sharded-1035"><a href="#masked_hamming_sharded-1035"><span class="linenos">1035</span></a>        <span class="k">if</span> <span class="n">labels_cpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1036"><a href="#masked_hamming_sharded-1036"><span class="linenos">1036</span></a>            <span class="n">labels_a_tile</span> <span class="o">=</span> <span class="n">labels_cpu</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">a_start</span><span class="p">:</span><span class="n">shard</span><span class="o">.</span><span class="n">a_end</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-1037"><a href="#masked_hamming_sharded-1037"><span class="linenos">1037</span></a>                <span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span>
</span><span id="masked_hamming_sharded-1038"><a href="#masked_hamming_sharded-1038"><span class="linenos">1038</span></a>            <span class="p">)</span>
</span><span id="masked_hamming_sharded-1039"><a href="#masked_hamming_sharded-1039"><span class="linenos">1039</span></a>            <span class="n">labels_b_tile</span> <span class="o">=</span> <span class="n">labels_cpu</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">b_start</span><span class="p">:</span><span class="n">shard</span><span class="o">.</span><span class="n">b_end</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-1040"><a href="#masked_hamming_sharded-1040"><span class="linenos">1040</span></a>                <span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span>
</span><span id="masked_hamming_sharded-1041"><a href="#masked_hamming_sharded-1041"><span class="linenos">1041</span></a>            <span class="p">)</span>
</span><span id="masked_hamming_sharded-1042"><a href="#masked_hamming_sharded-1042"><span class="linenos">1042</span></a>
</span><span id="masked_hamming_sharded-1043"><a href="#masked_hamming_sharded-1043"><span class="linenos">1043</span></a>        <span class="n">is_diagonal</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">a_start</span> <span class="o">==</span> <span class="n">shard</span><span class="o">.</span><span class="n">b_start</span>
</span><span id="masked_hamming_sharded-1044"><a href="#masked_hamming_sharded-1044"><span class="linenos">1044</span></a>
</span><span id="masked_hamming_sharded-1045"><a href="#masked_hamming_sharded-1045"><span class="linenos">1045</span></a>        <span class="k">if</span> <span class="n">is_diagonal</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1046"><a href="#masked_hamming_sharded-1046"><span class="linenos">1046</span></a>            <span class="n">labels_tile</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="masked_hamming_sharded-1047"><a href="#masked_hamming_sharded-1047"><span class="linenos">1047</span></a>            <span class="k">if</span> <span class="n">labels_cpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1048"><a href="#masked_hamming_sharded-1048"><span class="linenos">1048</span></a>                <span class="n">labels_tile</span> <span class="o">=</span> <span class="n">labels_cpu</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">a_start</span><span class="p">:</span><span class="n">shard</span><span class="o">.</span><span class="n">a_end</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-1049"><a href="#masked_hamming_sharded-1049"><span class="linenos">1049</span></a>                    <span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span>
</span><span id="masked_hamming_sharded-1050"><a href="#masked_hamming_sharded-1050"><span class="linenos">1050</span></a>                <span class="p">)</span>
</span><span id="masked_hamming_sharded-1051"><a href="#masked_hamming_sharded-1051"><span class="linenos">1051</span></a>
</span><span id="masked_hamming_sharded-1052"><a href="#masked_hamming_sharded-1052"><span class="linenos">1052</span></a>            <span class="n">indices</span><span class="p">,</span> <span class="n">categories</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">masked_hamming_cuda_async</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-1053"><a href="#masked_hamming_sharded-1053"><span class="linenos">1053</span></a>                <span class="n">data_a_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1054"><a href="#masked_hamming_sharded-1054"><span class="linenos">1054</span></a>                <span class="n">mask_a_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1055"><a href="#masked_hamming_sharded-1055"><span class="linenos">1055</span></a>                <span class="n">labels_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1056"><a href="#masked_hamming_sharded-1056"><span class="linenos">1056</span></a>                <span class="n">match_threshold</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1057"><a href="#masked_hamming_sharded-1057"><span class="linenos">1057</span></a>                <span class="n">non_match_threshold</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1058"><a href="#masked_hamming_sharded-1058"><span class="linenos">1058</span></a>                <span class="n">is_similarity</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1059"><a href="#masked_hamming_sharded-1059"><span class="linenos">1059</span></a>                <span class="n">include_flags</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1060"><a href="#masked_hamming_sharded-1060"><span class="linenos">1060</span></a>                <span class="n">max_pairs_per_shard</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1061"><a href="#masked_hamming_sharded-1061"><span class="linenos">1061</span></a>                <span class="n">r_dim</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1062"><a href="#masked_hamming_sharded-1062"><span class="linenos">1062</span></a>                <span class="n">theta_dim</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1063"><a href="#masked_hamming_sharded-1063"><span class="linenos">1063</span></a>                <span class="n">d0_dim</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1064"><a href="#masked_hamming_sharded-1064"><span class="linenos">1064</span></a>                <span class="n">d1_dim</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1065"><a href="#masked_hamming_sharded-1065"><span class="linenos">1065</span></a>            <span class="p">)</span>
</span><span id="masked_hamming_sharded-1066"><a href="#masked_hamming_sharded-1066"><span class="linenos">1066</span></a>        <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1067"><a href="#masked_hamming_sharded-1067"><span class="linenos">1067</span></a>            <span class="n">indices</span><span class="p">,</span> <span class="n">categories</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">masked_hamming_ab_cuda_async</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-1068"><a href="#masked_hamming_sharded-1068"><span class="linenos">1068</span></a>                <span class="n">data_a_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1069"><a href="#masked_hamming_sharded-1069"><span class="linenos">1069</span></a>                <span class="n">mask_a_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1070"><a href="#masked_hamming_sharded-1070"><span class="linenos">1070</span></a>                <span class="n">data_b_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1071"><a href="#masked_hamming_sharded-1071"><span class="linenos">1071</span></a>                <span class="n">mask_b_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1072"><a href="#masked_hamming_sharded-1072"><span class="linenos">1072</span></a>                <span class="n">labels_a_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1073"><a href="#masked_hamming_sharded-1073"><span class="linenos">1073</span></a>                <span class="n">labels_b_tile</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1074"><a href="#masked_hamming_sharded-1074"><span class="linenos">1074</span></a>                <span class="n">match_threshold</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1075"><a href="#masked_hamming_sharded-1075"><span class="linenos">1075</span></a>                <span class="n">non_match_threshold</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1076"><a href="#masked_hamming_sharded-1076"><span class="linenos">1076</span></a>                <span class="n">is_similarity</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1077"><a href="#masked_hamming_sharded-1077"><span class="linenos">1077</span></a>                <span class="n">include_flags</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1078"><a href="#masked_hamming_sharded-1078"><span class="linenos">1078</span></a>                <span class="n">max_pairs_per_shard</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1079"><a href="#masked_hamming_sharded-1079"><span class="linenos">1079</span></a>                <span class="n">r_dim</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1080"><a href="#masked_hamming_sharded-1080"><span class="linenos">1080</span></a>                <span class="n">theta_dim</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1081"><a href="#masked_hamming_sharded-1081"><span class="linenos">1081</span></a>                <span class="n">d0_dim</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1082"><a href="#masked_hamming_sharded-1082"><span class="linenos">1082</span></a>                <span class="n">d1_dim</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1083"><a href="#masked_hamming_sharded-1083"><span class="linenos">1083</span></a>            <span class="p">)</span>
</span><span id="masked_hamming_sharded-1084"><a href="#masked_hamming_sharded-1084"><span class="linenos">1084</span></a>
</span><span id="masked_hamming_sharded-1085"><a href="#masked_hamming_sharded-1085"><span class="linenos">1085</span></a>        <span class="k">return</span> <span class="n">ShardKernelResult</span><span class="p">(</span>
</span><span id="masked_hamming_sharded-1086"><a href="#masked_hamming_sharded-1086"><span class="linenos">1086</span></a>            <span class="n">shard</span><span class="o">=</span><span class="n">shard</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1087"><a href="#masked_hamming_sharded-1087"><span class="linenos">1087</span></a>            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1088"><a href="#masked_hamming_sharded-1088"><span class="linenos">1088</span></a>            <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1089"><a href="#masked_hamming_sharded-1089"><span class="linenos">1089</span></a>            <span class="n">distances</span><span class="o">=</span><span class="n">distances</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1090"><a href="#masked_hamming_sharded-1090"><span class="linenos">1090</span></a>            <span class="n">count</span><span class="o">=</span><span class="n">count</span><span class="p">,</span>
</span><span id="masked_hamming_sharded-1091"><a href="#masked_hamming_sharded-1091"><span class="linenos">1091</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_sharded-1092"><a href="#masked_hamming_sharded-1092"><span class="linenos">1092</span></a>
</span><span id="masked_hamming_sharded-1093"><a href="#masked_hamming_sharded-1093"><span class="linenos">1093</span></a>    <span class="c1"># Run shards with async kernel execution across devices</span>
</span><span id="masked_hamming_sharded-1094"><a href="#masked_hamming_sharded-1094"><span class="linenos">1094</span></a>    <span class="n">shard_results</span> <span class="o">=</span> <span class="n">_run_shards_async</span><span class="p">(</span><span class="n">shards</span><span class="p">,</span> <span class="n">launch_kernel</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1095"><a href="#masked_hamming_sharded-1095"><span class="linenos">1095</span></a>
</span><span id="masked_hamming_sharded-1096"><a href="#masked_hamming_sharded-1096"><span class="linenos">1096</span></a>    <span class="c1"># Aggregate results</span>
</span><span id="masked_hamming_sharded-1097"><a href="#masked_hamming_sharded-1097"><span class="linenos">1097</span></a>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shard_results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1098"><a href="#masked_hamming_sharded-1098"><span class="linenos">1098</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="masked_hamming_sharded-1099"><a href="#masked_hamming_sharded-1099"><span class="linenos">1099</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
</span><span id="masked_hamming_sharded-1100"><a href="#masked_hamming_sharded-1100"><span class="linenos">1100</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span>
</span><span id="masked_hamming_sharded-1101"><a href="#masked_hamming_sharded-1101"><span class="linenos">1101</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span><span id="masked_hamming_sharded-1102"><a href="#masked_hamming_sharded-1102"><span class="linenos">1102</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
</span><span id="masked_hamming_sharded-1103"><a href="#masked_hamming_sharded-1103"><span class="linenos">1103</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_sharded-1104"><a href="#masked_hamming_sharded-1104"><span class="linenos">1104</span></a>
</span><span id="masked_hamming_sharded-1105"><a href="#masked_hamming_sharded-1105"><span class="linenos">1105</span></a>    <span class="c1"># Collect and concatenate results, respecting max_pairs limit</span>
</span><span id="masked_hamming_sharded-1106"><a href="#masked_hamming_sharded-1106"><span class="linenos">1106</span></a>    <span class="n">all_indices</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="masked_hamming_sharded-1107"><a href="#masked_hamming_sharded-1107"><span class="linenos">1107</span></a>    <span class="n">all_categories</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="masked_hamming_sharded-1108"><a href="#masked_hamming_sharded-1108"><span class="linenos">1108</span></a>    <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="masked_hamming_sharded-1109"><a href="#masked_hamming_sharded-1109"><span class="linenos">1109</span></a>    <span class="n">total_count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="masked_hamming_sharded-1110"><a href="#masked_hamming_sharded-1110"><span class="linenos">1110</span></a>
</span><span id="masked_hamming_sharded-1111"><a href="#masked_hamming_sharded-1111"><span class="linenos">1111</span></a>    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">shard_results</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1112"><a href="#masked_hamming_sharded-1112"><span class="linenos">1112</span></a>        <span class="n">remaining</span> <span class="o">=</span> <span class="n">max_pairs</span> <span class="o">-</span> <span class="n">total_count</span>
</span><span id="masked_hamming_sharded-1113"><a href="#masked_hamming_sharded-1113"><span class="linenos">1113</span></a>        <span class="k">if</span> <span class="n">remaining</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="masked_hamming_sharded-1114"><a href="#masked_hamming_sharded-1114"><span class="linenos">1114</span></a>            <span class="k">break</span>
</span><span id="masked_hamming_sharded-1115"><a href="#masked_hamming_sharded-1115"><span class="linenos">1115</span></a>        <span class="n">take_count</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">count</span><span class="p">,</span> <span class="n">remaining</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1116"><a href="#masked_hamming_sharded-1116"><span class="linenos">1116</span></a>        <span class="n">all_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">indices</span><span class="p">[:</span><span class="n">take_count</span><span class="p">])</span>
</span><span id="masked_hamming_sharded-1117"><a href="#masked_hamming_sharded-1117"><span class="linenos">1117</span></a>        <span class="n">all_categories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">categories</span><span class="p">[:</span><span class="n">take_count</span><span class="p">])</span>
</span><span id="masked_hamming_sharded-1118"><a href="#masked_hamming_sharded-1118"><span class="linenos">1118</span></a>        <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">distances</span><span class="p">[:</span><span class="n">take_count</span><span class="p">])</span>
</span><span id="masked_hamming_sharded-1119"><a href="#masked_hamming_sharded-1119"><span class="linenos">1119</span></a>        <span class="n">total_count</span> <span class="o">+=</span> <span class="n">take_count</span>
</span><span id="masked_hamming_sharded-1120"><a href="#masked_hamming_sharded-1120"><span class="linenos">1120</span></a>
</span><span id="masked_hamming_sharded-1121"><a href="#masked_hamming_sharded-1121"><span class="linenos">1121</span></a>    <span class="n">pair_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_indices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1122"><a href="#masked_hamming_sharded-1122"><span class="linenos">1122</span></a>    <span class="n">categories</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_categories</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1123"><a href="#masked_hamming_sharded-1123"><span class="linenos">1123</span></a>    <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_distances</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1124"><a href="#masked_hamming_sharded-1124"><span class="linenos">1124</span></a>    <span class="n">count</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">total_count</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span><span id="masked_hamming_sharded-1125"><a href="#masked_hamming_sharded-1125"><span class="linenos">1125</span></a>
</span><span id="masked_hamming_sharded-1126"><a href="#masked_hamming_sharded-1126"><span class="linenos">1126</span></a>    <span class="k">return</span> <span class="n">pair_indices</span><span class="p">,</span> <span class="n">categories</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">count</span>
</span></pre></div>


            <div class="docstring"><p>Sharded version of masked_hamming_cuda for multi-GPU and multi-host datasets.</p>

<p>Computes the lower triangle (i &gt; j) by tiling and distributing across devices.
Accepts either packed (int32) or unpacked (uint8) data - packing is done on GPU.</p>

<p>For multi-host operation:</p>

<ul>
<li>Each host should have the FULL data tensor (or be able to access all rows)</li>
<li>Set host_index to this host's index (0 to num_hosts-1)</li>
<li>Set num_hosts to total number of hosts</li>
<li>Each host will process only its assigned tiles</li>
<li>Results from all hosts should be aggregated by the caller</li>
</ul>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>data:</strong>  Tensor of shape [M, k_words] int32 (packed) OR
[M, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</li>
<li><strong>mask:</strong>  Same shape/dtype as data</li>
<li><strong>labels:</strong>  Optional int32 tensor of shape [M] with identity labels.</li>
<li><strong>match_threshold:</strong>  Threshold for match classification (default: 0.35)</li>
<li><strong>non_match_threshold:</strong>  Threshold for non-match classification (default: 0.35)</li>
<li><strong>is_similarity:</strong>  If True, higher values = more similar</li>
<li><strong>include_flags:</strong>  Bitmask of categories to include (default: INCLUDE_ALL)</li>
<li><strong>max_pairs:</strong>  Maximum total pairs to return (default: 1,000,000)</li>
<li><strong>dims:</strong>  Optional tuple (r_dim, theta_dim, d0_dim, d1_dim)</li>
<li><strong>r_dim, theta_dim, d0_dim, d1_dim:</strong>  Iris code dimensions</li>
<li><strong>min_shards:</strong>  Minimum number of shards (useful for testing on single GPU)</li>
<li><strong>max_tile_size:</strong>  Maximum rows per tile (None = auto based on memory)</li>
<li><strong>host_index:</strong>  Index of this host for multi-host operation (default: 0)</li>
<li><strong>num_hosts:</strong>  Total number of hosts for multi-host operation (default: 1)</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>Tuple of (pair_indices, categories, distances, count):</p>
  
  <ul>
  <li>pair_indices: [N, 2] int32 - (row, col) indices of pairs (row &gt; col)</li>
  <li>categories: [N] uint8 - category codes</li>
  <li>distances: [N] float32 - distance values</li>
  <li>count: [1] int32 - actual number of pairs</li>
  </ul>
</blockquote>

<h6 id="example">Example:</h6>

<blockquote>
  <p>Multi-GPU matching (automatically uses all available GPUs):</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cuda_iris_matcher</span> <span class="k">as</span> <span class="nn">ih</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Large dataset on CPU</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mh">0x7FFFFFFF</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Automatically distributes across GPUs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairs</span><span class="p">,</span> <span class="n">cats</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_sharded</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using </span><span class="si">{</span><span class="n">ih</span><span class="o">.</span><span class="n">get_device_count</span><span class="p">()</span><span class="si">}</span><span class="s2"> GPUs, found </span><span class="si">{</span><span class="n">count</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2"> pairs&quot;</span><span class="p">)</span>
</code></pre>
  </div>
  
  <p>Force tiling on single GPU (useful for memory-limited scenarios):</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="n">pairs</span><span class="p">,</span> <span class="n">cats</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_sharded</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">min_shards</span><span class="o">=</span><span class="mi">4</span>  <span class="c1"># Split into at least 4 tiles</span>
<span class="gp">... </span><span class="p">)</span>
</code></pre>
  </div>
  
  <p>Multi-host distributed computation:</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="c1"># On host 0 of 4:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result_0</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_sharded</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">host_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_hosts</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># On host 1 of 4:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result_1</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_sharded</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">host_index</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hosts</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ... aggregate results from all hosts</span>
</code></pre>
  </div>
</blockquote>
</div>


                </section>
                <section id="masked_hamming_ab_sharded">
                            <input id="masked_hamming_ab_sharded-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">masked_hamming_ab_sharded</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">data_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">data_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">mask_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">labels_a</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">labels_b</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span>,</span><span class="param">	<span class="n">non_match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span>,</span><span class="param">	<span class="n">is_similarity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>,</span><span class="param">	<span class="n">include_flags</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">15</span>,</span><span class="param">	<span class="n">max_pairs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000</span>,</span><span class="param">	<span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span>,</span><span class="param">	<span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">host_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>,</span><span class="param">	<span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span></span><span class="return-annotation">) -> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="masked_hamming_ab_sharded-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#masked_hamming_ab_sharded"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="masked_hamming_ab_sharded-519"><a href="#masked_hamming_ab_sharded-519"><span class="linenos">519</span></a><span class="k">def</span> <span class="nf">masked_hamming_ab_sharded</span><span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-520"><a href="#masked_hamming_ab_sharded-520"><span class="linenos">520</span></a>    <span class="n">data_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-521"><a href="#masked_hamming_ab_sharded-521"><span class="linenos">521</span></a>    <span class="n">mask_a</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-522"><a href="#masked_hamming_ab_sharded-522"><span class="linenos">522</span></a>    <span class="n">data_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-523"><a href="#masked_hamming_ab_sharded-523"><span class="linenos">523</span></a>    <span class="n">mask_b</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-524"><a href="#masked_hamming_ab_sharded-524"><span class="linenos">524</span></a>    <span class="n">labels_a</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-525"><a href="#masked_hamming_ab_sharded-525"><span class="linenos">525</span></a>    <span class="n">labels_b</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-526"><a href="#masked_hamming_ab_sharded-526"><span class="linenos">526</span></a>    <span class="n">match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-527"><a href="#masked_hamming_ab_sharded-527"><span class="linenos">527</span></a>    <span class="n">non_match_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.35</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-528"><a href="#masked_hamming_ab_sharded-528"><span class="linenos">528</span></a>    <span class="n">is_similarity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-529"><a href="#masked_hamming_ab_sharded-529"><span class="linenos">529</span></a>    <span class="n">include_flags</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">INCLUDE_ALL</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-530"><a href="#masked_hamming_ab_sharded-530"><span class="linenos">530</span></a>    <span class="n">max_pairs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1_000_000</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-531"><a href="#masked_hamming_ab_sharded-531"><span class="linenos">531</span></a>    <span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-532"><a href="#masked_hamming_ab_sharded-532"><span class="linenos">532</span></a>    <span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_R_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-533"><a href="#masked_hamming_ab_sharded-533"><span class="linenos">533</span></a>    <span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_THETA_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-534"><a href="#masked_hamming_ab_sharded-534"><span class="linenos">534</span></a>    <span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D0_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-535"><a href="#masked_hamming_ab_sharded-535"><span class="linenos">535</span></a>    <span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D1_DIM</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-536"><a href="#masked_hamming_ab_sharded-536"><span class="linenos">536</span></a>    <span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-537"><a href="#masked_hamming_ab_sharded-537"><span class="linenos">537</span></a>    <span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-538"><a href="#masked_hamming_ab_sharded-538"><span class="linenos">538</span></a>    <span class="n">host_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-539"><a href="#masked_hamming_ab_sharded-539"><span class="linenos">539</span></a>    <span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-540"><a href="#masked_hamming_ab_sharded-540"><span class="linenos">540</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
</span><span id="masked_hamming_ab_sharded-541"><a href="#masked_hamming_ab_sharded-541"><span class="linenos">541</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Sharded version of masked_hamming_ab_cuda for multi-GPU and multi-host datasets.</span>
</span><span id="masked_hamming_ab_sharded-542"><a href="#masked_hamming_ab_sharded-542"><span class="linenos">542</span></a>
</span><span id="masked_hamming_ab_sharded-543"><a href="#masked_hamming_ab_sharded-543"><span class="linenos">543</span></a><span class="sd">    Automatically distributes computation across all available CUDA devices and</span>
</span><span id="masked_hamming_ab_sharded-544"><a href="#masked_hamming_ab_sharded-544"><span class="linenos">544</span></a><span class="sd">    handles cases where A or B don&#39;t fit on a single device. Accepts either</span>
</span><span id="masked_hamming_ab_sharded-545"><a href="#masked_hamming_ab_sharded-545"><span class="linenos">545</span></a><span class="sd">    packed (int32) or unpacked (uint8) data - packing is done on GPU automatically.</span>
</span><span id="masked_hamming_ab_sharded-546"><a href="#masked_hamming_ab_sharded-546"><span class="linenos">546</span></a>
</span><span id="masked_hamming_ab_sharded-547"><a href="#masked_hamming_ab_sharded-547"><span class="linenos">547</span></a><span class="sd">    For multi-host operation:</span>
</span><span id="masked_hamming_ab_sharded-548"><a href="#masked_hamming_ab_sharded-548"><span class="linenos">548</span></a><span class="sd">    - Each host should have the FULL data tensors (or be able to access all rows)</span>
</span><span id="masked_hamming_ab_sharded-549"><a href="#masked_hamming_ab_sharded-549"><span class="linenos">549</span></a><span class="sd">    - Set host_index to this host&#39;s index (0 to num_hosts-1)</span>
</span><span id="masked_hamming_ab_sharded-550"><a href="#masked_hamming_ab_sharded-550"><span class="linenos">550</span></a><span class="sd">    - Set num_hosts to total number of hosts</span>
</span><span id="masked_hamming_ab_sharded-551"><a href="#masked_hamming_ab_sharded-551"><span class="linenos">551</span></a><span class="sd">    - Each host will process only its assigned tiles</span>
</span><span id="masked_hamming_ab_sharded-552"><a href="#masked_hamming_ab_sharded-552"><span class="linenos">552</span></a><span class="sd">    - Results from all hosts should be aggregated by the caller</span>
</span><span id="masked_hamming_ab_sharded-553"><a href="#masked_hamming_ab_sharded-553"><span class="linenos">553</span></a>
</span><span id="masked_hamming_ab_sharded-554"><a href="#masked_hamming_ab_sharded-554"><span class="linenos">554</span></a><span class="sd">    Args:</span>
</span><span id="masked_hamming_ab_sharded-555"><a href="#masked_hamming_ab_sharded-555"><span class="linenos">555</span></a><span class="sd">        data_a: Tensor of shape [M_A, k_words] int32 (packed) OR</span>
</span><span id="masked_hamming_ab_sharded-556"><a href="#masked_hamming_ab_sharded-556"><span class="linenos">556</span></a><span class="sd">                [M_A, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</span>
</span><span id="masked_hamming_ab_sharded-557"><a href="#masked_hamming_ab_sharded-557"><span class="linenos">557</span></a><span class="sd">        mask_a: Same shape/dtype as data_a</span>
</span><span id="masked_hamming_ab_sharded-558"><a href="#masked_hamming_ab_sharded-558"><span class="linenos">558</span></a><span class="sd">        data_b: Tensor of shape [M_B, k_words] int32 (packed) OR</span>
</span><span id="masked_hamming_ab_sharded-559"><a href="#masked_hamming_ab_sharded-559"><span class="linenos">559</span></a><span class="sd">                [M_B, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</span>
</span><span id="masked_hamming_ab_sharded-560"><a href="#masked_hamming_ab_sharded-560"><span class="linenos">560</span></a><span class="sd">        mask_b: Same shape/dtype as data_b</span>
</span><span id="masked_hamming_ab_sharded-561"><a href="#masked_hamming_ab_sharded-561"><span class="linenos">561</span></a><span class="sd">        labels_a: Optional int32 tensor of shape [M_A] with identity labels.</span>
</span><span id="masked_hamming_ab_sharded-562"><a href="#masked_hamming_ab_sharded-562"><span class="linenos">562</span></a><span class="sd">        labels_b: Optional int32 tensor of shape [M_B] with identity labels.</span>
</span><span id="masked_hamming_ab_sharded-563"><a href="#masked_hamming_ab_sharded-563"><span class="linenos">563</span></a><span class="sd">        match_threshold: Threshold for match classification (default: 0.35)</span>
</span><span id="masked_hamming_ab_sharded-564"><a href="#masked_hamming_ab_sharded-564"><span class="linenos">564</span></a><span class="sd">        non_match_threshold: Threshold for non-match classification (default: 0.35)</span>
</span><span id="masked_hamming_ab_sharded-565"><a href="#masked_hamming_ab_sharded-565"><span class="linenos">565</span></a><span class="sd">        is_similarity: If True, higher values = more similar</span>
</span><span id="masked_hamming_ab_sharded-566"><a href="#masked_hamming_ab_sharded-566"><span class="linenos">566</span></a><span class="sd">        include_flags: Bitmask of categories to include (default: INCLUDE_ALL)</span>
</span><span id="masked_hamming_ab_sharded-567"><a href="#masked_hamming_ab_sharded-567"><span class="linenos">567</span></a><span class="sd">        max_pairs: Maximum total pairs to return (default: 1,000,000)</span>
</span><span id="masked_hamming_ab_sharded-568"><a href="#masked_hamming_ab_sharded-568"><span class="linenos">568</span></a><span class="sd">        dims: Optional tuple (r_dim, theta_dim, d0_dim, d1_dim)</span>
</span><span id="masked_hamming_ab_sharded-569"><a href="#masked_hamming_ab_sharded-569"><span class="linenos">569</span></a><span class="sd">        r_dim, theta_dim, d0_dim, d1_dim: Iris code dimensions</span>
</span><span id="masked_hamming_ab_sharded-570"><a href="#masked_hamming_ab_sharded-570"><span class="linenos">570</span></a><span class="sd">        min_shards: Minimum number of shards (useful for testing on single GPU)</span>
</span><span id="masked_hamming_ab_sharded-571"><a href="#masked_hamming_ab_sharded-571"><span class="linenos">571</span></a><span class="sd">        max_tile_size: Maximum rows per tile (None = auto based on memory)</span>
</span><span id="masked_hamming_ab_sharded-572"><a href="#masked_hamming_ab_sharded-572"><span class="linenos">572</span></a><span class="sd">        host_index: Index of this host for multi-host operation (default: 0)</span>
</span><span id="masked_hamming_ab_sharded-573"><a href="#masked_hamming_ab_sharded-573"><span class="linenos">573</span></a><span class="sd">        num_hosts: Total number of hosts for multi-host operation (default: 1)</span>
</span><span id="masked_hamming_ab_sharded-574"><a href="#masked_hamming_ab_sharded-574"><span class="linenos">574</span></a>
</span><span id="masked_hamming_ab_sharded-575"><a href="#masked_hamming_ab_sharded-575"><span class="linenos">575</span></a><span class="sd">    Returns:</span>
</span><span id="masked_hamming_ab_sharded-576"><a href="#masked_hamming_ab_sharded-576"><span class="linenos">576</span></a><span class="sd">        Tuple of (pair_indices, categories, distances, count):</span>
</span><span id="masked_hamming_ab_sharded-577"><a href="#masked_hamming_ab_sharded-577"><span class="linenos">577</span></a><span class="sd">        - pair_indices: [N, 2] int32 - (row_in_A, row_in_B) indices of pairs</span>
</span><span id="masked_hamming_ab_sharded-578"><a href="#masked_hamming_ab_sharded-578"><span class="linenos">578</span></a><span class="sd">        - categories: [N] uint8 - category codes</span>
</span><span id="masked_hamming_ab_sharded-579"><a href="#masked_hamming_ab_sharded-579"><span class="linenos">579</span></a><span class="sd">        - distances: [N] float32 - distance values</span>
</span><span id="masked_hamming_ab_sharded-580"><a href="#masked_hamming_ab_sharded-580"><span class="linenos">580</span></a><span class="sd">        - count: [1] int32 - actual number of pairs</span>
</span><span id="masked_hamming_ab_sharded-581"><a href="#masked_hamming_ab_sharded-581"><span class="linenos">581</span></a>
</span><span id="masked_hamming_ab_sharded-582"><a href="#masked_hamming_ab_sharded-582"><span class="linenos">582</span></a><span class="sd">    Example:</span>
</span><span id="masked_hamming_ab_sharded-583"><a href="#masked_hamming_ab_sharded-583"><span class="linenos">583</span></a><span class="sd">        Large-scale gallery vs probe matching across multiple GPUs:</span>
</span><span id="masked_hamming_ab_sharded-584"><a href="#masked_hamming_ab_sharded-584"><span class="linenos">584</span></a>
</span><span id="masked_hamming_ab_sharded-585"><a href="#masked_hamming_ab_sharded-585"><span class="linenos">585</span></a><span class="sd">        &gt;&gt;&gt; import torch</span>
</span><span id="masked_hamming_ab_sharded-586"><a href="#masked_hamming_ab_sharded-586"><span class="linenos">586</span></a><span class="sd">        &gt;&gt;&gt; import cuda_iris_matcher as ih</span>
</span><span id="masked_hamming_ab_sharded-587"><a href="#masked_hamming_ab_sharded-587"><span class="linenos">587</span></a><span class="sd">        &gt;&gt;&gt; # Large gallery (100K enrolled users) and probe set (1K queries)</span>
</span><span id="masked_hamming_ab_sharded-588"><a href="#masked_hamming_ab_sharded-588"><span class="linenos">588</span></a><span class="sd">        &gt;&gt;&gt; gallery = torch.randint(0, 2**31, (100000, 400), dtype=torch.int32)</span>
</span><span id="masked_hamming_ab_sharded-589"><a href="#masked_hamming_ab_sharded-589"><span class="linenos">589</span></a><span class="sd">        &gt;&gt;&gt; gallery_mask = torch.full_like(gallery, 0x7FFFFFFF)</span>
</span><span id="masked_hamming_ab_sharded-590"><a href="#masked_hamming_ab_sharded-590"><span class="linenos">590</span></a><span class="sd">        &gt;&gt;&gt; probe = torch.randint(0, 2**31, (1000, 400), dtype=torch.int32)</span>
</span><span id="masked_hamming_ab_sharded-591"><a href="#masked_hamming_ab_sharded-591"><span class="linenos">591</span></a><span class="sd">        &gt;&gt;&gt; probe_mask = torch.full_like(probe, 0x7FFFFFFF)</span>
</span><span id="masked_hamming_ab_sharded-592"><a href="#masked_hamming_ab_sharded-592"><span class="linenos">592</span></a><span class="sd">        &gt;&gt;&gt; # Distributes 100M comparisons across all GPUs</span>
</span><span id="masked_hamming_ab_sharded-593"><a href="#masked_hamming_ab_sharded-593"><span class="linenos">593</span></a><span class="sd">        &gt;&gt;&gt; pairs, _, dists, count = ih.masked_hamming_ab_sharded(</span>
</span><span id="masked_hamming_ab_sharded-594"><a href="#masked_hamming_ab_sharded-594"><span class="linenos">594</span></a><span class="sd">        ...     gallery, gallery_mask, probe, probe_mask</span>
</span><span id="masked_hamming_ab_sharded-595"><a href="#masked_hamming_ab_sharded-595"><span class="linenos">595</span></a><span class="sd">        ... )</span>
</span><span id="masked_hamming_ab_sharded-596"><a href="#masked_hamming_ab_sharded-596"><span class="linenos">596</span></a><span class="sd">        &gt;&gt;&gt; print(f&quot;Completed {count.item()} comparisons&quot;)</span>
</span><span id="masked_hamming_ab_sharded-597"><a href="#masked_hamming_ab_sharded-597"><span class="linenos">597</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="masked_hamming_ab_sharded-598"><a href="#masked_hamming_ab_sharded-598"><span class="linenos">598</span></a>    <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span> <span class="o">=</span> <span class="n">_resolve_dims</span><span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-599"><a href="#masked_hamming_ab_sharded-599"><span class="linenos">599</span></a>        <span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span>
</span><span id="masked_hamming_ab_sharded-600"><a href="#masked_hamming_ab_sharded-600"><span class="linenos">600</span></a>    <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-601"><a href="#masked_hamming_ab_sharded-601"><span class="linenos">601</span></a>    <span class="n">k_words</span> <span class="o">=</span> <span class="n">r_dim</span> <span class="o">*</span> <span class="n">theta_dim</span> <span class="o">*</span> <span class="n">d0_dim</span> <span class="o">*</span> <span class="n">d1_dim</span> <span class="o">//</span> <span class="mi">32</span>
</span><span id="masked_hamming_ab_sharded-602"><a href="#masked_hamming_ab_sharded-602"><span class="linenos">602</span></a>
</span><span id="masked_hamming_ab_sharded-603"><a href="#masked_hamming_ab_sharded-603"><span class="linenos">603</span></a>    <span class="n">m_a</span> <span class="o">=</span> <span class="n">data_a</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-604"><a href="#masked_hamming_ab_sharded-604"><span class="linenos">604</span></a>    <span class="n">m_b</span> <span class="o">=</span> <span class="n">data_b</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-605"><a href="#masked_hamming_ab_sharded-605"><span class="linenos">605</span></a>
</span><span id="masked_hamming_ab_sharded-606"><a href="#masked_hamming_ab_sharded-606"><span class="linenos">606</span></a>    <span class="c1"># Determine number of devices</span>
</span><span id="masked_hamming_ab_sharded-607"><a href="#masked_hamming_ab_sharded-607"><span class="linenos">607</span></a>    <span class="n">num_devices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
</span><span id="masked_hamming_ab_sharded-608"><a href="#masked_hamming_ab_sharded-608"><span class="linenos">608</span></a>    <span class="k">if</span> <span class="n">num_devices</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-609"><a href="#masked_hamming_ab_sharded-609"><span class="linenos">609</span></a>        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;No CUDA devices available&quot;</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-610"><a href="#masked_hamming_ab_sharded-610"><span class="linenos">610</span></a>
</span><span id="masked_hamming_ab_sharded-611"><a href="#masked_hamming_ab_sharded-611"><span class="linenos">611</span></a>    <span class="c1"># Check if data is packed or unpacked</span>
</span><span id="masked_hamming_ab_sharded-612"><a href="#masked_hamming_ab_sharded-612"><span class="linenos">612</span></a>    <span class="n">data_a_is_packed</span> <span class="o">=</span> <span class="n">_is_packed</span><span class="p">(</span><span class="n">data_a</span><span class="p">,</span> <span class="n">k_words</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-613"><a href="#masked_hamming_ab_sharded-613"><span class="linenos">613</span></a>    <span class="n">data_b_is_packed</span> <span class="o">=</span> <span class="n">_is_packed</span><span class="p">(</span><span class="n">data_b</span><span class="p">,</span> <span class="n">k_words</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-614"><a href="#masked_hamming_ab_sharded-614"><span class="linenos">614</span></a>    <span class="n">mask_a_is_packed</span> <span class="o">=</span> <span class="n">_is_packed</span><span class="p">(</span><span class="n">mask_a</span><span class="p">,</span> <span class="n">k_words</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-615"><a href="#masked_hamming_ab_sharded-615"><span class="linenos">615</span></a>    <span class="n">mask_b_is_packed</span> <span class="o">=</span> <span class="n">_is_packed</span><span class="p">(</span><span class="n">mask_b</span><span class="p">,</span> <span class="n">k_words</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-616"><a href="#masked_hamming_ab_sharded-616"><span class="linenos">616</span></a>
</span><span id="masked_hamming_ab_sharded-617"><a href="#masked_hamming_ab_sharded-617"><span class="linenos">617</span></a>    <span class="c1"># For single GPU without forced sharding, use non-sharded kernel directly</span>
</span><span id="masked_hamming_ab_sharded-618"><a href="#masked_hamming_ab_sharded-618"><span class="linenos">618</span></a>    <span class="k">if</span> <span class="n">num_devices</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">min_shards</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-619"><a href="#masked_hamming_ab_sharded-619"><span class="linenos">619</span></a>        <span class="n">primary_device</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="masked_hamming_ab_sharded-620"><a href="#masked_hamming_ab_sharded-620"><span class="linenos">620</span></a>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">primary_device</span><span class="p">):</span>
</span><span id="masked_hamming_ab_sharded-621"><a href="#masked_hamming_ab_sharded-621"><span class="linenos">621</span></a>            <span class="k">if</span> <span class="n">data_a_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-622"><a href="#masked_hamming_ab_sharded-622"><span class="linenos">622</span></a>                <span class="n">data_a_gpu</span> <span class="o">=</span> <span class="n">data_a</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">data_a</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">data_a</span>
</span><span id="masked_hamming_ab_sharded-623"><a href="#masked_hamming_ab_sharded-623"><span class="linenos">623</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-624"><a href="#masked_hamming_ab_sharded-624"><span class="linenos">624</span></a>                <span class="n">data_a_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">data_a</span><span class="p">,</span> <span class="n">primary_device</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-625"><a href="#masked_hamming_ab_sharded-625"><span class="linenos">625</span></a>            <span class="k">if</span> <span class="n">mask_a_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-626"><a href="#masked_hamming_ab_sharded-626"><span class="linenos">626</span></a>                <span class="n">mask_a_gpu</span> <span class="o">=</span> <span class="n">mask_a</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">mask_a</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">mask_a</span>
</span><span id="masked_hamming_ab_sharded-627"><a href="#masked_hamming_ab_sharded-627"><span class="linenos">627</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-628"><a href="#masked_hamming_ab_sharded-628"><span class="linenos">628</span></a>                <span class="n">mask_a_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">mask_a</span><span class="p">,</span> <span class="n">primary_device</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-629"><a href="#masked_hamming_ab_sharded-629"><span class="linenos">629</span></a>            <span class="k">if</span> <span class="n">data_b_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-630"><a href="#masked_hamming_ab_sharded-630"><span class="linenos">630</span></a>                <span class="n">data_b_gpu</span> <span class="o">=</span> <span class="n">data_b</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">data_b</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">data_b</span>
</span><span id="masked_hamming_ab_sharded-631"><a href="#masked_hamming_ab_sharded-631"><span class="linenos">631</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-632"><a href="#masked_hamming_ab_sharded-632"><span class="linenos">632</span></a>                <span class="n">data_b_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">data_b</span><span class="p">,</span> <span class="n">primary_device</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-633"><a href="#masked_hamming_ab_sharded-633"><span class="linenos">633</span></a>            <span class="k">if</span> <span class="n">mask_b_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-634"><a href="#masked_hamming_ab_sharded-634"><span class="linenos">634</span></a>                <span class="n">mask_b_gpu</span> <span class="o">=</span> <span class="n">mask_b</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">mask_b</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">mask_b</span>
</span><span id="masked_hamming_ab_sharded-635"><a href="#masked_hamming_ab_sharded-635"><span class="linenos">635</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-636"><a href="#masked_hamming_ab_sharded-636"><span class="linenos">636</span></a>                <span class="n">mask_b_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">mask_b</span><span class="p">,</span> <span class="n">primary_device</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-637"><a href="#masked_hamming_ab_sharded-637"><span class="linenos">637</span></a>        <span class="n">labels_a_gpu</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="masked_hamming_ab_sharded-638"><a href="#masked_hamming_ab_sharded-638"><span class="linenos">638</span></a>        <span class="n">labels_b_gpu</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="masked_hamming_ab_sharded-639"><a href="#masked_hamming_ab_sharded-639"><span class="linenos">639</span></a>        <span class="k">if</span> <span class="n">labels_a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-640"><a href="#masked_hamming_ab_sharded-640"><span class="linenos">640</span></a>            <span class="n">labels_a_gpu</span> <span class="o">=</span> <span class="n">labels_a</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">labels_a</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">labels_a</span>
</span><span id="masked_hamming_ab_sharded-641"><a href="#masked_hamming_ab_sharded-641"><span class="linenos">641</span></a>        <span class="k">if</span> <span class="n">labels_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-642"><a href="#masked_hamming_ab_sharded-642"><span class="linenos">642</span></a>            <span class="n">labels_b_gpu</span> <span class="o">=</span> <span class="n">labels_b</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">primary_device</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">labels_b</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">labels_b</span>
</span><span id="masked_hamming_ab_sharded-643"><a href="#masked_hamming_ab_sharded-643"><span class="linenos">643</span></a>        <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">masked_hamming_ab_cuda</span><span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-644"><a href="#masked_hamming_ab_sharded-644"><span class="linenos">644</span></a>            <span class="n">data_a_gpu</span><span class="p">,</span> <span class="n">mask_a_gpu</span><span class="p">,</span> <span class="n">data_b_gpu</span><span class="p">,</span> <span class="n">mask_b_gpu</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-645"><a href="#masked_hamming_ab_sharded-645"><span class="linenos">645</span></a>            <span class="n">labels_a_gpu</span><span class="p">,</span> <span class="n">labels_b_gpu</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-646"><a href="#masked_hamming_ab_sharded-646"><span class="linenos">646</span></a>            <span class="n">match_threshold</span><span class="p">,</span> <span class="n">non_match_threshold</span><span class="p">,</span> <span class="n">is_similarity</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-647"><a href="#masked_hamming_ab_sharded-647"><span class="linenos">647</span></a>            <span class="n">include_flags</span><span class="p">,</span> <span class="n">max_pairs</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span>
</span><span id="masked_hamming_ab_sharded-648"><a href="#masked_hamming_ab_sharded-648"><span class="linenos">648</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-649"><a href="#masked_hamming_ab_sharded-649"><span class="linenos">649</span></a>
</span><span id="masked_hamming_ab_sharded-650"><a href="#masked_hamming_ab_sharded-650"><span class="linenos">650</span></a>    <span class="c1"># Compute shard configurations</span>
</span><span id="masked_hamming_ab_sharded-651"><a href="#masked_hamming_ab_sharded-651"><span class="linenos">651</span></a>    <span class="c1"># For multi-host, compute shards as if all hosts&#39; GPUs were available</span>
</span><span id="masked_hamming_ab_sharded-652"><a href="#masked_hamming_ab_sharded-652"><span class="linenos">652</span></a>    <span class="n">total_devices</span> <span class="o">=</span> <span class="n">num_devices</span> <span class="o">*</span> <span class="n">num_hosts</span>
</span><span id="masked_hamming_ab_sharded-653"><a href="#masked_hamming_ab_sharded-653"><span class="linenos">653</span></a>    <span class="n">all_shards</span> <span class="o">=</span> <span class="n">_compute_shard_configs</span><span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-654"><a href="#masked_hamming_ab_sharded-654"><span class="linenos">654</span></a>        <span class="n">m_a</span><span class="p">,</span> <span class="n">m_b</span><span class="p">,</span> <span class="n">k_words</span><span class="p">,</span> <span class="n">max_pairs</span><span class="p">,</span> <span class="n">total_devices</span><span class="p">,</span> <span class="n">min_shards</span> <span class="o">*</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">max_tile_size</span>
</span><span id="masked_hamming_ab_sharded-655"><a href="#masked_hamming_ab_sharded-655"><span class="linenos">655</span></a>    <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-656"><a href="#masked_hamming_ab_sharded-656"><span class="linenos">656</span></a>
</span><span id="masked_hamming_ab_sharded-657"><a href="#masked_hamming_ab_sharded-657"><span class="linenos">657</span></a>    <span class="c1"># Filter to only shards assigned to this host</span>
</span><span id="masked_hamming_ab_sharded-658"><a href="#masked_hamming_ab_sharded-658"><span class="linenos">658</span></a>    <span class="n">shards</span> <span class="o">=</span> <span class="n">_filter_shards_for_host</span><span class="p">(</span><span class="n">all_shards</span><span class="p">,</span> <span class="n">host_index</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-659"><a href="#masked_hamming_ab_sharded-659"><span class="linenos">659</span></a>
</span><span id="masked_hamming_ab_sharded-660"><a href="#masked_hamming_ab_sharded-660"><span class="linenos">660</span></a>    <span class="c1"># If no shards assigned to this host, return empty results</span>
</span><span id="masked_hamming_ab_sharded-661"><a href="#masked_hamming_ab_sharded-661"><span class="linenos">661</span></a>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-662"><a href="#masked_hamming_ab_sharded-662"><span class="linenos">662</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-663"><a href="#masked_hamming_ab_sharded-663"><span class="linenos">663</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
</span><span id="masked_hamming_ab_sharded-664"><a href="#masked_hamming_ab_sharded-664"><span class="linenos">664</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span>
</span><span id="masked_hamming_ab_sharded-665"><a href="#masked_hamming_ab_sharded-665"><span class="linenos">665</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span><span id="masked_hamming_ab_sharded-666"><a href="#masked_hamming_ab_sharded-666"><span class="linenos">666</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
</span><span id="masked_hamming_ab_sharded-667"><a href="#masked_hamming_ab_sharded-667"><span class="linenos">667</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-668"><a href="#masked_hamming_ab_sharded-668"><span class="linenos">668</span></a>
</span><span id="masked_hamming_ab_sharded-669"><a href="#masked_hamming_ab_sharded-669"><span class="linenos">669</span></a>    <span class="n">max_pairs_per_shard</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_pairs</span> <span class="o">//</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_shards</span><span class="p">)),</span> <span class="mi">1000</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-670"><a href="#masked_hamming_ab_sharded-670"><span class="linenos">670</span></a>
</span><span id="masked_hamming_ab_sharded-671"><a href="#masked_hamming_ab_sharded-671"><span class="linenos">671</span></a>    <span class="c1"># For labels, keep on CPU for now (small)</span>
</span><span id="masked_hamming_ab_sharded-672"><a href="#masked_hamming_ab_sharded-672"><span class="linenos">672</span></a>    <span class="n">labels_a_cpu</span> <span class="o">=</span> <span class="n">labels_a</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">labels_a</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">labels_a</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">labels_a</span>
</span><span id="masked_hamming_ab_sharded-673"><a href="#masked_hamming_ab_sharded-673"><span class="linenos">673</span></a>    <span class="n">labels_b_cpu</span> <span class="o">=</span> <span class="n">labels_b</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">labels_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">labels_b</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">labels_b</span>
</span><span id="masked_hamming_ab_sharded-674"><a href="#masked_hamming_ab_sharded-674"><span class="linenos">674</span></a>
</span><span id="masked_hamming_ab_sharded-675"><a href="#masked_hamming_ab_sharded-675"><span class="linenos">675</span></a>    <span class="c1"># Ensure data is on CPU for direct transfers to each GPU</span>
</span><span id="masked_hamming_ab_sharded-676"><a href="#masked_hamming_ab_sharded-676"><span class="linenos">676</span></a>    <span class="n">data_a_cpu</span> <span class="o">=</span> <span class="n">data_a</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">data_a</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">data_a</span>
</span><span id="masked_hamming_ab_sharded-677"><a href="#masked_hamming_ab_sharded-677"><span class="linenos">677</span></a>    <span class="n">mask_a_cpu</span> <span class="o">=</span> <span class="n">mask_a</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">mask_a</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">mask_a</span>
</span><span id="masked_hamming_ab_sharded-678"><a href="#masked_hamming_ab_sharded-678"><span class="linenos">678</span></a>    <span class="n">data_b_cpu</span> <span class="o">=</span> <span class="n">data_b</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">data_b</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">data_b</span>
</span><span id="masked_hamming_ab_sharded-679"><a href="#masked_hamming_ab_sharded-679"><span class="linenos">679</span></a>    <span class="n">mask_b_cpu</span> <span class="o">=</span> <span class="n">mask_b</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">if</span> <span class="n">mask_b</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="n">mask_b</span>
</span><span id="masked_hamming_ab_sharded-680"><a href="#masked_hamming_ab_sharded-680"><span class="linenos">680</span></a>
</span><span id="masked_hamming_ab_sharded-681"><a href="#masked_hamming_ab_sharded-681"><span class="linenos">681</span></a>    <span class="c1"># Determine which row ranges each GPU needs for A and B</span>
</span><span id="masked_hamming_ab_sharded-682"><a href="#masked_hamming_ab_sharded-682"><span class="linenos">682</span></a>    <span class="n">device_ids</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">device_id</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">shards</span><span class="p">))</span>
</span><span id="masked_hamming_ab_sharded-683"><a href="#masked_hamming_ab_sharded-683"><span class="linenos">683</span></a>    <span class="n">device_a_ranges</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">set</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{</span><span class="n">d</span><span class="p">:</span> <span class="nb">set</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">device_ids</span><span class="p">}</span>
</span><span id="masked_hamming_ab_sharded-684"><a href="#masked_hamming_ab_sharded-684"><span class="linenos">684</span></a>    <span class="n">device_b_ranges</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">set</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{</span><span class="n">d</span><span class="p">:</span> <span class="nb">set</span><span class="p">()</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">device_ids</span><span class="p">}</span>
</span><span id="masked_hamming_ab_sharded-685"><a href="#masked_hamming_ab_sharded-685"><span class="linenos">685</span></a>    <span class="k">for</span> <span class="n">shard</span> <span class="ow">in</span> <span class="n">shards</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-686"><a href="#masked_hamming_ab_sharded-686"><span class="linenos">686</span></a>        <span class="n">device_a_ranges</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">shard</span><span class="o">.</span><span class="n">a_start</span><span class="p">,</span> <span class="n">shard</span><span class="o">.</span><span class="n">a_end</span><span class="p">))</span>
</span><span id="masked_hamming_ab_sharded-687"><a href="#masked_hamming_ab_sharded-687"><span class="linenos">687</span></a>        <span class="n">device_b_ranges</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">]</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">shard</span><span class="o">.</span><span class="n">b_start</span><span class="p">,</span> <span class="n">shard</span><span class="o">.</span><span class="n">b_end</span><span class="p">))</span>
</span><span id="masked_hamming_ab_sharded-688"><a href="#masked_hamming_ab_sharded-688"><span class="linenos">688</span></a>
</span><span id="masked_hamming_ab_sharded-689"><a href="#masked_hamming_ab_sharded-689"><span class="linenos">689</span></a>    <span class="k">def</span> <span class="nf">merge_ranges</span><span class="p">(</span><span class="n">ranges</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
</span><span id="masked_hamming_ab_sharded-690"><a href="#masked_hamming_ab_sharded-690"><span class="linenos">690</span></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">ranges</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-691"><a href="#masked_hamming_ab_sharded-691"><span class="linenos">691</span></a>            <span class="k">return</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-692"><a href="#masked_hamming_ab_sharded-692"><span class="linenos">692</span></a>        <span class="k">return</span> <span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ranges</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ranges</span><span class="p">))</span>
</span><span id="masked_hamming_ab_sharded-693"><a href="#masked_hamming_ab_sharded-693"><span class="linenos">693</span></a>
</span><span id="masked_hamming_ab_sharded-694"><a href="#masked_hamming_ab_sharded-694"><span class="linenos">694</span></a>    <span class="n">device_a_row_ranges</span> <span class="o">=</span> <span class="p">{</span><span class="n">d</span><span class="p">:</span> <span class="n">merge_ranges</span><span class="p">(</span><span class="n">ranges</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">ranges</span> <span class="ow">in</span> <span class="n">device_a_ranges</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span><span id="masked_hamming_ab_sharded-695"><a href="#masked_hamming_ab_sharded-695"><span class="linenos">695</span></a>    <span class="n">device_b_row_ranges</span> <span class="o">=</span> <span class="p">{</span><span class="n">d</span><span class="p">:</span> <span class="n">merge_ranges</span><span class="p">(</span><span class="n">ranges</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">ranges</span> <span class="ow">in</span> <span class="n">device_b_ranges</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</span><span id="masked_hamming_ab_sharded-696"><a href="#masked_hamming_ab_sharded-696"><span class="linenos">696</span></a>
</span><span id="masked_hamming_ab_sharded-697"><a href="#masked_hamming_ab_sharded-697"><span class="linenos">697</span></a>    <span class="c1"># Transfer data directly from CPU to each GPU in parallel</span>
</span><span id="masked_hamming_ab_sharded-698"><a href="#masked_hamming_ab_sharded-698"><span class="linenos">698</span></a>    <span class="k">def</span> <span class="nf">transfer_to_device</span><span class="p">(</span><span class="n">device_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
</span><span id="masked_hamming_ab_sharded-699"><a href="#masked_hamming_ab_sharded-699"><span class="linenos">699</span></a>        <span class="n">a_start</span><span class="p">,</span> <span class="n">a_end</span> <span class="o">=</span> <span class="n">device_a_row_ranges</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-700"><a href="#masked_hamming_ab_sharded-700"><span class="linenos">700</span></a>        <span class="n">b_start</span><span class="p">,</span> <span class="n">b_end</span> <span class="o">=</span> <span class="n">device_b_row_ranges</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-701"><a href="#masked_hamming_ab_sharded-701"><span class="linenos">701</span></a>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_id</span><span class="p">):</span>
</span><span id="masked_hamming_ab_sharded-702"><a href="#masked_hamming_ab_sharded-702"><span class="linenos">702</span></a>            <span class="c1"># Transfer A slices</span>
</span><span id="masked_hamming_ab_sharded-703"><a href="#masked_hamming_ab_sharded-703"><span class="linenos">703</span></a>            <span class="n">data_a_slice</span> <span class="o">=</span> <span class="n">data_a_cpu</span><span class="p">[</span><span class="n">a_start</span><span class="p">:</span><span class="n">a_end</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-704"><a href="#masked_hamming_ab_sharded-704"><span class="linenos">704</span></a>            <span class="n">mask_a_slice</span> <span class="o">=</span> <span class="n">mask_a_cpu</span><span class="p">[</span><span class="n">a_start</span><span class="p">:</span><span class="n">a_end</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-705"><a href="#masked_hamming_ab_sharded-705"><span class="linenos">705</span></a>            <span class="k">if</span> <span class="n">data_a_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-706"><a href="#masked_hamming_ab_sharded-706"><span class="linenos">706</span></a>                <span class="n">data_a_gpu</span> <span class="o">=</span> <span class="n">data_a_slice</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-707"><a href="#masked_hamming_ab_sharded-707"><span class="linenos">707</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-708"><a href="#masked_hamming_ab_sharded-708"><span class="linenos">708</span></a>                <span class="n">data_a_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">data_a_slice</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-709"><a href="#masked_hamming_ab_sharded-709"><span class="linenos">709</span></a>            <span class="k">if</span> <span class="n">mask_a_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-710"><a href="#masked_hamming_ab_sharded-710"><span class="linenos">710</span></a>                <span class="n">mask_a_gpu</span> <span class="o">=</span> <span class="n">mask_a_slice</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-711"><a href="#masked_hamming_ab_sharded-711"><span class="linenos">711</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-712"><a href="#masked_hamming_ab_sharded-712"><span class="linenos">712</span></a>                <span class="n">mask_a_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">mask_a_slice</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-713"><a href="#masked_hamming_ab_sharded-713"><span class="linenos">713</span></a>
</span><span id="masked_hamming_ab_sharded-714"><a href="#masked_hamming_ab_sharded-714"><span class="linenos">714</span></a>            <span class="c1"># Transfer B slices</span>
</span><span id="masked_hamming_ab_sharded-715"><a href="#masked_hamming_ab_sharded-715"><span class="linenos">715</span></a>            <span class="n">data_b_slice</span> <span class="o">=</span> <span class="n">data_b_cpu</span><span class="p">[</span><span class="n">b_start</span><span class="p">:</span><span class="n">b_end</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-716"><a href="#masked_hamming_ab_sharded-716"><span class="linenos">716</span></a>            <span class="n">mask_b_slice</span> <span class="o">=</span> <span class="n">mask_b_cpu</span><span class="p">[</span><span class="n">b_start</span><span class="p">:</span><span class="n">b_end</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-717"><a href="#masked_hamming_ab_sharded-717"><span class="linenos">717</span></a>            <span class="k">if</span> <span class="n">data_b_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-718"><a href="#masked_hamming_ab_sharded-718"><span class="linenos">718</span></a>                <span class="n">data_b_gpu</span> <span class="o">=</span> <span class="n">data_b_slice</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-719"><a href="#masked_hamming_ab_sharded-719"><span class="linenos">719</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-720"><a href="#masked_hamming_ab_sharded-720"><span class="linenos">720</span></a>                <span class="n">data_b_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">data_b_slice</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-721"><a href="#masked_hamming_ab_sharded-721"><span class="linenos">721</span></a>            <span class="k">if</span> <span class="n">mask_b_is_packed</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-722"><a href="#masked_hamming_ab_sharded-722"><span class="linenos">722</span></a>                <span class="n">mask_b_gpu</span> <span class="o">=</span> <span class="n">mask_b_slice</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-723"><a href="#masked_hamming_ab_sharded-723"><span class="linenos">723</span></a>            <span class="k">else</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-724"><a href="#masked_hamming_ab_sharded-724"><span class="linenos">724</span></a>                <span class="n">mask_b_gpu</span> <span class="o">=</span> <span class="n">_pack_on_device</span><span class="p">(</span><span class="n">mask_b_slice</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-725"><a href="#masked_hamming_ab_sharded-725"><span class="linenos">725</span></a>
</span><span id="masked_hamming_ab_sharded-726"><a href="#masked_hamming_ab_sharded-726"><span class="linenos">726</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-727"><a href="#masked_hamming_ab_sharded-727"><span class="linenos">727</span></a>        <span class="k">return</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">data_a_gpu</span><span class="p">,</span> <span class="n">mask_a_gpu</span><span class="p">,</span> <span class="n">data_b_gpu</span><span class="p">,</span> <span class="n">mask_b_gpu</span><span class="p">,</span> <span class="n">a_start</span><span class="p">,</span> <span class="n">b_start</span>
</span><span id="masked_hamming_ab_sharded-728"><a href="#masked_hamming_ab_sharded-728"><span class="linenos">728</span></a>
</span><span id="masked_hamming_ab_sharded-729"><a href="#masked_hamming_ab_sharded-729"><span class="linenos">729</span></a>    <span class="c1"># Transfer to all GPUs in parallel</span>
</span><span id="masked_hamming_ab_sharded-730"><a href="#masked_hamming_ab_sharded-730"><span class="linenos">730</span></a>    <span class="n">device_data</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
</span><span id="masked_hamming_ab_sharded-731"><a href="#masked_hamming_ab_sharded-731"><span class="linenos">731</span></a>    <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">device_ids</span><span class="p">))</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-732"><a href="#masked_hamming_ab_sharded-732"><span class="linenos">732</span></a>        <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">transfer_to_device</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">device_ids</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-733"><a href="#masked_hamming_ab_sharded-733"><span class="linenos">733</span></a>        <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">as_completed</span><span class="p">(</span><span class="n">futures</span><span class="p">):</span>
</span><span id="masked_hamming_ab_sharded-734"><a href="#masked_hamming_ab_sharded-734"><span class="linenos">734</span></a>            <span class="n">device_id</span><span class="p">,</span> <span class="n">da</span><span class="p">,</span> <span class="n">ma</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">mb</span><span class="p">,</span> <span class="n">a_off</span><span class="p">,</span> <span class="n">b_off</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
</span><span id="masked_hamming_ab_sharded-735"><a href="#masked_hamming_ab_sharded-735"><span class="linenos">735</span></a>            <span class="n">device_data</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">da</span><span class="p">,</span> <span class="n">ma</span><span class="p">,</span> <span class="n">db</span><span class="p">,</span> <span class="n">mb</span><span class="p">,</span> <span class="n">a_off</span><span class="p">,</span> <span class="n">b_off</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-736"><a href="#masked_hamming_ab_sharded-736"><span class="linenos">736</span></a>
</span><span id="masked_hamming_ab_sharded-737"><a href="#masked_hamming_ab_sharded-737"><span class="linenos">737</span></a>    <span class="c1"># Define kernel launch function</span>
</span><span id="masked_hamming_ab_sharded-738"><a href="#masked_hamming_ab_sharded-738"><span class="linenos">738</span></a>    <span class="k">def</span> <span class="nf">launch_kernel</span><span class="p">(</span><span class="n">shard</span><span class="p">:</span> <span class="n">ShardConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ShardKernelResult</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-739"><a href="#masked_hamming_ab_sharded-739"><span class="linenos">739</span></a>        <span class="n">data_a_gpu</span><span class="p">,</span> <span class="n">mask_a_gpu</span><span class="p">,</span> <span class="n">data_b_gpu</span><span class="p">,</span> <span class="n">mask_b_gpu</span><span class="p">,</span> <span class="n">a_offset</span><span class="p">,</span> <span class="n">b_offset</span> <span class="o">=</span> <span class="n">device_data</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-740"><a href="#masked_hamming_ab_sharded-740"><span class="linenos">740</span></a>
</span><span id="masked_hamming_ab_sharded-741"><a href="#masked_hamming_ab_sharded-741"><span class="linenos">741</span></a>        <span class="c1"># Adjust indices relative to the data slices on this device</span>
</span><span id="masked_hamming_ab_sharded-742"><a href="#masked_hamming_ab_sharded-742"><span class="linenos">742</span></a>        <span class="n">local_a_start</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">a_start</span> <span class="o">-</span> <span class="n">a_offset</span>
</span><span id="masked_hamming_ab_sharded-743"><a href="#masked_hamming_ab_sharded-743"><span class="linenos">743</span></a>        <span class="n">local_a_end</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">a_end</span> <span class="o">-</span> <span class="n">a_offset</span>
</span><span id="masked_hamming_ab_sharded-744"><a href="#masked_hamming_ab_sharded-744"><span class="linenos">744</span></a>        <span class="n">local_b_start</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">b_start</span> <span class="o">-</span> <span class="n">b_offset</span>
</span><span id="masked_hamming_ab_sharded-745"><a href="#masked_hamming_ab_sharded-745"><span class="linenos">745</span></a>        <span class="n">local_b_end</span> <span class="o">=</span> <span class="n">shard</span><span class="o">.</span><span class="n">b_end</span> <span class="o">-</span> <span class="n">b_offset</span>
</span><span id="masked_hamming_ab_sharded-746"><a href="#masked_hamming_ab_sharded-746"><span class="linenos">746</span></a>
</span><span id="masked_hamming_ab_sharded-747"><a href="#masked_hamming_ab_sharded-747"><span class="linenos">747</span></a>        <span class="n">data_a_tile</span> <span class="o">=</span> <span class="n">data_a_gpu</span><span class="p">[</span><span class="n">local_a_start</span><span class="p">:</span><span class="n">local_a_end</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-748"><a href="#masked_hamming_ab_sharded-748"><span class="linenos">748</span></a>        <span class="n">mask_a_tile</span> <span class="o">=</span> <span class="n">mask_a_gpu</span><span class="p">[</span><span class="n">local_a_start</span><span class="p">:</span><span class="n">local_a_end</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-749"><a href="#masked_hamming_ab_sharded-749"><span class="linenos">749</span></a>        <span class="n">data_b_tile</span> <span class="o">=</span> <span class="n">data_b_gpu</span><span class="p">[</span><span class="n">local_b_start</span><span class="p">:</span><span class="n">local_b_end</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-750"><a href="#masked_hamming_ab_sharded-750"><span class="linenos">750</span></a>        <span class="n">mask_b_tile</span> <span class="o">=</span> <span class="n">mask_b_gpu</span><span class="p">[</span><span class="n">local_b_start</span><span class="p">:</span><span class="n">local_b_end</span><span class="p">]</span>
</span><span id="masked_hamming_ab_sharded-751"><a href="#masked_hamming_ab_sharded-751"><span class="linenos">751</span></a>
</span><span id="masked_hamming_ab_sharded-752"><a href="#masked_hamming_ab_sharded-752"><span class="linenos">752</span></a>        <span class="n">labels_a_tile</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="masked_hamming_ab_sharded-753"><a href="#masked_hamming_ab_sharded-753"><span class="linenos">753</span></a>        <span class="n">labels_b_tile</span> <span class="o">=</span> <span class="kc">None</span>
</span><span id="masked_hamming_ab_sharded-754"><a href="#masked_hamming_ab_sharded-754"><span class="linenos">754</span></a>        <span class="k">if</span> <span class="n">labels_a_cpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">labels_b_cpu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-755"><a href="#masked_hamming_ab_sharded-755"><span class="linenos">755</span></a>            <span class="n">labels_a_tile</span> <span class="o">=</span> <span class="n">labels_a_cpu</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">a_start</span><span class="p">:</span><span class="n">shard</span><span class="o">.</span><span class="n">a_end</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-756"><a href="#masked_hamming_ab_sharded-756"><span class="linenos">756</span></a>                <span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span>
</span><span id="masked_hamming_ab_sharded-757"><a href="#masked_hamming_ab_sharded-757"><span class="linenos">757</span></a>            <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-758"><a href="#masked_hamming_ab_sharded-758"><span class="linenos">758</span></a>            <span class="n">labels_b_tile</span> <span class="o">=</span> <span class="n">labels_b_cpu</span><span class="p">[</span><span class="n">shard</span><span class="o">.</span><span class="n">b_start</span><span class="p">:</span><span class="n">shard</span><span class="o">.</span><span class="n">b_end</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-759"><a href="#masked_hamming_ab_sharded-759"><span class="linenos">759</span></a>                <span class="n">shard</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span>
</span><span id="masked_hamming_ab_sharded-760"><a href="#masked_hamming_ab_sharded-760"><span class="linenos">760</span></a>            <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-761"><a href="#masked_hamming_ab_sharded-761"><span class="linenos">761</span></a>
</span><span id="masked_hamming_ab_sharded-762"><a href="#masked_hamming_ab_sharded-762"><span class="linenos">762</span></a>        <span class="n">indices</span><span class="p">,</span> <span class="n">categories</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">masked_hamming_ab_cuda_async</span><span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-763"><a href="#masked_hamming_ab_sharded-763"><span class="linenos">763</span></a>            <span class="n">data_a_tile</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-764"><a href="#masked_hamming_ab_sharded-764"><span class="linenos">764</span></a>            <span class="n">mask_a_tile</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-765"><a href="#masked_hamming_ab_sharded-765"><span class="linenos">765</span></a>            <span class="n">data_b_tile</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-766"><a href="#masked_hamming_ab_sharded-766"><span class="linenos">766</span></a>            <span class="n">mask_b_tile</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-767"><a href="#masked_hamming_ab_sharded-767"><span class="linenos">767</span></a>            <span class="n">labels_a_tile</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-768"><a href="#masked_hamming_ab_sharded-768"><span class="linenos">768</span></a>            <span class="n">labels_b_tile</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-769"><a href="#masked_hamming_ab_sharded-769"><span class="linenos">769</span></a>            <span class="n">match_threshold</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-770"><a href="#masked_hamming_ab_sharded-770"><span class="linenos">770</span></a>            <span class="n">non_match_threshold</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-771"><a href="#masked_hamming_ab_sharded-771"><span class="linenos">771</span></a>            <span class="n">is_similarity</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-772"><a href="#masked_hamming_ab_sharded-772"><span class="linenos">772</span></a>            <span class="n">include_flags</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-773"><a href="#masked_hamming_ab_sharded-773"><span class="linenos">773</span></a>            <span class="n">max_pairs_per_shard</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-774"><a href="#masked_hamming_ab_sharded-774"><span class="linenos">774</span></a>            <span class="n">r_dim</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-775"><a href="#masked_hamming_ab_sharded-775"><span class="linenos">775</span></a>            <span class="n">theta_dim</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-776"><a href="#masked_hamming_ab_sharded-776"><span class="linenos">776</span></a>            <span class="n">d0_dim</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-777"><a href="#masked_hamming_ab_sharded-777"><span class="linenos">777</span></a>            <span class="n">d1_dim</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-778"><a href="#masked_hamming_ab_sharded-778"><span class="linenos">778</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-779"><a href="#masked_hamming_ab_sharded-779"><span class="linenos">779</span></a>
</span><span id="masked_hamming_ab_sharded-780"><a href="#masked_hamming_ab_sharded-780"><span class="linenos">780</span></a>        <span class="k">return</span> <span class="n">ShardKernelResult</span><span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-781"><a href="#masked_hamming_ab_sharded-781"><span class="linenos">781</span></a>            <span class="n">shard</span><span class="o">=</span><span class="n">shard</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-782"><a href="#masked_hamming_ab_sharded-782"><span class="linenos">782</span></a>            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-783"><a href="#masked_hamming_ab_sharded-783"><span class="linenos">783</span></a>            <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-784"><a href="#masked_hamming_ab_sharded-784"><span class="linenos">784</span></a>            <span class="n">distances</span><span class="o">=</span><span class="n">distances</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-785"><a href="#masked_hamming_ab_sharded-785"><span class="linenos">785</span></a>            <span class="n">count</span><span class="o">=</span><span class="n">count</span><span class="p">,</span>
</span><span id="masked_hamming_ab_sharded-786"><a href="#masked_hamming_ab_sharded-786"><span class="linenos">786</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-787"><a href="#masked_hamming_ab_sharded-787"><span class="linenos">787</span></a>
</span><span id="masked_hamming_ab_sharded-788"><a href="#masked_hamming_ab_sharded-788"><span class="linenos">788</span></a>    <span class="c1"># Run shards with async kernel execution across devices</span>
</span><span id="masked_hamming_ab_sharded-789"><a href="#masked_hamming_ab_sharded-789"><span class="linenos">789</span></a>    <span class="n">shard_results</span> <span class="o">=</span> <span class="n">_run_shards_async</span><span class="p">(</span><span class="n">shards</span><span class="p">,</span> <span class="n">launch_kernel</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-790"><a href="#masked_hamming_ab_sharded-790"><span class="linenos">790</span></a>
</span><span id="masked_hamming_ab_sharded-791"><a href="#masked_hamming_ab_sharded-791"><span class="linenos">791</span></a>    <span class="c1"># Aggregate results</span>
</span><span id="masked_hamming_ab_sharded-792"><a href="#masked_hamming_ab_sharded-792"><span class="linenos">792</span></a>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shard_results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-793"><a href="#masked_hamming_ab_sharded-793"><span class="linenos">793</span></a>        <span class="c1"># Return empty tensors</span>
</span><span id="masked_hamming_ab_sharded-794"><a href="#masked_hamming_ab_sharded-794"><span class="linenos">794</span></a>        <span class="k">return</span> <span class="p">(</span>
</span><span id="masked_hamming_ab_sharded-795"><a href="#masked_hamming_ab_sharded-795"><span class="linenos">795</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
</span><span id="masked_hamming_ab_sharded-796"><a href="#masked_hamming_ab_sharded-796"><span class="linenos">796</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">),</span>
</span><span id="masked_hamming_ab_sharded-797"><a href="#masked_hamming_ab_sharded-797"><span class="linenos">797</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
</span><span id="masked_hamming_ab_sharded-798"><a href="#masked_hamming_ab_sharded-798"><span class="linenos">798</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
</span><span id="masked_hamming_ab_sharded-799"><a href="#masked_hamming_ab_sharded-799"><span class="linenos">799</span></a>        <span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-800"><a href="#masked_hamming_ab_sharded-800"><span class="linenos">800</span></a>
</span><span id="masked_hamming_ab_sharded-801"><a href="#masked_hamming_ab_sharded-801"><span class="linenos">801</span></a>    <span class="c1"># Collect and concatenate results, respecting max_pairs limit</span>
</span><span id="masked_hamming_ab_sharded-802"><a href="#masked_hamming_ab_sharded-802"><span class="linenos">802</span></a>    <span class="n">all_indices</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="masked_hamming_ab_sharded-803"><a href="#masked_hamming_ab_sharded-803"><span class="linenos">803</span></a>    <span class="n">all_categories</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="masked_hamming_ab_sharded-804"><a href="#masked_hamming_ab_sharded-804"><span class="linenos">804</span></a>    <span class="n">all_distances</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="masked_hamming_ab_sharded-805"><a href="#masked_hamming_ab_sharded-805"><span class="linenos">805</span></a>    <span class="n">total_count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span id="masked_hamming_ab_sharded-806"><a href="#masked_hamming_ab_sharded-806"><span class="linenos">806</span></a>
</span><span id="masked_hamming_ab_sharded-807"><a href="#masked_hamming_ab_sharded-807"><span class="linenos">807</span></a>    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">shard_results</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-808"><a href="#masked_hamming_ab_sharded-808"><span class="linenos">808</span></a>        <span class="n">remaining</span> <span class="o">=</span> <span class="n">max_pairs</span> <span class="o">-</span> <span class="n">total_count</span>
</span><span id="masked_hamming_ab_sharded-809"><a href="#masked_hamming_ab_sharded-809"><span class="linenos">809</span></a>        <span class="k">if</span> <span class="n">remaining</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span id="masked_hamming_ab_sharded-810"><a href="#masked_hamming_ab_sharded-810"><span class="linenos">810</span></a>            <span class="k">break</span>
</span><span id="masked_hamming_ab_sharded-811"><a href="#masked_hamming_ab_sharded-811"><span class="linenos">811</span></a>        <span class="n">take_count</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">count</span><span class="p">,</span> <span class="n">remaining</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-812"><a href="#masked_hamming_ab_sharded-812"><span class="linenos">812</span></a>        <span class="n">all_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">indices</span><span class="p">[:</span><span class="n">take_count</span><span class="p">])</span>
</span><span id="masked_hamming_ab_sharded-813"><a href="#masked_hamming_ab_sharded-813"><span class="linenos">813</span></a>        <span class="n">all_categories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">categories</span><span class="p">[:</span><span class="n">take_count</span><span class="p">])</span>
</span><span id="masked_hamming_ab_sharded-814"><a href="#masked_hamming_ab_sharded-814"><span class="linenos">814</span></a>        <span class="n">all_distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">distances</span><span class="p">[:</span><span class="n">take_count</span><span class="p">])</span>
</span><span id="masked_hamming_ab_sharded-815"><a href="#masked_hamming_ab_sharded-815"><span class="linenos">815</span></a>        <span class="n">total_count</span> <span class="o">+=</span> <span class="n">take_count</span>
</span><span id="masked_hamming_ab_sharded-816"><a href="#masked_hamming_ab_sharded-816"><span class="linenos">816</span></a>
</span><span id="masked_hamming_ab_sharded-817"><a href="#masked_hamming_ab_sharded-817"><span class="linenos">817</span></a>    <span class="n">pair_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_indices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-818"><a href="#masked_hamming_ab_sharded-818"><span class="linenos">818</span></a>    <span class="n">categories</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_categories</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-819"><a href="#masked_hamming_ab_sharded-819"><span class="linenos">819</span></a>    <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_distances</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-820"><a href="#masked_hamming_ab_sharded-820"><span class="linenos">820</span></a>    <span class="n">count</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">total_count</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span><span id="masked_hamming_ab_sharded-821"><a href="#masked_hamming_ab_sharded-821"><span class="linenos">821</span></a>
</span><span id="masked_hamming_ab_sharded-822"><a href="#masked_hamming_ab_sharded-822"><span class="linenos">822</span></a>    <span class="k">return</span> <span class="n">pair_indices</span><span class="p">,</span> <span class="n">categories</span><span class="p">,</span> <span class="n">distances</span><span class="p">,</span> <span class="n">count</span>
</span></pre></div>


            <div class="docstring"><p>Sharded version of masked_hamming_ab_cuda for multi-GPU and multi-host datasets.</p>

<p>Automatically distributes computation across all available CUDA devices and
handles cases where A or B don't fit on a single device. Accepts either
packed (int32) or unpacked (uint8) data - packing is done on GPU automatically.</p>

<p>For multi-host operation:</p>

<ul>
<li>Each host should have the FULL data tensors (or be able to access all rows)</li>
<li>Set host_index to this host's index (0 to num_hosts-1)</li>
<li>Set num_hosts to total number of hosts</li>
<li>Each host will process only its assigned tiles</li>
<li>Results from all hosts should be aggregated by the caller</li>
</ul>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>data_a:</strong>  Tensor of shape [M_A, k_words] int32 (packed) OR
[M_A, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</li>
<li><strong>mask_a:</strong>  Same shape/dtype as data_a</li>
<li><strong>data_b:</strong>  Tensor of shape [M_B, k_words] int32 (packed) OR
[M_B, r_dim, theta_dim, d0_dim, d1_dim] uint8 (unpacked)</li>
<li><strong>mask_b:</strong>  Same shape/dtype as data_b</li>
<li><strong>labels_a:</strong>  Optional int32 tensor of shape [M_A] with identity labels.</li>
<li><strong>labels_b:</strong>  Optional int32 tensor of shape [M_B] with identity labels.</li>
<li><strong>match_threshold:</strong>  Threshold for match classification (default: 0.35)</li>
<li><strong>non_match_threshold:</strong>  Threshold for non-match classification (default: 0.35)</li>
<li><strong>is_similarity:</strong>  If True, higher values = more similar</li>
<li><strong>include_flags:</strong>  Bitmask of categories to include (default: INCLUDE_ALL)</li>
<li><strong>max_pairs:</strong>  Maximum total pairs to return (default: 1,000,000)</li>
<li><strong>dims:</strong>  Optional tuple (r_dim, theta_dim, d0_dim, d1_dim)</li>
<li><strong>r_dim, theta_dim, d0_dim, d1_dim:</strong>  Iris code dimensions</li>
<li><strong>min_shards:</strong>  Minimum number of shards (useful for testing on single GPU)</li>
<li><strong>max_tile_size:</strong>  Maximum rows per tile (None = auto based on memory)</li>
<li><strong>host_index:</strong>  Index of this host for multi-host operation (default: 0)</li>
<li><strong>num_hosts:</strong>  Total number of hosts for multi-host operation (default: 1)</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>Tuple of (pair_indices, categories, distances, count):</p>
  
  <ul>
  <li>pair_indices: [N, 2] int32 - (row_in_A, row_in_B) indices of pairs</li>
  <li>categories: [N] uint8 - category codes</li>
  <li>distances: [N] float32 - distance values</li>
  <li>count: [1] int32 - actual number of pairs</li>
  </ul>
</blockquote>

<h6 id="example">Example:</h6>

<blockquote>
  <p>Large-scale gallery vs probe matching across multiple GPUs:</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cuda_iris_matcher</span> <span class="k">as</span> <span class="nn">ih</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Large gallery (100K enrolled users) and probe set (1K queries)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gallery</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="mi">100000</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gallery_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">gallery</span><span class="p">,</span> <span class="mh">0x7FFFFFFF</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">400</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">probe_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">probe</span><span class="p">,</span> <span class="mh">0x7FFFFFFF</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Distributes 100M comparisons across all GPUs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_ab_sharded</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">gallery</span><span class="p">,</span> <span class="n">gallery_mask</span><span class="p">,</span> <span class="n">probe</span><span class="p">,</span> <span class="n">probe_mask</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Completed </span><span class="si">{</span><span class="n">count</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2"> comparisons&quot;</span><span class="p">)</span>
</code></pre>
  </div>
</blockquote>
</div>


                </section>
                <section id="pack_theta_major_batched">
                            <input id="pack_theta_major_batched-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">pack_theta_major_batched</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">bits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>,</span><span class="param">	<span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>,</span><span class="param">	<span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span>,</span><span class="param">	<span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span>,</span><span class="param">	<span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">device_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span></span><span class="return-annotation">) -> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>:</span></span>

                <label class="view-source-button" for="pack_theta_major_batched-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#pack_theta_major_batched"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="pack_theta_major_batched-1129"><a href="#pack_theta_major_batched-1129"><span class="linenos">1129</span></a><span class="k">def</span> <span class="nf">pack_theta_major_batched</span><span class="p">(</span>
</span><span id="pack_theta_major_batched-1130"><a href="#pack_theta_major_batched-1130"><span class="linenos">1130</span></a>    <span class="n">bits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
</span><span id="pack_theta_major_batched-1131"><a href="#pack_theta_major_batched-1131"><span class="linenos">1131</span></a>    <span class="n">dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="pack_theta_major_batched-1132"><a href="#pack_theta_major_batched-1132"><span class="linenos">1132</span></a>    <span class="n">r_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_R_DIM</span><span class="p">,</span>
</span><span id="pack_theta_major_batched-1133"><a href="#pack_theta_major_batched-1133"><span class="linenos">1133</span></a>    <span class="n">theta_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_THETA_DIM</span><span class="p">,</span>
</span><span id="pack_theta_major_batched-1134"><a href="#pack_theta_major_batched-1134"><span class="linenos">1134</span></a>    <span class="n">d0_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D0_DIM</span><span class="p">,</span>
</span><span id="pack_theta_major_batched-1135"><a href="#pack_theta_major_batched-1135"><span class="linenos">1135</span></a>    <span class="n">d1_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">DEFAULT_D1_DIM</span><span class="p">,</span>
</span><span id="pack_theta_major_batched-1136"><a href="#pack_theta_major_batched-1136"><span class="linenos">1136</span></a>    <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="pack_theta_major_batched-1137"><a href="#pack_theta_major_batched-1137"><span class="linenos">1137</span></a>    <span class="n">device_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="pack_theta_major_batched-1138"><a href="#pack_theta_major_batched-1138"><span class="linenos">1138</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
</span><span id="pack_theta_major_batched-1139"><a href="#pack_theta_major_batched-1139"><span class="linenos">1139</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Pack iris code bits in batches when data doesn&#39;t fit on GPU.</span>
</span><span id="pack_theta_major_batched-1140"><a href="#pack_theta_major_batched-1140"><span class="linenos">1140</span></a>
</span><span id="pack_theta_major_batched-1141"><a href="#pack_theta_major_batched-1141"><span class="linenos">1141</span></a><span class="sd">    This function handles large datasets that exceed GPU memory by packing</span>
</span><span id="pack_theta_major_batched-1142"><a href="#pack_theta_major_batched-1142"><span class="linenos">1142</span></a><span class="sd">    in batches and accumulating results on CPU.</span>
</span><span id="pack_theta_major_batched-1143"><a href="#pack_theta_major_batched-1143"><span class="linenos">1143</span></a>
</span><span id="pack_theta_major_batched-1144"><a href="#pack_theta_major_batched-1144"><span class="linenos">1144</span></a><span class="sd">    Args:</span>
</span><span id="pack_theta_major_batched-1145"><a href="#pack_theta_major_batched-1145"><span class="linenos">1145</span></a><span class="sd">        bits: CPU uint8 tensor of shape (M, r_dim, theta_dim, d0_dim, d1_dim)</span>
</span><span id="pack_theta_major_batched-1146"><a href="#pack_theta_major_batched-1146"><span class="linenos">1146</span></a><span class="sd">              with values in {0, 1}.</span>
</span><span id="pack_theta_major_batched-1147"><a href="#pack_theta_major_batched-1147"><span class="linenos">1147</span></a><span class="sd">        dims: Optional tuple (r_dim, theta_dim, d0_dim, d1_dim)</span>
</span><span id="pack_theta_major_batched-1148"><a href="#pack_theta_major_batched-1148"><span class="linenos">1148</span></a><span class="sd">        r_dim, theta_dim, d0_dim, d1_dim: Iris code dimensions</span>
</span><span id="pack_theta_major_batched-1149"><a href="#pack_theta_major_batched-1149"><span class="linenos">1149</span></a><span class="sd">        batch_size: Number of samples to pack per batch (None = auto based on memory)</span>
</span><span id="pack_theta_major_batched-1150"><a href="#pack_theta_major_batched-1150"><span class="linenos">1150</span></a><span class="sd">        device_id: CUDA device to use for packing</span>
</span><span id="pack_theta_major_batched-1151"><a href="#pack_theta_major_batched-1151"><span class="linenos">1151</span></a>
</span><span id="pack_theta_major_batched-1152"><a href="#pack_theta_major_batched-1152"><span class="linenos">1152</span></a><span class="sd">    Returns:</span>
</span><span id="pack_theta_major_batched-1153"><a href="#pack_theta_major_batched-1153"><span class="linenos">1153</span></a><span class="sd">        CPU int32 tensor of shape (M, k_words) with packed bits in theta-major order.</span>
</span><span id="pack_theta_major_batched-1154"><a href="#pack_theta_major_batched-1154"><span class="linenos">1154</span></a>
</span><span id="pack_theta_major_batched-1155"><a href="#pack_theta_major_batched-1155"><span class="linenos">1155</span></a><span class="sd">    Example:</span>
</span><span id="pack_theta_major_batched-1156"><a href="#pack_theta_major_batched-1156"><span class="linenos">1156</span></a><span class="sd">        Pack a large dataset that doesn&#39;t fit on GPU:</span>
</span><span id="pack_theta_major_batched-1157"><a href="#pack_theta_major_batched-1157"><span class="linenos">1157</span></a>
</span><span id="pack_theta_major_batched-1158"><a href="#pack_theta_major_batched-1158"><span class="linenos">1158</span></a><span class="sd">        &gt;&gt;&gt; import torch</span>
</span><span id="pack_theta_major_batched-1159"><a href="#pack_theta_major_batched-1159"><span class="linenos">1159</span></a><span class="sd">        &gt;&gt;&gt; import cuda_iris_matcher as ih</span>
</span><span id="pack_theta_major_batched-1160"><a href="#pack_theta_major_batched-1160"><span class="linenos">1160</span></a><span class="sd">        &gt;&gt;&gt; # 1 million iris codes on CPU (too large for GPU)</span>
</span><span id="pack_theta_major_batched-1161"><a href="#pack_theta_major_batched-1161"><span class="linenos">1161</span></a><span class="sd">        &gt;&gt;&gt; raw_codes = torch.randint(0, 2, (1000000, 16, 200, 2, 2), dtype=torch.uint8)</span>
</span><span id="pack_theta_major_batched-1162"><a href="#pack_theta_major_batched-1162"><span class="linenos">1162</span></a><span class="sd">        &gt;&gt;&gt; raw_masks = torch.ones_like(raw_codes)</span>
</span><span id="pack_theta_major_batched-1163"><a href="#pack_theta_major_batched-1163"><span class="linenos">1163</span></a><span class="sd">        &gt;&gt;&gt; # Pack in batches (auto-determines batch size based on GPU memory)</span>
</span><span id="pack_theta_major_batched-1164"><a href="#pack_theta_major_batched-1164"><span class="linenos">1164</span></a><span class="sd">        &gt;&gt;&gt; packed_codes = ih.pack_theta_major_batched(raw_codes)</span>
</span><span id="pack_theta_major_batched-1165"><a href="#pack_theta_major_batched-1165"><span class="linenos">1165</span></a><span class="sd">        &gt;&gt;&gt; packed_masks = ih.pack_theta_major_batched(raw_masks)</span>
</span><span id="pack_theta_major_batched-1166"><a href="#pack_theta_major_batched-1166"><span class="linenos">1166</span></a><span class="sd">        &gt;&gt;&gt; print(packed_codes.shape)  # torch.Size([1000000, 400])</span>
</span><span id="pack_theta_major_batched-1167"><a href="#pack_theta_major_batched-1167"><span class="linenos">1167</span></a><span class="sd">        &gt;&gt;&gt; # Now use with sharded matching</span>
</span><span id="pack_theta_major_batched-1168"><a href="#pack_theta_major_batched-1168"><span class="linenos">1168</span></a><span class="sd">        &gt;&gt;&gt; pairs, _, dists, count = ih.masked_hamming_sharded(packed_codes, packed_masks)</span>
</span><span id="pack_theta_major_batched-1169"><a href="#pack_theta_major_batched-1169"><span class="linenos">1169</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="pack_theta_major_batched-1170"><a href="#pack_theta_major_batched-1170"><span class="linenos">1170</span></a>    <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span> <span class="o">=</span> <span class="n">_resolve_dims</span><span class="p">(</span>
</span><span id="pack_theta_major_batched-1171"><a href="#pack_theta_major_batched-1171"><span class="linenos">1171</span></a>        <span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span>
</span><span id="pack_theta_major_batched-1172"><a href="#pack_theta_major_batched-1172"><span class="linenos">1172</span></a>    <span class="p">)</span>
</span><span id="pack_theta_major_batched-1173"><a href="#pack_theta_major_batched-1173"><span class="linenos">1173</span></a>    <span class="n">k_words</span> <span class="o">=</span> <span class="n">r_dim</span> <span class="o">*</span> <span class="n">theta_dim</span> <span class="o">*</span> <span class="n">d0_dim</span> <span class="o">*</span> <span class="n">d1_dim</span> <span class="o">//</span> <span class="mi">32</span>
</span><span id="pack_theta_major_batched-1174"><a href="#pack_theta_major_batched-1174"><span class="linenos">1174</span></a>
</span><span id="pack_theta_major_batched-1175"><a href="#pack_theta_major_batched-1175"><span class="linenos">1175</span></a>    <span class="k">if</span> <span class="n">bits</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
</span><span id="pack_theta_major_batched-1176"><a href="#pack_theta_major_batched-1176"><span class="linenos">1176</span></a>        <span class="c1"># Already on GPU - clone first since pack_theta_major is in-place</span>
</span><span id="pack_theta_major_batched-1177"><a href="#pack_theta_major_batched-1177"><span class="linenos">1177</span></a>        <span class="n">bits_clone</span> <span class="o">=</span> <span class="n">bits</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span><span id="pack_theta_major_batched-1178"><a href="#pack_theta_major_batched-1178"><span class="linenos">1178</span></a>        <span class="k">return</span> <span class="n">pack_theta_major_cuda</span><span class="p">(</span><span class="n">bits_clone</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span><span id="pack_theta_major_batched-1179"><a href="#pack_theta_major_batched-1179"><span class="linenos">1179</span></a>
</span><span id="pack_theta_major_batched-1180"><a href="#pack_theta_major_batched-1180"><span class="linenos">1180</span></a>    <span class="n">m</span> <span class="o">=</span> <span class="n">bits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span id="pack_theta_major_batched-1181"><a href="#pack_theta_major_batched-1181"><span class="linenos">1181</span></a>
</span><span id="pack_theta_major_batched-1182"><a href="#pack_theta_major_batched-1182"><span class="linenos">1182</span></a>    <span class="c1"># Auto-determine batch size based on available memory</span>
</span><span id="pack_theta_major_batched-1183"><a href="#pack_theta_major_batched-1183"><span class="linenos">1183</span></a>    <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="pack_theta_major_batched-1184"><a href="#pack_theta_major_batched-1184"><span class="linenos">1184</span></a>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_id</span><span class="p">):</span>
</span><span id="pack_theta_major_batched-1185"><a href="#pack_theta_major_batched-1185"><span class="linenos">1185</span></a>            <span class="n">available_mem</span> <span class="o">=</span> <span class="n">_get_available_memory</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="pack_theta_major_batched-1186"><a href="#pack_theta_major_batched-1186"><span class="linenos">1186</span></a>            <span class="c1"># Each sample: unpacked = r*theta*d0*d1 bytes, packed = k_words*4 bytes</span>
</span><span id="pack_theta_major_batched-1187"><a href="#pack_theta_major_batched-1187"><span class="linenos">1187</span></a>            <span class="n">bytes_per_sample_unpacked</span> <span class="o">=</span> <span class="n">r_dim</span> <span class="o">*</span> <span class="n">theta_dim</span> <span class="o">*</span> <span class="n">d0_dim</span> <span class="o">*</span> <span class="n">d1_dim</span>
</span><span id="pack_theta_major_batched-1188"><a href="#pack_theta_major_batched-1188"><span class="linenos">1188</span></a>            <span class="n">bytes_per_sample_packed</span> <span class="o">=</span> <span class="n">k_words</span> <span class="o">*</span> <span class="mi">4</span>
</span><span id="pack_theta_major_batched-1189"><a href="#pack_theta_major_batched-1189"><span class="linenos">1189</span></a>            <span class="c1"># Need space for both unpacked input and packed output</span>
</span><span id="pack_theta_major_batched-1190"><a href="#pack_theta_major_batched-1190"><span class="linenos">1190</span></a>            <span class="n">bytes_per_sample</span> <span class="o">=</span> <span class="n">bytes_per_sample_unpacked</span> <span class="o">+</span> <span class="n">bytes_per_sample_packed</span>
</span><span id="pack_theta_major_batched-1191"><a href="#pack_theta_major_batched-1191"><span class="linenos">1191</span></a>            <span class="c1"># Use 50% of available memory for safety</span>
</span><span id="pack_theta_major_batched-1192"><a href="#pack_theta_major_batched-1192"><span class="linenos">1192</span></a>            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">available_mem</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="n">bytes_per_sample</span><span class="p">))</span>
</span><span id="pack_theta_major_batched-1193"><a href="#pack_theta_major_batched-1193"><span class="linenos">1193</span></a>            <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
</span><span id="pack_theta_major_batched-1194"><a href="#pack_theta_major_batched-1194"><span class="linenos">1194</span></a>
</span><span id="pack_theta_major_batched-1195"><a href="#pack_theta_major_batched-1195"><span class="linenos">1195</span></a>    <span class="c1"># Allocate output on CPU</span>
</span><span id="pack_theta_major_batched-1196"><a href="#pack_theta_major_batched-1196"><span class="linenos">1196</span></a>    <span class="n">packed_cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">k_words</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</span><span id="pack_theta_major_batched-1197"><a href="#pack_theta_major_batched-1197"><span class="linenos">1197</span></a>
</span><span id="pack_theta_major_batched-1198"><a href="#pack_theta_major_batched-1198"><span class="linenos">1198</span></a>    <span class="c1"># Pack in batches</span>
</span><span id="pack_theta_major_batched-1199"><a href="#pack_theta_major_batched-1199"><span class="linenos">1199</span></a>    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device_id</span><span class="p">):</span>
</span><span id="pack_theta_major_batched-1200"><a href="#pack_theta_major_batched-1200"><span class="linenos">1200</span></a>        <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
</span><span id="pack_theta_major_batched-1201"><a href="#pack_theta_major_batched-1201"><span class="linenos">1201</span></a>            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
</span><span id="pack_theta_major_batched-1202"><a href="#pack_theta_major_batched-1202"><span class="linenos">1202</span></a>
</span><span id="pack_theta_major_batched-1203"><a href="#pack_theta_major_batched-1203"><span class="linenos">1203</span></a>            <span class="c1"># Copy batch to GPU</span>
</span><span id="pack_theta_major_batched-1204"><a href="#pack_theta_major_batched-1204"><span class="linenos">1204</span></a>            <span class="n">batch_gpu</span> <span class="o">=</span> <span class="n">bits</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device_id</span><span class="p">)</span>
</span><span id="pack_theta_major_batched-1205"><a href="#pack_theta_major_batched-1205"><span class="linenos">1205</span></a>
</span><span id="pack_theta_major_batched-1206"><a href="#pack_theta_major_batched-1206"><span class="linenos">1206</span></a>            <span class="c1"># Pack on GPU</span>
</span><span id="pack_theta_major_batched-1207"><a href="#pack_theta_major_batched-1207"><span class="linenos">1207</span></a>            <span class="n">packed_batch</span> <span class="o">=</span> <span class="n">pack_theta_major_cuda</span><span class="p">(</span>
</span><span id="pack_theta_major_batched-1208"><a href="#pack_theta_major_batched-1208"><span class="linenos">1208</span></a>                <span class="n">batch_gpu</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">r_dim</span><span class="p">,</span> <span class="n">theta_dim</span><span class="p">,</span> <span class="n">d0_dim</span><span class="p">,</span> <span class="n">d1_dim</span>
</span><span id="pack_theta_major_batched-1209"><a href="#pack_theta_major_batched-1209"><span class="linenos">1209</span></a>            <span class="p">)</span>
</span><span id="pack_theta_major_batched-1210"><a href="#pack_theta_major_batched-1210"><span class="linenos">1210</span></a>
</span><span id="pack_theta_major_batched-1211"><a href="#pack_theta_major_batched-1211"><span class="linenos">1211</span></a>            <span class="c1"># Copy back to CPU</span>
</span><span id="pack_theta_major_batched-1212"><a href="#pack_theta_major_batched-1212"><span class="linenos">1212</span></a>            <span class="n">packed_cpu</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">packed_batch</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
</span><span id="pack_theta_major_batched-1213"><a href="#pack_theta_major_batched-1213"><span class="linenos">1213</span></a>
</span><span id="pack_theta_major_batched-1214"><a href="#pack_theta_major_batched-1214"><span class="linenos">1214</span></a>            <span class="c1"># Free GPU memory</span>
</span><span id="pack_theta_major_batched-1215"><a href="#pack_theta_major_batched-1215"><span class="linenos">1215</span></a>            <span class="k">del</span> <span class="n">batch_gpu</span><span class="p">,</span> <span class="n">packed_batch</span>
</span><span id="pack_theta_major_batched-1216"><a href="#pack_theta_major_batched-1216"><span class="linenos">1216</span></a>            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span id="pack_theta_major_batched-1217"><a href="#pack_theta_major_batched-1217"><span class="linenos">1217</span></a>
</span><span id="pack_theta_major_batched-1218"><a href="#pack_theta_major_batched-1218"><span class="linenos">1218</span></a>    <span class="k">return</span> <span class="n">packed_cpu</span>
</span></pre></div>


            <div class="docstring"><p>Pack iris code bits in batches when data doesn't fit on GPU.</p>

<p>This function handles large datasets that exceed GPU memory by packing
in batches and accumulating results on CPU.</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>bits:</strong>  CPU uint8 tensor of shape (M, r_dim, theta_dim, d0_dim, d1_dim)
with values in {0, 1}.</li>
<li><strong>dims:</strong>  Optional tuple (r_dim, theta_dim, d0_dim, d1_dim)</li>
<li><strong>r_dim, theta_dim, d0_dim, d1_dim:</strong>  Iris code dimensions</li>
<li><strong>batch_size:</strong>  Number of samples to pack per batch (None = auto based on memory)</li>
<li><strong>device_id:</strong>  CUDA device to use for packing</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>CPU int32 tensor of shape (M, k_words) with packed bits in theta-major order.</p>
</blockquote>

<h6 id="example">Example:</h6>

<blockquote>
  <p>Pack a large dataset that doesn't fit on GPU:</p>
  
  <div class="pdoc-code codehilite">
<pre><span></span><code><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cuda_iris_matcher</span> <span class="k">as</span> <span class="nn">ih</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1 million iris codes on CPU (too large for GPU)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">raw_codes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1000000</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">raw_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">raw_codes</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Pack in batches (auto-determines batch size based on GPU memory)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">packed_codes</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">pack_theta_major_batched</span><span class="p">(</span><span class="n">raw_codes</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">packed_masks</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">pack_theta_major_batched</span><span class="p">(</span><span class="n">raw_masks</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">packed_codes</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1000000, 400])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now use with sharded matching</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pairs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">ih</span><span class="o">.</span><span class="n">masked_hamming_sharded</span><span class="p">(</span><span class="n">packed_codes</span><span class="p">,</span> <span class="n">packed_masks</span><span class="p">)</span>
</code></pre>
  </div>
</blockquote>
</div>


                </section>
                <section id="get_device_count">
                            <input id="get_device_count-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_device_count</span><span class="signature pdoc-code condensed">(<span class="return-annotation">) -> <span class="nb">int</span>:</span></span>

                <label class="view-source-button" for="get_device_count-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#get_device_count"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_device_count-1221"><a href="#get_device_count-1221"><span class="linenos">1221</span></a><span class="k">def</span> <span class="nf">get_device_count</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="get_device_count-1222"><a href="#get_device_count-1222"><span class="linenos">1222</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Return the number of available CUDA devices.&quot;&quot;&quot;</span>
</span><span id="get_device_count-1223"><a href="#get_device_count-1223"><span class="linenos">1223</span></a>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
</span></pre></div>


            <div class="docstring"><p>Return the number of available CUDA devices.</p>
</div>


                </section>
                <section id="get_shard_info">
                            <input id="get_shard_info-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_shard_info</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">m_a</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">m_b</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">k_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">400</span>,</span><span class="param">	<span class="n">max_pairs_per_shard</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000</span>,</span><span class="param">	<span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">host_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>,</span><span class="param">	<span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span></span><span class="return-annotation">) -> <span class="n">List</span><span class="p">[</span><span class="n"><a href="#ShardConfig">ShardConfig</a></span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="get_shard_info-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#get_shard_info"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_shard_info-1226"><a href="#get_shard_info-1226"><span class="linenos">1226</span></a><span class="k">def</span> <span class="nf">get_shard_info</span><span class="p">(</span>
</span><span id="get_shard_info-1227"><a href="#get_shard_info-1227"><span class="linenos">1227</span></a>    <span class="n">m_a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="get_shard_info-1228"><a href="#get_shard_info-1228"><span class="linenos">1228</span></a>    <span class="n">m_b</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="get_shard_info-1229"><a href="#get_shard_info-1229"><span class="linenos">1229</span></a>    <span class="n">k_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span>
</span><span id="get_shard_info-1230"><a href="#get_shard_info-1230"><span class="linenos">1230</span></a>    <span class="n">max_pairs_per_shard</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">,</span>
</span><span id="get_shard_info-1231"><a href="#get_shard_info-1231"><span class="linenos">1231</span></a>    <span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="get_shard_info-1232"><a href="#get_shard_info-1232"><span class="linenos">1232</span></a>    <span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="get_shard_info-1233"><a href="#get_shard_info-1233"><span class="linenos">1233</span></a>    <span class="n">host_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="get_shard_info-1234"><a href="#get_shard_info-1234"><span class="linenos">1234</span></a>    <span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="get_shard_info-1235"><a href="#get_shard_info-1235"><span class="linenos">1235</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">ShardConfig</span><span class="p">]:</span>
</span><span id="get_shard_info-1236"><a href="#get_shard_info-1236"><span class="linenos">1236</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Get shard configuration info for planning (debugging/inspection).</span>
</span><span id="get_shard_info-1237"><a href="#get_shard_info-1237"><span class="linenos">1237</span></a>
</span><span id="get_shard_info-1238"><a href="#get_shard_info-1238"><span class="linenos">1238</span></a><span class="sd">    Args:</span>
</span><span id="get_shard_info-1239"><a href="#get_shard_info-1239"><span class="linenos">1239</span></a><span class="sd">        m_a: Number of rows in A</span>
</span><span id="get_shard_info-1240"><a href="#get_shard_info-1240"><span class="linenos">1240</span></a><span class="sd">        m_b: Number of rows in B</span>
</span><span id="get_shard_info-1241"><a href="#get_shard_info-1241"><span class="linenos">1241</span></a><span class="sd">        k_words: Number of int32 words per row (default 400 for standard iris)</span>
</span><span id="get_shard_info-1242"><a href="#get_shard_info-1242"><span class="linenos">1242</span></a><span class="sd">        max_pairs_per_shard: Max pairs per shard</span>
</span><span id="get_shard_info-1243"><a href="#get_shard_info-1243"><span class="linenos">1243</span></a><span class="sd">        min_shards: Minimum number of shards</span>
</span><span id="get_shard_info-1244"><a href="#get_shard_info-1244"><span class="linenos">1244</span></a><span class="sd">        max_tile_size: Maximum tile size</span>
</span><span id="get_shard_info-1245"><a href="#get_shard_info-1245"><span class="linenos">1245</span></a><span class="sd">        host_index: Index of this host for multi-host operation (default: 0)</span>
</span><span id="get_shard_info-1246"><a href="#get_shard_info-1246"><span class="linenos">1246</span></a><span class="sd">        num_hosts: Total number of hosts for multi-host operation (default: 1)</span>
</span><span id="get_shard_info-1247"><a href="#get_shard_info-1247"><span class="linenos">1247</span></a>
</span><span id="get_shard_info-1248"><a href="#get_shard_info-1248"><span class="linenos">1248</span></a><span class="sd">    Returns:</span>
</span><span id="get_shard_info-1249"><a href="#get_shard_info-1249"><span class="linenos">1249</span></a><span class="sd">        List of ShardConfig objects for this host</span>
</span><span id="get_shard_info-1250"><a href="#get_shard_info-1250"><span class="linenos">1250</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="get_shard_info-1251"><a href="#get_shard_info-1251"><span class="linenos">1251</span></a>    <span class="n">num_devices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
</span><span id="get_shard_info-1252"><a href="#get_shard_info-1252"><span class="linenos">1252</span></a>    <span class="n">total_devices</span> <span class="o">=</span> <span class="n">num_devices</span> <span class="o">*</span> <span class="n">num_hosts</span>
</span><span id="get_shard_info-1253"><a href="#get_shard_info-1253"><span class="linenos">1253</span></a>    <span class="n">all_shards</span> <span class="o">=</span> <span class="n">_compute_shard_configs</span><span class="p">(</span>
</span><span id="get_shard_info-1254"><a href="#get_shard_info-1254"><span class="linenos">1254</span></a>        <span class="n">m_a</span><span class="p">,</span> <span class="n">m_b</span><span class="p">,</span> <span class="n">k_words</span><span class="p">,</span> <span class="n">max_pairs_per_shard</span><span class="p">,</span> <span class="n">total_devices</span><span class="p">,</span> <span class="n">min_shards</span> <span class="o">*</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">max_tile_size</span>
</span><span id="get_shard_info-1255"><a href="#get_shard_info-1255"><span class="linenos">1255</span></a>    <span class="p">)</span>
</span><span id="get_shard_info-1256"><a href="#get_shard_info-1256"><span class="linenos">1256</span></a>    <span class="k">return</span> <span class="n">_filter_shards_for_host</span><span class="p">(</span><span class="n">all_shards</span><span class="p">,</span> <span class="n">host_index</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Get shard configuration info for planning (debugging/inspection).</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>m_a:</strong>  Number of rows in A</li>
<li><strong>m_b:</strong>  Number of rows in B</li>
<li><strong>k_words:</strong>  Number of int32 words per row (default 400 for standard iris)</li>
<li><strong>max_pairs_per_shard:</strong>  Max pairs per shard</li>
<li><strong>min_shards:</strong>  Minimum number of shards</li>
<li><strong>max_tile_size:</strong>  Maximum tile size</li>
<li><strong>host_index:</strong>  Index of this host for multi-host operation (default: 0)</li>
<li><strong>num_hosts:</strong>  Total number of hosts for multi-host operation (default: 1)</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>List of ShardConfig objects for this host</p>
</blockquote>
</div>


                </section>
                <section id="get_self_shard_info">
                            <input id="get_self_shard_info-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_self_shard_info</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">m</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">k_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">400</span>,</span><span class="param">	<span class="n">max_pairs_per_shard</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000</span>,</span><span class="param">	<span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">host_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>,</span><span class="param">	<span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span></span><span class="return-annotation">) -> <span class="n">List</span><span class="p">[</span><span class="n"><a href="#ShardConfig">ShardConfig</a></span><span class="p">]</span>:</span></span>

                <label class="view-source-button" for="get_self_shard_info-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#get_self_shard_info"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_self_shard_info-1259"><a href="#get_self_shard_info-1259"><span class="linenos">1259</span></a><span class="k">def</span> <span class="nf">get_self_shard_info</span><span class="p">(</span>
</span><span id="get_self_shard_info-1260"><a href="#get_self_shard_info-1260"><span class="linenos">1260</span></a>    <span class="n">m</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="get_self_shard_info-1261"><a href="#get_self_shard_info-1261"><span class="linenos">1261</span></a>    <span class="n">k_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span>
</span><span id="get_self_shard_info-1262"><a href="#get_self_shard_info-1262"><span class="linenos">1262</span></a>    <span class="n">max_pairs_per_shard</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">,</span>
</span><span id="get_self_shard_info-1263"><a href="#get_self_shard_info-1263"><span class="linenos">1263</span></a>    <span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="get_self_shard_info-1264"><a href="#get_self_shard_info-1264"><span class="linenos">1264</span></a>    <span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="get_self_shard_info-1265"><a href="#get_self_shard_info-1265"><span class="linenos">1265</span></a>    <span class="n">host_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="get_self_shard_info-1266"><a href="#get_self_shard_info-1266"><span class="linenos">1266</span></a>    <span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="get_self_shard_info-1267"><a href="#get_self_shard_info-1267"><span class="linenos">1267</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">ShardConfig</span><span class="p">]:</span>
</span><span id="get_self_shard_info-1268"><a href="#get_self_shard_info-1268"><span class="linenos">1268</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Get shard configuration info for self-comparison (lower triangle).</span>
</span><span id="get_self_shard_info-1269"><a href="#get_self_shard_info-1269"><span class="linenos">1269</span></a>
</span><span id="get_self_shard_info-1270"><a href="#get_self_shard_info-1270"><span class="linenos">1270</span></a><span class="sd">    Args:</span>
</span><span id="get_self_shard_info-1271"><a href="#get_self_shard_info-1271"><span class="linenos">1271</span></a><span class="sd">        m: Number of rows</span>
</span><span id="get_self_shard_info-1272"><a href="#get_self_shard_info-1272"><span class="linenos">1272</span></a><span class="sd">        k_words: Number of int32 words per row (default 400 for standard iris)</span>
</span><span id="get_self_shard_info-1273"><a href="#get_self_shard_info-1273"><span class="linenos">1273</span></a><span class="sd">        max_pairs_per_shard: Max pairs per shard</span>
</span><span id="get_self_shard_info-1274"><a href="#get_self_shard_info-1274"><span class="linenos">1274</span></a><span class="sd">        min_shards: Minimum number of shards</span>
</span><span id="get_self_shard_info-1275"><a href="#get_self_shard_info-1275"><span class="linenos">1275</span></a><span class="sd">        max_tile_size: Maximum tile size</span>
</span><span id="get_self_shard_info-1276"><a href="#get_self_shard_info-1276"><span class="linenos">1276</span></a><span class="sd">        host_index: Index of this host for multi-host operation (default: 0)</span>
</span><span id="get_self_shard_info-1277"><a href="#get_self_shard_info-1277"><span class="linenos">1277</span></a><span class="sd">        num_hosts: Total number of hosts for multi-host operation (default: 1)</span>
</span><span id="get_self_shard_info-1278"><a href="#get_self_shard_info-1278"><span class="linenos">1278</span></a>
</span><span id="get_self_shard_info-1279"><a href="#get_self_shard_info-1279"><span class="linenos">1279</span></a><span class="sd">    Returns:</span>
</span><span id="get_self_shard_info-1280"><a href="#get_self_shard_info-1280"><span class="linenos">1280</span></a><span class="sd">        List of ShardConfig objects for this host</span>
</span><span id="get_self_shard_info-1281"><a href="#get_self_shard_info-1281"><span class="linenos">1281</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="get_self_shard_info-1282"><a href="#get_self_shard_info-1282"><span class="linenos">1282</span></a>    <span class="n">num_devices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
</span><span id="get_self_shard_info-1283"><a href="#get_self_shard_info-1283"><span class="linenos">1283</span></a>    <span class="n">total_devices</span> <span class="o">=</span> <span class="n">num_devices</span> <span class="o">*</span> <span class="n">num_hosts</span>
</span><span id="get_self_shard_info-1284"><a href="#get_self_shard_info-1284"><span class="linenos">1284</span></a>    <span class="n">all_shards</span> <span class="o">=</span> <span class="n">_compute_self_shard_configs</span><span class="p">(</span>
</span><span id="get_self_shard_info-1285"><a href="#get_self_shard_info-1285"><span class="linenos">1285</span></a>        <span class="n">m</span><span class="p">,</span> <span class="n">k_words</span><span class="p">,</span> <span class="n">max_pairs_per_shard</span><span class="p">,</span> <span class="n">total_devices</span><span class="p">,</span> <span class="n">min_shards</span> <span class="o">*</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">max_tile_size</span>
</span><span id="get_self_shard_info-1286"><a href="#get_self_shard_info-1286"><span class="linenos">1286</span></a>    <span class="p">)</span>
</span><span id="get_self_shard_info-1287"><a href="#get_self_shard_info-1287"><span class="linenos">1287</span></a>    <span class="k">return</span> <span class="n">_filter_shards_for_host</span><span class="p">(</span><span class="n">all_shards</span><span class="p">,</span> <span class="n">host_index</span><span class="p">,</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Get shard configuration info for self-comparison (lower triangle).</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>m:</strong>  Number of rows</li>
<li><strong>k_words:</strong>  Number of int32 words per row (default 400 for standard iris)</li>
<li><strong>max_pairs_per_shard:</strong>  Max pairs per shard</li>
<li><strong>min_shards:</strong>  Minimum number of shards</li>
<li><strong>max_tile_size:</strong>  Maximum tile size</li>
<li><strong>host_index:</strong>  Index of this host for multi-host operation (default: 0)</li>
<li><strong>num_hosts:</strong>  Total number of hosts for multi-host operation (default: 1)</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>List of ShardConfig objects for this host</p>
</blockquote>
</div>


                </section>
                <section id="get_total_shards">
                            <input id="get_total_shards-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr function">
            
        <span class="def">def</span>
        <span class="name">get_total_shards</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">m_a</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">m_b</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">k_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">400</span>,</span><span class="param">	<span class="n">max_pairs_per_shard</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000</span>,</span><span class="param">	<span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>,</span><span class="param">	<span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>,</span><span class="param">	<span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span></span><span class="return-annotation">) -> <span class="nb">int</span>:</span></span>

                <label class="view-source-button" for="get_total_shards-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#get_total_shards"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="get_total_shards-1290"><a href="#get_total_shards-1290"><span class="linenos">1290</span></a><span class="k">def</span> <span class="nf">get_total_shards</span><span class="p">(</span>
</span><span id="get_total_shards-1291"><a href="#get_total_shards-1291"><span class="linenos">1291</span></a>    <span class="n">m_a</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
</span><span id="get_total_shards-1292"><a href="#get_total_shards-1292"><span class="linenos">1292</span></a>    <span class="n">m_b</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="get_total_shards-1293"><a href="#get_total_shards-1293"><span class="linenos">1293</span></a>    <span class="n">k_words</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">400</span><span class="p">,</span>
</span><span id="get_total_shards-1294"><a href="#get_total_shards-1294"><span class="linenos">1294</span></a>    <span class="n">max_pairs_per_shard</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000</span><span class="p">,</span>
</span><span id="get_total_shards-1295"><a href="#get_total_shards-1295"><span class="linenos">1295</span></a>    <span class="n">min_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="get_total_shards-1296"><a href="#get_total_shards-1296"><span class="linenos">1296</span></a>    <span class="n">max_tile_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
</span><span id="get_total_shards-1297"><a href="#get_total_shards-1297"><span class="linenos">1297</span></a>    <span class="n">num_hosts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span><span id="get_total_shards-1298"><a href="#get_total_shards-1298"><span class="linenos">1298</span></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span id="get_total_shards-1299"><a href="#get_total_shards-1299"><span class="linenos">1299</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Get total number of shards across all hosts.</span>
</span><span id="get_total_shards-1300"><a href="#get_total_shards-1300"><span class="linenos">1300</span></a>
</span><span id="get_total_shards-1301"><a href="#get_total_shards-1301"><span class="linenos">1301</span></a><span class="sd">    Useful for determining how work will be distributed before launching.</span>
</span><span id="get_total_shards-1302"><a href="#get_total_shards-1302"><span class="linenos">1302</span></a>
</span><span id="get_total_shards-1303"><a href="#get_total_shards-1303"><span class="linenos">1303</span></a><span class="sd">    Args:</span>
</span><span id="get_total_shards-1304"><a href="#get_total_shards-1304"><span class="linenos">1304</span></a><span class="sd">        m_a: Number of rows in A (or total rows for self-comparison)</span>
</span><span id="get_total_shards-1305"><a href="#get_total_shards-1305"><span class="linenos">1305</span></a><span class="sd">        m_b: Number of rows in B (None for self-comparison)</span>
</span><span id="get_total_shards-1306"><a href="#get_total_shards-1306"><span class="linenos">1306</span></a><span class="sd">        k_words: Number of int32 words per row (default 400 for standard iris)</span>
</span><span id="get_total_shards-1307"><a href="#get_total_shards-1307"><span class="linenos">1307</span></a><span class="sd">        max_pairs_per_shard: Max pairs per shard</span>
</span><span id="get_total_shards-1308"><a href="#get_total_shards-1308"><span class="linenos">1308</span></a><span class="sd">        min_shards: Minimum number of shards per host</span>
</span><span id="get_total_shards-1309"><a href="#get_total_shards-1309"><span class="linenos">1309</span></a><span class="sd">        max_tile_size: Maximum tile size</span>
</span><span id="get_total_shards-1310"><a href="#get_total_shards-1310"><span class="linenos">1310</span></a><span class="sd">        num_hosts: Total number of hosts</span>
</span><span id="get_total_shards-1311"><a href="#get_total_shards-1311"><span class="linenos">1311</span></a>
</span><span id="get_total_shards-1312"><a href="#get_total_shards-1312"><span class="linenos">1312</span></a><span class="sd">    Returns:</span>
</span><span id="get_total_shards-1313"><a href="#get_total_shards-1313"><span class="linenos">1313</span></a><span class="sd">        Total number of shards</span>
</span><span id="get_total_shards-1314"><a href="#get_total_shards-1314"><span class="linenos">1314</span></a><span class="sd">    &quot;&quot;&quot;</span>
</span><span id="get_total_shards-1315"><a href="#get_total_shards-1315"><span class="linenos">1315</span></a>    <span class="n">num_devices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
</span><span id="get_total_shards-1316"><a href="#get_total_shards-1316"><span class="linenos">1316</span></a>    <span class="n">total_devices</span> <span class="o">=</span> <span class="n">num_devices</span> <span class="o">*</span> <span class="n">num_hosts</span>
</span><span id="get_total_shards-1317"><a href="#get_total_shards-1317"><span class="linenos">1317</span></a>
</span><span id="get_total_shards-1318"><a href="#get_total_shards-1318"><span class="linenos">1318</span></a>    <span class="k">if</span> <span class="n">m_b</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
</span><span id="get_total_shards-1319"><a href="#get_total_shards-1319"><span class="linenos">1319</span></a>        <span class="c1"># Self-comparison</span>
</span><span id="get_total_shards-1320"><a href="#get_total_shards-1320"><span class="linenos">1320</span></a>        <span class="n">shards</span> <span class="o">=</span> <span class="n">_compute_self_shard_configs</span><span class="p">(</span>
</span><span id="get_total_shards-1321"><a href="#get_total_shards-1321"><span class="linenos">1321</span></a>            <span class="n">m_a</span><span class="p">,</span> <span class="n">k_words</span><span class="p">,</span> <span class="n">max_pairs_per_shard</span><span class="p">,</span> <span class="n">total_devices</span><span class="p">,</span> <span class="n">min_shards</span> <span class="o">*</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">max_tile_size</span>
</span><span id="get_total_shards-1322"><a href="#get_total_shards-1322"><span class="linenos">1322</span></a>        <span class="p">)</span>
</span><span id="get_total_shards-1323"><a href="#get_total_shards-1323"><span class="linenos">1323</span></a>    <span class="k">else</span><span class="p">:</span>
</span><span id="get_total_shards-1324"><a href="#get_total_shards-1324"><span class="linenos">1324</span></a>        <span class="c1"># A vs B comparison</span>
</span><span id="get_total_shards-1325"><a href="#get_total_shards-1325"><span class="linenos">1325</span></a>        <span class="n">shards</span> <span class="o">=</span> <span class="n">_compute_shard_configs</span><span class="p">(</span>
</span><span id="get_total_shards-1326"><a href="#get_total_shards-1326"><span class="linenos">1326</span></a>            <span class="n">m_a</span><span class="p">,</span> <span class="n">m_b</span><span class="p">,</span> <span class="n">k_words</span><span class="p">,</span> <span class="n">max_pairs_per_shard</span><span class="p">,</span> <span class="n">total_devices</span><span class="p">,</span> <span class="n">min_shards</span> <span class="o">*</span> <span class="n">num_hosts</span><span class="p">,</span> <span class="n">max_tile_size</span>
</span><span id="get_total_shards-1327"><a href="#get_total_shards-1327"><span class="linenos">1327</span></a>        <span class="p">)</span>
</span><span id="get_total_shards-1328"><a href="#get_total_shards-1328"><span class="linenos">1328</span></a>    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">shards</span><span class="p">)</span>
</span></pre></div>


            <div class="docstring"><p>Get total number of shards across all hosts.</p>

<p>Useful for determining how work will be distributed before launching.</p>

<h6 id="arguments">Arguments:</h6>

<ul>
<li><strong>m_a:</strong>  Number of rows in A (or total rows for self-comparison)</li>
<li><strong>m_b:</strong>  Number of rows in B (None for self-comparison)</li>
<li><strong>k_words:</strong>  Number of int32 words per row (default 400 for standard iris)</li>
<li><strong>max_pairs_per_shard:</strong>  Max pairs per shard</li>
<li><strong>min_shards:</strong>  Minimum number of shards per host</li>
<li><strong>max_tile_size:</strong>  Maximum tile size</li>
<li><strong>num_hosts:</strong>  Total number of hosts</li>
</ul>

<h6 id="returns">Returns:</h6>

<blockquote>
  <p>Total number of shards</p>
</blockquote>
</div>


                </section>
                <section id="ShardConfig">
                            <input id="ShardConfig-view-source" class="view-source-toggle-state" type="checkbox" aria-hidden="true" tabindex="-1">
<div class="attr class">
                    <div class="decorator decorator-dataclass">@dataclass</div>

    <span class="def">class</span>
    <span class="name">ShardConfig</span>:

                <label class="view-source-button" for="ShardConfig-view-source"><span>View Source</span></label>

    </div>
    <a class="headerlink" href="#ShardConfig"></a>
            <div class="pdoc-code codehilite"><pre><span></span><span id="ShardConfig-31"><a href="#ShardConfig-31"><span class="linenos">31</span></a><span class="nd">@dataclass</span>
</span><span id="ShardConfig-32"><a href="#ShardConfig-32"><span class="linenos">32</span></a><span class="k">class</span> <span class="nc">ShardConfig</span><span class="p">:</span>
</span><span id="ShardConfig-33"><a href="#ShardConfig-33"><span class="linenos">33</span></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Configuration for a single shard of computation.&quot;&quot;&quot;</span>
</span><span id="ShardConfig-34"><a href="#ShardConfig-34"><span class="linenos">34</span></a>
</span><span id="ShardConfig-35"><a href="#ShardConfig-35"><span class="linenos">35</span></a>    <span class="n">device_id</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># CUDA device ID (local to this host)</span>
</span><span id="ShardConfig-36"><a href="#ShardConfig-36"><span class="linenos">36</span></a>    <span class="n">a_start</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># Start index in A</span>
</span><span id="ShardConfig-37"><a href="#ShardConfig-37"><span class="linenos">37</span></a>    <span class="n">a_end</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># End index in A (exclusive)</span>
</span><span id="ShardConfig-38"><a href="#ShardConfig-38"><span class="linenos">38</span></a>    <span class="n">b_start</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># Start index in B</span>
</span><span id="ShardConfig-39"><a href="#ShardConfig-39"><span class="linenos">39</span></a>    <span class="n">b_end</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># End index in B (exclusive)</span>
</span><span id="ShardConfig-40"><a href="#ShardConfig-40"><span class="linenos">40</span></a>    <span class="n">global_shard_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Global shard ID across all hosts (for deterministic mapping)</span>
</span></pre></div>


            <div class="docstring"><p>Configuration for a single shard of computation.</p>
</div>


                            <div id="ShardConfig.__init__" class="classattr">
                                <div class="attr function">
            
        <span class="name">ShardConfig</span><span class="signature pdoc-code multiline">(<span class="param">	<span class="n">device_id</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">a_start</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">a_end</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">b_start</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">b_end</span><span class="p">:</span> <span class="nb">int</span>,</span><span class="param">	<span class="n">global_shard_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span></span>)</span>

        
    </div>
    <a class="headerlink" href="#ShardConfig.__init__"></a>
    
    

                            </div>
                            <div id="ShardConfig.device_id" class="classattr">
                                <div class="attr variable">
            <span class="name">device_id</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#ShardConfig.device_id"></a>
    
    

                            </div>
                            <div id="ShardConfig.a_start" class="classattr">
                                <div class="attr variable">
            <span class="name">a_start</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#ShardConfig.a_start"></a>
    
    

                            </div>
                            <div id="ShardConfig.a_end" class="classattr">
                                <div class="attr variable">
            <span class="name">a_end</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#ShardConfig.a_end"></a>
    
    

                            </div>
                            <div id="ShardConfig.b_start" class="classattr">
                                <div class="attr variable">
            <span class="name">b_start</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#ShardConfig.b_start"></a>
    
    

                            </div>
                            <div id="ShardConfig.b_end" class="classattr">
                                <div class="attr variable">
            <span class="name">b_end</span><span class="annotation">: int</span>

        
    </div>
    <a class="headerlink" href="#ShardConfig.b_end"></a>
    
    

                            </div>
                            <div id="ShardConfig.global_shard_id" class="classattr">
                                <div class="attr variable">
            <span class="name">global_shard_id</span><span class="annotation">: int</span>        =
<span class="default_value">0</span>

        
    </div>
    <a class="headerlink" href="#ShardConfig.global_shard_id"></a>
    
    

                            </div>
                </section>
                <section id="CATEGORY_TRUE_MATCH">
                    <div class="attr variable">
            <span class="name">CATEGORY_TRUE_MATCH</span>        =
<span class="default_value">0</span>

        
    </div>
    <a class="headerlink" href="#CATEGORY_TRUE_MATCH"></a>
    
    

                </section>
                <section id="CATEGORY_FALSE_MATCH">
                    <div class="attr variable">
            <span class="name">CATEGORY_FALSE_MATCH</span>        =
<span class="default_value">1</span>

        
    </div>
    <a class="headerlink" href="#CATEGORY_FALSE_MATCH"></a>
    
    

                </section>
                <section id="CATEGORY_FALSE_NON_MATCH">
                    <div class="attr variable">
            <span class="name">CATEGORY_FALSE_NON_MATCH</span>        =
<span class="default_value">2</span>

        
    </div>
    <a class="headerlink" href="#CATEGORY_FALSE_NON_MATCH"></a>
    
    

                </section>
                <section id="CATEGORY_TRUE_NON_MATCH">
                    <div class="attr variable">
            <span class="name">CATEGORY_TRUE_NON_MATCH</span>        =
<span class="default_value">3</span>

        
    </div>
    <a class="headerlink" href="#CATEGORY_TRUE_NON_MATCH"></a>
    
    

                </section>
                <section id="INCLUDE_TM">
                    <div class="attr variable">
            <span class="name">INCLUDE_TM</span>        =
<span class="default_value">1</span>

        
    </div>
    <a class="headerlink" href="#INCLUDE_TM"></a>
    
    

                </section>
                <section id="INCLUDE_FM">
                    <div class="attr variable">
            <span class="name">INCLUDE_FM</span>        =
<span class="default_value">2</span>

        
    </div>
    <a class="headerlink" href="#INCLUDE_FM"></a>
    
    

                </section>
                <section id="INCLUDE_FNM">
                    <div class="attr variable">
            <span class="name">INCLUDE_FNM</span>        =
<span class="default_value">4</span>

        
    </div>
    <a class="headerlink" href="#INCLUDE_FNM"></a>
    
    

                </section>
                <section id="INCLUDE_TNM">
                    <div class="attr variable">
            <span class="name">INCLUDE_TNM</span>        =
<span class="default_value">8</span>

        
    </div>
    <a class="headerlink" href="#INCLUDE_TNM"></a>
    
    

                </section>
                <section id="INCLUDE_ALL">
                    <div class="attr variable">
            <span class="name">INCLUDE_ALL</span>        =
<span class="default_value">15</span>

        
    </div>
    <a class="headerlink" href="#INCLUDE_ALL"></a>
    
    

                </section>
                <section id="DEFAULT_R_DIM">
                    <div class="attr variable">
            <span class="name">DEFAULT_R_DIM</span>        =
<span class="default_value">16</span>

        
    </div>
    <a class="headerlink" href="#DEFAULT_R_DIM"></a>
    
    

                </section>
                <section id="DEFAULT_THETA_DIM">
                    <div class="attr variable">
            <span class="name">DEFAULT_THETA_DIM</span>        =
<span class="default_value">200</span>

        
    </div>
    <a class="headerlink" href="#DEFAULT_THETA_DIM"></a>
    
    

                </section>
                <section id="DEFAULT_D0_DIM">
                    <div class="attr variable">
            <span class="name">DEFAULT_D0_DIM</span>        =
<span class="default_value">2</span>

        
    </div>
    <a class="headerlink" href="#DEFAULT_D0_DIM"></a>
    
    

                </section>
                <section id="DEFAULT_D1_DIM">
                    <div class="attr variable">
            <span class="name">DEFAULT_D1_DIM</span>        =
<span class="default_value">2</span>

        
    </div>
    <a class="headerlink" href="#DEFAULT_D1_DIM"></a>
    
    

                </section>
                <section id="DEFAULT_DIMS">
                    <div class="attr variable">
            <span class="name">DEFAULT_DIMS</span>        =
<span class="default_value">(16, 200, 2, 2)</span>

        
    </div>
    <a class="headerlink" href="#DEFAULT_DIMS"></a>
    
    

                </section>
    </main>
</body>
</html>